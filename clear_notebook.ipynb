{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/grinenko/anna/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DataSet2.csv\", sep=\";\")#, parse_dates=['Timestamp']) #, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "df = data.groupby(data.index // K).mean() #усреднение\n",
    "df_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError, MeanAbsolutePercentageError\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mase = MeanAbsoluteScaledError(multioutput='raw_values')\n",
    "mase_uni = MeanAbsoluteScaledError(multioutput='uniform_average')\n",
    "mape = MeanAbsolutePercentageError(multioutput='raw_values')\n",
    "# mae = MeanAbsoluteError()\n",
    "\n",
    "\n",
    "#if ‘raw_values’, returns a full set of errors in case of multioutput input. If ‘uniform_average’, errors of all outputs are averaged with uniform weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408096, 67)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_np[:, :]\n",
    "# dataset = np.concatenate((dataset[:, :8], dataset[:, 9:]), axis=1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Отбор признаков на минималках__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408096, 65)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удаление константных\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.001)\n",
    "dataset1 = selector.fit_transform(dataset) \n",
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8, 14}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(range(67)) - set(selector.get_feature_names_out(input_features=np.arange(dataset.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(dataset[:, 14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(dataset[:, 8], color=\"green\")\n",
    "plt.plot(np.concatenate((dataset[:32714, 8], dataset[32717:, 8])))\n",
    "plt.title(\"consumption_09\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00292  0.00733  0.754974 1.658794 1.956544]\n",
      "[32714, 375379, 1, 1, 1]\n",
      "Изменение только в [[32713]\n",
      " [32714]\n",
      " [32715]\n",
      " [32716]]\n"
     ]
    }
   ],
   "source": [
    "values = np.unique(dataset[:, 8])\n",
    "print(values)\n",
    "print([len(np.argwhere(dataset[:, 8] == val)) for val in values])\n",
    "print(f\"Изменение только в {np.argwhere(dataset[1:, 8] - dataset[:-1, 8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эти датчики пока выкинем из-за константности: ** \n",
    "\n",
    "8: consumption_09\n",
    "\n",
    "14 : consumption_07 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('consumption_07', 'consumption_09')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns[15], columns[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f383075deb0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dataset[7])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import correlate\n",
    "# a = np.array([np.sin(x + 1) for x in range(10)])\n",
    "# b = np.array([np.sin(x + 2) for x in range(10)])\n",
    "# c = correlate(a, b, mode=\"full\") #mode='same'\n",
    "# plt.plot(a, label=\"a\")\n",
    "# plt.plot(b, label=\"b\")\n",
    "# plt.plot(c, label=\"correlation\")\n",
    "# plt.legend()\n",
    "# c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408096, 65)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset1\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "#еще не поняла до конца\n",
    "from sklearn.feature_selection import f_regression\n",
    "for i in range(1):\n",
    "    X = np.concatenate((dataset[:, :i], dataset[:, i + 1:]), axis=1)\n",
    "    y = dataset[:, i]\n",
    "    res = f_regression(X, y)\n",
    "    print(res[0].shape)\n",
    "    # plt.plot(res[0], label=\"f_statistics\")\n",
    "    # # plt.plot(res[1], label=\"p_values\")\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([np.sin(x) for x in range(10)])\n",
    "# b = np.array([np.sin(x + 1) + 1 for x in range(10)])\n",
    "# c = np.array([np.sin(x + 0.1) for x in range(10)])\n",
    "\n",
    "# f_regression(a[:, None], b), f_regression(a[:, None], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Прогнозирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408096, 65)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset1\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "window_sizes_for_clustering = 10\n",
    "# X, y = dataset[:-window_sizes_for_clustering, ...], dataset[window_sizes_for_clustering:, ...]\n",
    "# X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "n_split = round(0.2 * dataset.shape[0])\n",
    "dataset_train, dataset_test = dataset[:-n_split, ...], dataset[-n_split:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((326477, 65), (81619, 65))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape, dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes_for_clustering = [1, 5, 10, 15] #[1, 3, 5, 10, 15]\n",
    "Ns_clusters = [5, 7, 9, 11]#[2, 5, 7, 9, 11, 13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/grinenko/anna/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ns_clusters = [2]\n",
    "# window_sizes_for_clustering = [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, M, Q = 100, 100, 2\n",
    "# dataset_train = np.column_stack([[np.sin(x / 40) for x in range(N)], [0.01 + np.sin(x / 20) for x in range(N)]])\n",
    "# # dataset_train = np.array([np.sin(x / 10000) for x in range(N)])[:, None]\n",
    "# # dataset_train = np.column_stack(([np.sqrt(x / 10) for x in range(N)], [x + ])\n",
    "# dataset_test = np.column_stack([[np.sin(x / 30) for x in range(M)], [0.01 + np.sin(x / 30)*1.01 for x in range(M)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326477, 65)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.55554262e+00, 6.94718493e+00, 1.35044690e+00, 1.43435980e+00,\n",
       "       1.96606313e+00, 1.20993317e+01, 1.69550842e+01, 1.05198288e+01,\n",
       "       1.27387877e+01, 3.79597264e+01, 1.29898909e+01, 5.38906772e+02,\n",
       "       1.20528042e+01, 1.56745749e+01, 1.61560677e+01, 7.02821956e+01,\n",
       "       2.03564098e-01, 1.28193028e+01, 3.81318543e+01, 1.79147010e+01,\n",
       "       4.05475406e+01, 1.84169088e+01, 3.87928704e+01, 2.11052226e+01,\n",
       "       3.90328106e+01, 4.50636914e+01, 1.86632419e+01, 9.42706808e+01,\n",
       "       2.04214968e-01, 1.10960583e+01, 1.70836005e+01, 1.19863127e+01,\n",
       "       1.77617573e+01, 1.14188344e+01, 1.57698465e+01, 1.12312718e+01,\n",
       "       1.65940653e+01, 1.54125731e+01, 4.69899212e+01, 1.12249275e+02,\n",
       "       1.11860182e+02, 1.12835472e+02, 1.13178754e+02, 9.81358594e+00,\n",
       "       3.04801927e+01, 4.12606659e+01, 8.51470684e+01, 1.63650176e+02,\n",
       "       9.49847759e+01, 1.63775300e+02, 9.95543094e+01, 1.01413838e+02,\n",
       "       1.58447158e+02, 1.52425258e+02, 5.17034385e+01, 3.08712911e+01,\n",
       "       5.39028711e+00, 9.51146419e+00, 2.58141698e+00, 3.36839305e+00,\n",
       "       2.13196460e+00, 5.91093980e+00, 9.22074012e+00, 1.86075687e+01,\n",
       "       9.56283400e+00])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(dataset_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/grinenko/anna/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_clusters=5\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(254053, 11, 65), dataset_y.shape=(254053, 65)\n",
      "Epoch 1/30\n",
      "2382/2382 [==============================] - 78s 33ms/step - loss: 752.4075 - val_loss: 751.1846\n",
      "Epoch 2/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 750.0954 - val_loss: 748.8649\n",
      "Epoch 3/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 747.8126 - val_loss: 746.5582\n",
      "Epoch 4/30\n",
      "2382/2382 [==============================] - 78s 33ms/step - loss: 745.5405 - val_loss: 744.2713\n",
      "Epoch 5/30\n",
      "2382/2382 [==============================] - 79s 33ms/step - loss: 743.2957 - val_loss: 742.0249\n",
      "Epoch 6/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 741.0747 - val_loss: 739.8112\n",
      "Epoch 7/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 738.8793 - val_loss: 737.6223\n",
      "Epoch 8/30\n",
      "2382/2382 [==============================] - 75s 31ms/step - loss: 736.6967 - val_loss: 735.4496\n",
      "Epoch 9/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 734.5276 - val_loss: 733.2996\n",
      "Epoch 10/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 732.3763 - val_loss: 731.1794\n",
      "Epoch 11/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 730.2576 - val_loss: 729.0906\n",
      "Epoch 12/30\n",
      "2382/2382 [==============================] - 77s 32ms/step - loss: 728.1555 - val_loss: 727.0137\n",
      "Epoch 13/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 726.0626 - val_loss: 724.9455\n",
      "Epoch 14/30\n",
      "2382/2382 [==============================] - 77s 32ms/step - loss: 723.9832 - val_loss: 722.8806\n",
      "Epoch 15/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 721.9130 - val_loss: 720.8231\n",
      "Epoch 16/30\n",
      "2382/2382 [==============================] - 75s 31ms/step - loss: 719.8527 - val_loss: 718.7821\n",
      "Epoch 17/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 717.8083 - val_loss: 716.7634\n",
      "Epoch 18/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 715.7838 - val_loss: 714.7703\n",
      "Epoch 19/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 713.7877 - val_loss: 712.8035\n",
      "Epoch 20/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 711.8184 - val_loss: 710.8679\n",
      "Epoch 21/30\n",
      "2382/2382 [==============================] - 75s 31ms/step - loss: 709.8817 - val_loss: 708.9737\n",
      "Epoch 22/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 707.9745 - val_loss: 707.1259\n",
      "Epoch 23/30\n",
      "2382/2382 [==============================] - 75s 31ms/step - loss: 706.0974 - val_loss: 705.3233\n",
      "Epoch 24/30\n",
      "2382/2382 [==============================] - 77s 32ms/step - loss: 704.2524 - val_loss: 703.5615\n",
      "Epoch 25/30\n",
      "2382/2382 [==============================] - 75s 31ms/step - loss: 702.4373 - val_loss: 701.8328\n",
      "Epoch 26/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 700.6490 - val_loss: 700.1208\n",
      "Epoch 27/30\n",
      "2382/2382 [==============================] - 74s 31ms/step - loss: 698.8848 - val_loss: 698.4172\n",
      "Epoch 28/30\n",
      "2382/2382 [==============================] - 76s 32ms/step - loss: 697.1396 - val_loss: 696.7175\n",
      "Epoch 29/30\n",
      "2382/2382 [==============================] - 75s 32ms/step - loss: 695.4113 - val_loss: 695.0184\n",
      "Epoch 30/30\n",
      "2382/2382 [==============================] - 73s 31ms/step - loss: 693.6959 - val_loss: 693.3242\n",
      "1588/1588 [==============================] - 17s 11ms/step\n",
      "In calc_results: 152432, 50810, 50811, sum = 254053\n",
      "In split_to_train_test: dataset_X.shape=(17245, 11, 65), dataset_y.shape=(17245, 65)\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 5s 33ms/step - loss: 12448304503717888.0000 - val_loss: 12688242851708928.0000\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12448302356234240.0000 - val_loss: 12688242851708928.0000\n",
      "Epoch 3/30\n",
      "161/162 [============================>.] - ETA: 0s - loss: 12448281955139584.0000Restoring model weights from the end of the best epoch: 1.\n",
      "162/162 [==============================] - 5s 30ms/step - loss: 12448302356234240.0000 - val_loss: 12688242851708928.0000\n",
      "Epoch 3: early stopping\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10347, 3449, 3449, sum = 17245\n",
      "In split_to_train_test: dataset_X.shape=(30851, 11, 65), dataset_y.shape=(30851, 65)\n",
      "Epoch 1/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 971.5083 - val_loss: 996.1733\n",
      "Epoch 2/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 971.2666 - val_loss: 995.9323\n",
      "Epoch 3/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 971.0250 - val_loss: 995.6917\n",
      "Epoch 4/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 970.7836 - val_loss: 995.4510\n",
      "Epoch 5/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 970.5419 - val_loss: 995.2104\n",
      "Epoch 6/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 970.3009 - val_loss: 994.9697\n",
      "Epoch 7/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 970.0591 - val_loss: 994.7292\n",
      "Epoch 8/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 969.8178 - val_loss: 994.4889\n",
      "Epoch 9/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 969.5771 - val_loss: 994.2487\n",
      "Epoch 10/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 969.3348 - val_loss: 994.0086\n",
      "Epoch 11/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 969.0936 - val_loss: 993.7686\n",
      "Epoch 12/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 968.8527 - val_loss: 993.5284\n",
      "Epoch 13/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 968.6105 - val_loss: 993.2884\n",
      "Epoch 14/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 968.3695 - val_loss: 993.0482\n",
      "Epoch 15/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 968.1281 - val_loss: 992.8081\n",
      "Epoch 16/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 967.8865 - val_loss: 992.5677\n",
      "Epoch 17/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 967.6458 - val_loss: 992.3276\n",
      "Epoch 18/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 967.4044 - val_loss: 992.0873\n",
      "Epoch 19/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 967.1628 - val_loss: 991.8469\n",
      "Epoch 20/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 966.9217 - val_loss: 991.6068\n",
      "Epoch 21/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 966.6804 - val_loss: 991.3667\n",
      "Epoch 22/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 966.4389 - val_loss: 991.1268\n",
      "Epoch 23/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 966.1974 - val_loss: 990.8870\n",
      "Epoch 24/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 965.9568 - val_loss: 990.6470\n",
      "Epoch 25/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 965.7152 - val_loss: 990.4069\n",
      "Epoch 26/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 965.4745 - val_loss: 990.1669\n",
      "Epoch 27/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 965.2328 - val_loss: 989.9269\n",
      "Epoch 28/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 964.9921 - val_loss: 989.6867\n",
      "Epoch 29/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 964.7508 - val_loss: 989.4464\n",
      "Epoch 30/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 964.5095 - val_loss: 989.2065\n",
      "193/193 [==============================] - 2s 11ms/step\n",
      "In calc_results: 18511, 6170, 6170, sum = 30851\n",
      "In split_to_train_test: dataset_X.shape=(12479, 11, 65), dataset_y.shape=(12479, 65)\n",
      "Epoch 1/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1057.4504 - val_loss: 1067.1666\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1057.3270 - val_loss: 1067.0432\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1057.2037 - val_loss: 1066.9193\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1057.0801 - val_loss: 1066.7957\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.9570 - val_loss: 1066.6721\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.8331 - val_loss: 1066.5482\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.7096 - val_loss: 1066.4247\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.5861 - val_loss: 1066.3011\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1056.4629 - val_loss: 1066.1771\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.3394 - val_loss: 1066.0538\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1056.2158 - val_loss: 1065.9302\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1056.0923 - val_loss: 1065.8062\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1055.9691 - val_loss: 1065.6827\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1055.8457 - val_loss: 1065.5592\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1055.7219 - val_loss: 1065.4352\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1055.5988 - val_loss: 1065.3118\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1055.4751 - val_loss: 1065.1881\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1055.3516 - val_loss: 1065.0645\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1055.2280 - val_loss: 1064.9408\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1055.1051 - val_loss: 1064.8173\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1054.9813 - val_loss: 1064.6935\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1054.8577 - val_loss: 1064.5699\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1054.7345 - val_loss: 1064.4463\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1054.6111 - val_loss: 1064.3226\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1054.4875 - val_loss: 1064.1992\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1054.3643 - val_loss: 1064.0754\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1054.2407 - val_loss: 1063.9518\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1054.1173 - val_loss: 1063.8282\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1053.9938 - val_loss: 1063.7045\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1053.8705 - val_loss: 1063.5809\n",
      "78/78 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7487, 2496, 2496, sum = 12479\n",
      "In split_to_train_test: dataset_X.shape=(11838, 11, 65), dataset_y.shape=(11838, 65)\n",
      "Epoch 1/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 159.9384 - val_loss: 158.1322\n",
      "Epoch 2/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 159.7618 - val_loss: 157.9962\n",
      "Epoch 3/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 159.6146 - val_loss: 157.8676\n",
      "Epoch 4/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 159.4734 - val_loss: 157.7441\n",
      "Epoch 5/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 159.3363 - val_loss: 157.6237\n",
      "Epoch 6/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 159.2011 - val_loss: 157.5048\n",
      "Epoch 7/30\n",
      "111/111 [==============================] - 4s 33ms/step - loss: 159.0664 - val_loss: 157.3874\n",
      "Epoch 8/30\n",
      "111/111 [==============================] - 4s 33ms/step - loss: 158.9325 - val_loss: 157.2704\n",
      "Epoch 9/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 158.7990 - val_loss: 157.1539\n",
      "Epoch 10/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 158.6658 - val_loss: 157.0384\n",
      "Epoch 11/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 158.5328 - val_loss: 156.9235\n",
      "Epoch 12/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 158.4000 - val_loss: 156.8097\n",
      "Epoch 13/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 158.2673 - val_loss: 156.6960\n",
      "Epoch 14/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 158.1115 - val_loss: 156.5556\n",
      "Epoch 15/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 157.9489 - val_loss: 156.4235\n",
      "Epoch 16/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 157.7976 - val_loss: 156.2965\n",
      "Epoch 17/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 157.6505 - val_loss: 156.1737\n",
      "Epoch 18/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 157.5057 - val_loss: 156.0531\n",
      "Epoch 19/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 157.3632 - val_loss: 155.9344\n",
      "Epoch 20/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 157.2224 - val_loss: 155.8188\n",
      "Epoch 21/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 157.0837 - val_loss: 155.7041\n",
      "Epoch 22/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 156.9466 - val_loss: 155.5914\n",
      "Epoch 23/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 156.8105 - val_loss: 155.4785\n",
      "Epoch 24/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 156.6750 - val_loss: 155.3656\n",
      "Epoch 25/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 156.5401 - val_loss: 155.2530\n",
      "Epoch 26/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 156.4052 - val_loss: 155.1409\n",
      "Epoch 27/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 156.2706 - val_loss: 155.0287\n",
      "Epoch 28/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 156.1360 - val_loss: 154.9168\n",
      "Epoch 29/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 156.0016 - val_loss: 154.8051\n",
      "Epoch 30/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 155.8672 - val_loss: 154.6936\n",
      "74/74 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7103, 2367, 2368, sum = 11838\n",
      "N_clusters=7\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(14519, 11, 65), dataset_y.shape=(14519, 65)\n",
      "Epoch 1/30\n",
      "137/137 [==============================] - 5s 34ms/step - loss: 12516280951111680.0000 - val_loss: 12549838604337152.0000\n",
      "Epoch 2/30\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 12516282024853504.0000 - val_loss: 12549838604337152.0000\n",
      "Epoch 3/30\n",
      "137/137 [==============================] - ETA: 0s - loss: 12516282024853504.0000Restoring model weights from the end of the best epoch: 1.\n",
      "137/137 [==============================] - 4s 31ms/step - loss: 12516282024853504.0000 - val_loss: 12549838604337152.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8711, 2904, 2904, sum = 14519\n",
      "In split_to_train_test: dataset_X.shape=(29002, 11, 65), dataset_y.shape=(29002, 65)\n",
      "Epoch 1/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 988.1265 - val_loss: 1012.8270\n",
      "Epoch 2/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 987.6966 - val_loss: 1012.4000\n",
      "Epoch 3/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 987.2670 - val_loss: 1011.9734\n",
      "Epoch 4/30\n",
      "272/272 [==============================] - 9s 31ms/step - loss: 986.8371 - val_loss: 1011.5464\n",
      "Epoch 5/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 986.4071 - val_loss: 1011.1194\n",
      "Epoch 6/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 985.9775 - val_loss: 1010.6932\n",
      "Epoch 7/30\n",
      "272/272 [==============================] - 9s 31ms/step - loss: 985.5483 - val_loss: 1010.2668\n",
      "Epoch 8/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 985.1189 - val_loss: 1009.8402\n",
      "Epoch 9/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 984.6893 - val_loss: 1009.4140\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 8s 30ms/step - loss: 984.2601 - val_loss: 1008.9877\n",
      "Epoch 11/30\n",
      "272/272 [==============================] - 9s 31ms/step - loss: 983.8304 - val_loss: 1008.5612\n",
      "Epoch 12/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 983.4008 - val_loss: 1008.1351\n",
      "Epoch 13/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 982.9714 - val_loss: 1007.7089\n",
      "Epoch 14/30\n",
      "272/272 [==============================] - 9s 34ms/step - loss: 982.5419 - val_loss: 1007.2825\n",
      "Epoch 15/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 982.1132 - val_loss: 1006.8565\n",
      "Epoch 16/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 981.6836 - val_loss: 1006.4305\n",
      "Epoch 17/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 981.2538 - val_loss: 1006.0045\n",
      "Epoch 18/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 980.8256 - val_loss: 1005.5787\n",
      "Epoch 19/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 980.3965 - val_loss: 1005.1527\n",
      "Epoch 20/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 979.9678 - val_loss: 1004.7272\n",
      "Epoch 21/30\n",
      "272/272 [==============================] - 9s 31ms/step - loss: 979.5388 - val_loss: 1004.3013\n",
      "Epoch 22/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 979.1108 - val_loss: 1003.8759\n",
      "Epoch 23/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 978.6824 - val_loss: 1003.4505\n",
      "Epoch 24/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 978.2542 - val_loss: 1003.0255\n",
      "Epoch 25/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 977.8269 - val_loss: 1002.6005\n",
      "Epoch 26/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 977.4031 - val_loss: 1002.1794\n",
      "Epoch 27/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 976.9817 - val_loss: 1001.7587\n",
      "Epoch 28/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 976.5608 - val_loss: 1001.3384\n",
      "Epoch 29/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 976.1409 - val_loss: 1000.9187\n",
      "Epoch 30/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 975.7223 - val_loss: 1000.4995\n",
      "182/182 [==============================] - 2s 11ms/step\n",
      "In calc_results: 17401, 5801, 5800, sum = 29002\n",
      "In split_to_train_test: dataset_X.shape=(2616, 11, 65), dataset_y.shape=(2616, 65)\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48075488794509312.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 48075484499542016.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 48075484499542016.0000Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 48075484499542016.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3: early stopping\n",
      "17/17 [==============================] - 0s 12ms/step\n",
      "In calc_results: 1570, 523, 523, sum = 2616\n",
      "In split_to_train_test: dataset_X.shape=(86204, 11, 65), dataset_y.shape=(86204, 65)\n",
      "Epoch 1/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 579.4363 - val_loss: 576.3672\n",
      "Epoch 2/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 578.1638 - val_loss: 575.1001\n",
      "Epoch 3/30\n",
      "809/809 [==============================] - 25s 30ms/step - loss: 576.8984 - val_loss: 573.8394\n",
      "Epoch 4/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 575.6381 - val_loss: 572.5832\n",
      "Epoch 5/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 574.3803 - val_loss: 571.3309\n",
      "Epoch 6/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 573.1232 - val_loss: 570.0856\n",
      "Epoch 7/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 571.8708 - val_loss: 568.8535\n",
      "Epoch 8/30\n",
      "809/809 [==============================] - 28s 35ms/step - loss: 570.6258 - val_loss: 567.6436\n",
      "Epoch 9/30\n",
      "809/809 [==============================] - 27s 34ms/step - loss: 569.3854 - val_loss: 566.4487\n",
      "Epoch 10/30\n",
      "809/809 [==============================] - 28s 34ms/step - loss: 568.1508 - val_loss: 565.2601\n",
      "Epoch 11/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 566.9182 - val_loss: 564.0786\n",
      "Epoch 12/30\n",
      "809/809 [==============================] - 27s 33ms/step - loss: 565.6881 - val_loss: 562.9026\n",
      "Epoch 13/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 564.4601 - val_loss: 561.7302\n",
      "Epoch 14/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 563.2360 - val_loss: 560.5616\n",
      "Epoch 15/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 562.0177 - val_loss: 559.3996\n",
      "Epoch 16/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 560.8052 - val_loss: 558.2521\n",
      "Epoch 17/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 559.6026 - val_loss: 557.1126\n",
      "Epoch 18/30\n",
      "809/809 [==============================] - 27s 34ms/step - loss: 558.4046 - val_loss: 555.9788\n",
      "Epoch 19/30\n",
      "809/809 [==============================] - 27s 34ms/step - loss: 557.2108 - val_loss: 554.8489\n",
      "Epoch 20/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 556.0219 - val_loss: 553.7218\n",
      "Epoch 21/30\n",
      "809/809 [==============================] - 27s 33ms/step - loss: 554.8375 - val_loss: 552.5991\n",
      "Epoch 22/30\n",
      "809/809 [==============================] - 27s 33ms/step - loss: 553.6573 - val_loss: 551.4783\n",
      "Epoch 23/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 552.4836 - val_loss: 550.3608\n",
      "Epoch 24/30\n",
      "809/809 [==============================] - 26s 33ms/step - loss: 551.3134 - val_loss: 549.2468\n",
      "Epoch 25/30\n",
      "809/809 [==============================] - 28s 34ms/step - loss: 550.1503 - val_loss: 548.1386\n",
      "Epoch 26/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 548.9986 - val_loss: 547.0377\n",
      "Epoch 27/30\n",
      "809/809 [==============================] - 26s 32ms/step - loss: 547.8545 - val_loss: 545.9465\n",
      "Epoch 28/30\n",
      "809/809 [==============================] - 26s 33ms/step - loss: 546.7197 - val_loss: 544.8685\n",
      "Epoch 29/30\n",
      "809/809 [==============================] - 27s 33ms/step - loss: 545.5947 - val_loss: 543.8098\n",
      "Epoch 30/30\n",
      "809/809 [==============================] - 25s 31ms/step - loss: 544.4799 - val_loss: 542.7695\n",
      "539/539 [==============================] - 6s 11ms/step\n",
      "In calc_results: 51722, 17241, 17241, sum = 86204\n",
      "In split_to_train_test: dataset_X.shape=(174055, 11, 65), dataset_y.shape=(174055, 65)\n",
      "Epoch 1/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 839.9536 - val_loss: 839.1617\n",
      "Epoch 2/30\n",
      "1632/1632 [==============================] - 55s 34ms/step - loss: 837.9081 - val_loss: 837.1135\n",
      "Epoch 3/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 835.8923 - val_loss: 835.0760\n",
      "Epoch 4/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 833.8842 - val_loss: 833.0456\n",
      "Epoch 5/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 831.8871 - val_loss: 831.0407\n",
      "Epoch 6/30\n",
      "1632/1632 [==============================] - 51s 32ms/step - loss: 829.9197 - val_loss: 829.0660\n",
      "Epoch 7/30\n",
      "1632/1632 [==============================] - 54s 33ms/step - loss: 827.9762 - val_loss: 827.1169\n",
      "Epoch 8/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 826.0508 - val_loss: 825.1811\n",
      "Epoch 9/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 824.1354 - val_loss: 823.2590\n",
      "Epoch 10/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 822.2289 - val_loss: 821.3524\n",
      "Epoch 11/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 820.3333 - val_loss: 819.4645\n",
      "Epoch 12/30\n",
      "1632/1632 [==============================] - 50s 31ms/step - loss: 818.4565 - val_loss: 817.6033\n",
      "Epoch 13/30\n",
      "1632/1632 [==============================] - 53s 32ms/step - loss: 816.6070 - val_loss: 815.7617\n",
      "Epoch 14/30\n",
      "1632/1632 [==============================] - 51s 32ms/step - loss: 814.7704 - val_loss: 813.9251\n",
      "Epoch 15/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 812.9435 - val_loss: 812.0932\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 50s 31ms/step - loss: 811.1253 - val_loss: 810.2662\n",
      "Epoch 17/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 809.3154 - val_loss: 808.4497\n",
      "Epoch 18/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 807.5130 - val_loss: 806.6494\n",
      "Epoch 19/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 805.7239 - val_loss: 804.8696\n",
      "Epoch 20/30\n",
      "1632/1632 [==============================] - 50s 31ms/step - loss: 803.9500 - val_loss: 803.1099\n",
      "Epoch 21/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 802.1931 - val_loss: 801.3654\n",
      "Epoch 22/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 800.4531 - val_loss: 799.6329\n",
      "Epoch 23/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 798.7291 - val_loss: 797.9188\n",
      "Epoch 24/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 797.0267 - val_loss: 796.2214\n",
      "Epoch 25/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 795.3459 - val_loss: 794.5408\n",
      "Epoch 26/30\n",
      "1632/1632 [==============================] - 54s 33ms/step - loss: 793.6793 - val_loss: 792.8796\n",
      "Epoch 27/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 792.0259 - val_loss: 791.2419\n",
      "Epoch 28/30\n",
      "1632/1632 [==============================] - 52s 32ms/step - loss: 790.3884 - val_loss: 789.6306\n",
      "Epoch 29/30\n",
      "1632/1632 [==============================] - 53s 33ms/step - loss: 788.7671 - val_loss: 788.0451\n",
      "Epoch 30/30\n",
      "1632/1632 [==============================] - 51s 31ms/step - loss: 787.1623 - val_loss: 786.4822\n",
      "1088/1088 [==============================] - 13s 12ms/step\n",
      "In calc_results: 104433, 34811, 34811, sum = 174055\n",
      "In split_to_train_test: dataset_X.shape=(12030, 11, 65), dataset_y.shape=(12030, 65)\n",
      "Epoch 1/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1068.2998 - val_loss: 1077.2795\n",
      "Epoch 2/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1068.1461 - val_loss: 1077.1255\n",
      "Epoch 3/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1067.9924 - val_loss: 1076.9718\n",
      "Epoch 4/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1067.8386 - val_loss: 1076.8179\n",
      "Epoch 5/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1067.6852 - val_loss: 1076.6639\n",
      "Epoch 6/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1067.5315 - val_loss: 1076.5103\n",
      "Epoch 7/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1067.3779 - val_loss: 1076.3564\n",
      "Epoch 8/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1067.2246 - val_loss: 1076.2026\n",
      "Epoch 9/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1067.0708 - val_loss: 1076.0487\n",
      "Epoch 10/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1066.9172 - val_loss: 1075.8949\n",
      "Epoch 11/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1066.7638 - val_loss: 1075.7411\n",
      "Epoch 12/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1066.6101 - val_loss: 1075.5872\n",
      "Epoch 13/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1066.4567 - val_loss: 1075.4332\n",
      "Epoch 14/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1066.3031 - val_loss: 1075.2795\n",
      "Epoch 15/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1066.1493 - val_loss: 1075.1256\n",
      "Epoch 16/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1065.9957 - val_loss: 1074.9718\n",
      "Epoch 17/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1065.8423 - val_loss: 1074.8180\n",
      "Epoch 18/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1065.6881 - val_loss: 1074.6642\n",
      "Epoch 19/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1065.5350 - val_loss: 1074.5103\n",
      "Epoch 20/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1065.3817 - val_loss: 1074.3564\n",
      "Epoch 21/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1065.2277 - val_loss: 1074.2026\n",
      "Epoch 22/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1065.0742 - val_loss: 1074.0490\n",
      "Epoch 23/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1064.9209 - val_loss: 1073.8949\n",
      "Epoch 24/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1064.7668 - val_loss: 1073.7412\n",
      "Epoch 25/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1064.6135 - val_loss: 1073.5874\n",
      "Epoch 26/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1064.4598 - val_loss: 1073.4335\n",
      "Epoch 27/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1064.3064 - val_loss: 1073.2798\n",
      "Epoch 28/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1064.1531 - val_loss: 1073.1261\n",
      "Epoch 29/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1063.9995 - val_loss: 1072.9722\n",
      "Epoch 30/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1063.8461 - val_loss: 1072.8184\n",
      "76/76 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7218, 2406, 2406, sum = 12030\n",
      "In split_to_train_test: dataset_X.shape=(8040, 11, 65), dataset_y.shape=(8040, 65)\n",
      "Epoch 1/30\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 148.3418 - val_loss: 149.3226\n",
      "Epoch 2/30\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 148.2178 - val_loss: 149.2274\n",
      "Epoch 3/30\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 148.1214 - val_loss: 149.1388\n",
      "Epoch 4/30\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 148.0271 - val_loss: 149.0463\n",
      "Epoch 5/30\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 147.9274 - val_loss: 148.9500\n",
      "Epoch 6/30\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 147.8302 - val_loss: 148.8592\n",
      "Epoch 7/30\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 147.7355 - val_loss: 148.7704\n",
      "Epoch 8/30\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 147.6419 - val_loss: 148.6828\n",
      "Epoch 9/30\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 147.5490 - val_loss: 148.5961\n",
      "Epoch 10/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.4568 - val_loss: 148.5104\n",
      "Epoch 11/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.3649 - val_loss: 148.4248\n",
      "Epoch 12/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.2734 - val_loss: 148.3394\n",
      "Epoch 13/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.1822 - val_loss: 148.2546\n",
      "Epoch 14/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.0912 - val_loss: 148.1702\n",
      "Epoch 15/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.0004 - val_loss: 148.0860\n",
      "Epoch 16/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.9096 - val_loss: 148.0019\n",
      "Epoch 17/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.8190 - val_loss: 147.9181\n",
      "Epoch 18/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.7285 - val_loss: 147.8345\n",
      "Epoch 19/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.6381 - val_loss: 147.7511\n",
      "Epoch 20/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.5477 - val_loss: 147.6677\n",
      "Epoch 21/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.4575 - val_loss: 147.5847\n",
      "Epoch 22/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.3678 - val_loss: 147.5023\n",
      "Epoch 23/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.2791 - val_loss: 147.4203\n",
      "Epoch 24/30\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 146.1906 - val_loss: 147.3384\n",
      "Epoch 25/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.1025 - val_loss: 147.2577\n",
      "Epoch 26/30\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 146.0152 - val_loss: 147.1777\n",
      "Epoch 27/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 145.9285 - val_loss: 147.0984\n",
      "Epoch 28/30\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 145.8420 - val_loss: 147.0190\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 2s 33ms/step - loss: 145.7556 - val_loss: 146.9398\n",
      "Epoch 30/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 145.6691 - val_loss: 146.8607\n",
      "51/51 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4824, 1608, 1608, sum = 8040\n",
      "N_clusters=9\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(9813, 11, 65), dataset_y.shape=(9813, 65)\n",
      "Epoch 1/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 152.8392 - val_loss: 184.3333\n",
      "Epoch 2/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.7757 - val_loss: 184.2727\n",
      "Epoch 3/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.7245 - val_loss: 184.2143\n",
      "Epoch 4/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.6735 - val_loss: 184.1583\n",
      "Epoch 5/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.6229 - val_loss: 184.1039\n",
      "Epoch 6/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.5735 - val_loss: 184.0508\n",
      "Epoch 7/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.5259 - val_loss: 183.9985\n",
      "Epoch 8/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.4791 - val_loss: 183.9470\n",
      "Epoch 9/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.4327 - val_loss: 183.8953\n",
      "Epoch 10/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 152.3865 - val_loss: 183.8440\n",
      "Epoch 11/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 152.3405 - val_loss: 183.7923\n",
      "Epoch 12/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 152.2945 - val_loss: 183.7408\n",
      "Epoch 13/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.2487 - val_loss: 183.6889\n",
      "Epoch 14/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 152.2029 - val_loss: 183.6378\n",
      "Epoch 15/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 152.1574 - val_loss: 183.5860\n",
      "Epoch 16/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 152.1120 - val_loss: 183.5343\n",
      "Epoch 17/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.0666 - val_loss: 183.4828\n",
      "Epoch 18/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 152.0213 - val_loss: 183.4313\n",
      "Epoch 19/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.9760 - val_loss: 183.3798\n",
      "Epoch 20/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.9307 - val_loss: 183.3284\n",
      "Epoch 21/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 151.8854 - val_loss: 183.2769\n",
      "Epoch 22/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.8401 - val_loss: 183.2258\n",
      "Epoch 23/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.7950 - val_loss: 183.1737\n",
      "Epoch 24/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.7497 - val_loss: 183.1228\n",
      "Epoch 25/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 151.6989 - val_loss: 183.0726\n",
      "Epoch 26/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 151.6346 - val_loss: 183.0244\n",
      "Epoch 27/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.5846 - val_loss: 182.9758\n",
      "Epoch 28/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.5356 - val_loss: 182.9266\n",
      "Epoch 29/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.4877 - val_loss: 182.8768\n",
      "Epoch 30/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 151.4402 - val_loss: 182.8277\n",
      "62/62 [==============================] - 1s 11ms/step\n",
      "In calc_results: 5888, 1962, 1963, sum = 9813\n",
      "In split_to_train_test: dataset_X.shape=(104548, 11, 65), dataset_y.shape=(104548, 65)\n",
      "Epoch 1/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 925.9062 - val_loss: 925.8873\n",
      "Epoch 2/30\n",
      "981/981 [==============================] - 31s 31ms/step - loss: 924.7536 - val_loss: 924.7314\n",
      "Epoch 3/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 923.6182 - val_loss: 923.5818\n",
      "Epoch 4/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 922.4843 - val_loss: 922.4329\n",
      "Epoch 5/30\n",
      "981/981 [==============================] - 30s 30ms/step - loss: 921.3533 - val_loss: 921.2850\n",
      "Epoch 6/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 920.2230 - val_loss: 920.1380\n",
      "Epoch 7/30\n",
      "981/981 [==============================] - 32s 32ms/step - loss: 919.0948 - val_loss: 918.9927\n",
      "Epoch 8/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 917.9662 - val_loss: 917.8494\n",
      "Epoch 9/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 916.8401 - val_loss: 916.7100\n",
      "Epoch 10/30\n",
      "981/981 [==============================] - 31s 31ms/step - loss: 915.7205 - val_loss: 915.5848\n",
      "Epoch 11/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 914.6129 - val_loss: 914.4700\n",
      "Epoch 12/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 913.5093 - val_loss: 913.3586\n",
      "Epoch 13/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 912.4058 - val_loss: 912.2512\n",
      "Epoch 14/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 911.3067 - val_loss: 911.1494\n",
      "Epoch 15/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 910.2133 - val_loss: 910.0576\n",
      "Epoch 16/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 908.7955 - val_loss: 908.6110\n",
      "Epoch 17/30\n",
      "981/981 [==============================] - 31s 31ms/step - loss: 907.6700 - val_loss: 907.5051\n",
      "Epoch 18/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 906.5662 - val_loss: 906.4061\n",
      "Epoch 19/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 905.4664 - val_loss: 905.3152\n",
      "Epoch 20/30\n",
      "981/981 [==============================] - 33s 34ms/step - loss: 904.3732 - val_loss: 904.2289\n",
      "Epoch 21/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 903.2826 - val_loss: 903.1485\n",
      "Epoch 22/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 902.1959 - val_loss: 902.0729\n",
      "Epoch 23/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 901.1125 - val_loss: 901.0016\n",
      "Epoch 24/30\n",
      "981/981 [==============================] - 32s 33ms/step - loss: 900.0323 - val_loss: 899.9341\n",
      "Epoch 25/30\n",
      "981/981 [==============================] - 30s 31ms/step - loss: 898.9544 - val_loss: 898.8707\n",
      "Epoch 26/30\n",
      "981/981 [==============================] - 31s 31ms/step - loss: 897.8816 - val_loss: 897.8116\n",
      "Epoch 27/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 896.8132 - val_loss: 896.7599\n",
      "Epoch 28/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 895.7566 - val_loss: 895.7177\n",
      "Epoch 29/30\n",
      "981/981 [==============================] - 31s 32ms/step - loss: 894.7055 - val_loss: 894.6772\n",
      "Epoch 30/30\n",
      "981/981 [==============================] - 33s 33ms/step - loss: 893.6570 - val_loss: 893.6384\n",
      "654/654 [==============================] - 8s 11ms/step\n",
      "In calc_results: 62729, 20909, 20910, sum = 104548\n",
      "In split_to_train_test: dataset_X.shape=(18705, 11, 65), dataset_y.shape=(18705, 65)\n",
      "Epoch 1/30\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 330.3059 - val_loss: 327.3319\n",
      "Epoch 2/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 330.0786 - val_loss: 327.1094\n",
      "Epoch 3/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 329.7856 - val_loss: 326.7719\n",
      "Epoch 4/30\n",
      "176/176 [==============================] - 5s 30ms/step - loss: 329.4743 - val_loss: 326.4926\n",
      "Epoch 5/30\n",
      "176/176 [==============================] - 6s 34ms/step - loss: 329.1942 - val_loss: 326.2233\n",
      "Epoch 6/30\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 328.9206 - val_loss: 325.9579\n",
      "Epoch 7/30\n",
      "176/176 [==============================] - 5s 30ms/step - loss: 328.6507 - val_loss: 325.6959\n",
      "Epoch 8/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 328.3586 - val_loss: 325.3931\n",
      "Epoch 9/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 328.0615 - val_loss: 325.1098\n",
      "Epoch 10/30\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 327.7746 - val_loss: 324.8314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 327.4915 - val_loss: 324.5556\n",
      "Epoch 12/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 327.2105 - val_loss: 324.2816\n",
      "Epoch 13/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 326.9310 - val_loss: 324.0091\n",
      "Epoch 14/30\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 326.6526 - val_loss: 323.7373\n",
      "Epoch 15/30\n",
      "176/176 [==============================] - 6s 34ms/step - loss: 326.3746 - val_loss: 323.4663\n",
      "Epoch 16/30\n",
      "176/176 [==============================] - 6s 35ms/step - loss: 326.0973 - val_loss: 323.1960\n",
      "Epoch 17/30\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 325.8206 - val_loss: 322.9261\n",
      "Epoch 18/30\n",
      "176/176 [==============================] - 5s 30ms/step - loss: 325.5441 - val_loss: 322.6565\n",
      "Epoch 19/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 325.2679 - val_loss: 322.3871\n",
      "Epoch 20/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 324.9922 - val_loss: 322.1181\n",
      "Epoch 21/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 324.7167 - val_loss: 321.8493\n",
      "Epoch 22/30\n",
      "176/176 [==============================] - 5s 30ms/step - loss: 324.4414 - val_loss: 321.5809\n",
      "Epoch 23/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 324.1661 - val_loss: 321.3125\n",
      "Epoch 24/30\n",
      "176/176 [==============================] - 5s 30ms/step - loss: 323.8408 - val_loss: 320.9475\n",
      "Epoch 25/30\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 323.5125 - val_loss: 320.6725\n",
      "Epoch 26/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 323.2305 - val_loss: 320.3985\n",
      "Epoch 27/30\n",
      "176/176 [==============================] - 6s 31ms/step - loss: 322.9492 - val_loss: 320.1254\n",
      "Epoch 28/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 322.6686 - val_loss: 319.8528\n",
      "Epoch 29/30\n",
      "176/176 [==============================] - 6s 31ms/step - loss: 322.3888 - val_loss: 319.5807\n",
      "Epoch 30/30\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 322.1094 - val_loss: 319.3092\n",
      "117/117 [==============================] - 1s 11ms/step\n",
      "In calc_results: 11223, 3741, 3741, sum = 18705\n",
      "In split_to_train_test: dataset_X.shape=(73172, 11, 65), dataset_y.shape=(73172, 65)\n",
      "Epoch 1/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 821.6855 - val_loss: 819.3323\n",
      "Epoch 2/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 820.9253 - val_loss: 818.5552\n",
      "Epoch 3/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 820.1494 - val_loss: 817.8038\n",
      "Epoch 4/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 819.4059 - val_loss: 817.0627\n",
      "Epoch 5/30\n",
      "686/686 [==============================] - 22s 31ms/step - loss: 818.6691 - val_loss: 816.3254\n",
      "Epoch 6/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 817.9343 - val_loss: 815.5892\n",
      "Epoch 7/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 817.2026 - val_loss: 814.8544\n",
      "Epoch 8/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 816.4715 - val_loss: 814.1210\n",
      "Epoch 9/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 815.7430 - val_loss: 813.3884\n",
      "Epoch 10/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 815.0167 - val_loss: 812.6590\n",
      "Epoch 11/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 814.2972 - val_loss: 811.9399\n",
      "Epoch 12/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 813.5870 - val_loss: 811.2245\n",
      "Epoch 13/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 812.8807 - val_loss: 810.5152\n",
      "Epoch 14/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 812.1790 - val_loss: 809.8097\n",
      "Epoch 15/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 811.4793 - val_loss: 809.1056\n",
      "Epoch 16/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 810.7809 - val_loss: 808.4030\n",
      "Epoch 17/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 810.0833 - val_loss: 807.7021\n",
      "Epoch 18/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 809.3877 - val_loss: 807.0033\n",
      "Epoch 19/30\n",
      "686/686 [==============================] - 22s 33ms/step - loss: 808.6921 - val_loss: 806.3053\n",
      "Epoch 20/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 807.9985 - val_loss: 805.6091\n",
      "Epoch 21/30\n",
      "686/686 [==============================] - 21s 30ms/step - loss: 807.3058 - val_loss: 804.9153\n",
      "Epoch 22/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 806.6144 - val_loss: 804.2236\n",
      "Epoch 23/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 805.9257 - val_loss: 803.5344\n",
      "Epoch 24/30\n",
      "686/686 [==============================] - 22s 32ms/step - loss: 805.2385 - val_loss: 802.8481\n",
      "Epoch 25/30\n",
      "686/686 [==============================] - 23s 34ms/step - loss: 804.5571 - val_loss: 802.1700\n",
      "Epoch 26/30\n",
      "686/686 [==============================] - 23s 34ms/step - loss: 803.8795 - val_loss: 801.4950\n",
      "Epoch 27/30\n",
      "686/686 [==============================] - 23s 34ms/step - loss: 803.2025 - val_loss: 800.8210\n",
      "Epoch 28/30\n",
      "686/686 [==============================] - 23s 34ms/step - loss: 802.5267 - val_loss: 800.1481\n",
      "Epoch 29/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 801.8519 - val_loss: 799.4762\n",
      "Epoch 30/30\n",
      "686/686 [==============================] - 21s 31ms/step - loss: 801.1784 - val_loss: 798.8043\n",
      "458/458 [==============================] - 5s 11ms/step\n",
      "In calc_results: 43903, 14635, 14634, sum = 73172\n",
      "In split_to_train_test: dataset_X.shape=(14475, 11, 65), dataset_y.shape=(14475, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515616304922624.0000 - val_loss: 12545824957399040.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 12515616304922624.0000 - val_loss: 12545824957399040.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515062254141440.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 12515613083697152.0000 - val_loss: 12545824957399040.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8685, 2895, 2895, sum = 14475\n",
      "In split_to_train_test: dataset_X.shape=(22212, 11, 65), dataset_y.shape=(22212, 65)\n",
      "Epoch 1/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 993.1428 - val_loss: 1014.5627\n",
      "Epoch 2/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 992.8744 - val_loss: 1014.2957\n",
      "Epoch 3/30\n",
      "209/209 [==============================] - 6s 30ms/step - loss: 992.6068 - val_loss: 1014.0288\n",
      "Epoch 4/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 992.3389 - val_loss: 1013.7625\n",
      "Epoch 5/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 992.0719 - val_loss: 1013.4958\n",
      "Epoch 6/30\n",
      "209/209 [==============================] - 7s 34ms/step - loss: 991.8041 - val_loss: 1013.2296\n",
      "Epoch 7/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 991.5364 - val_loss: 1012.9632\n",
      "Epoch 8/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 991.2689 - val_loss: 1012.6970\n",
      "Epoch 9/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 991.0018 - val_loss: 1012.4308\n",
      "Epoch 10/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 990.7343 - val_loss: 1012.1646\n",
      "Epoch 11/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 990.4666 - val_loss: 1011.8986\n",
      "Epoch 12/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 990.2001 - val_loss: 1011.6327\n",
      "Epoch 13/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 989.9322 - val_loss: 1011.3668\n",
      "Epoch 14/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 989.6647 - val_loss: 1011.1008\n",
      "Epoch 15/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 989.3973 - val_loss: 1010.8351\n",
      "Epoch 16/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 989.1298 - val_loss: 1010.5693\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 7s 31ms/step - loss: 988.8624 - val_loss: 1010.3034\n",
      "Epoch 18/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 988.5950 - val_loss: 1010.0374\n",
      "Epoch 19/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 988.3279 - val_loss: 1009.7718\n",
      "Epoch 20/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 988.0605 - val_loss: 1009.5057\n",
      "Epoch 21/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 987.7935 - val_loss: 1009.2401\n",
      "Epoch 22/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 987.5261 - val_loss: 1008.9740\n",
      "Epoch 23/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 987.2587 - val_loss: 1008.7086\n",
      "Epoch 24/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 986.9910 - val_loss: 1008.4426\n",
      "Epoch 25/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 986.7243 - val_loss: 1008.1770\n",
      "Epoch 26/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 986.4570 - val_loss: 1007.9109\n",
      "Epoch 27/30\n",
      "209/209 [==============================] - 7s 31ms/step - loss: 986.1898 - val_loss: 1007.6456\n",
      "Epoch 28/30\n",
      "209/209 [==============================] - 6s 31ms/step - loss: 985.9227 - val_loss: 1007.3795\n",
      "Epoch 29/30\n",
      "209/209 [==============================] - 7s 32ms/step - loss: 985.6553 - val_loss: 1007.1139\n",
      "Epoch 30/30\n",
      "209/209 [==============================] - 7s 33ms/step - loss: 985.3885 - val_loss: 1006.8480\n",
      "139/139 [==============================] - 1s 11ms/step\n",
      "In calc_results: 13327, 4443, 4442, sum = 22212\n",
      "In split_to_train_test: dataset_X.shape=(7289, 11, 65), dataset_y.shape=(7289, 65)\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - 2s 35ms/step - loss: 1077.8505 - val_loss: 1082.9818\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1077.7963 - val_loss: 1082.9279\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1077.7422 - val_loss: 1082.8739\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1077.6884 - val_loss: 1082.8198\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1077.6344 - val_loss: 1082.7662\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1077.5809 - val_loss: 1082.7123\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1077.5271 - val_loss: 1082.6583\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1077.4731 - val_loss: 1082.6044\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 35ms/step - loss: 1077.4194 - val_loss: 1082.5508\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1077.3658 - val_loss: 1082.4967\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 35ms/step - loss: 1077.3116 - val_loss: 1082.4427\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1077.2582 - val_loss: 1082.3889\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1077.2045 - val_loss: 1082.3351\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1077.1508 - val_loss: 1082.2812\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1077.0968 - val_loss: 1082.2273\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1077.0427 - val_loss: 1082.1733\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.9894 - val_loss: 1082.1196\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.9354 - val_loss: 1082.0658\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.8817 - val_loss: 1082.0118\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.8278 - val_loss: 1081.9579\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.7743 - val_loss: 1081.9041\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1076.7206 - val_loss: 1081.8502\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.6666 - val_loss: 1081.7963\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.6129 - val_loss: 1081.7423\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.5590 - val_loss: 1081.6886\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.5052 - val_loss: 1081.6346\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.4515 - val_loss: 1081.5809\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.3981 - val_loss: 1081.5270\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.3440 - val_loss: 1081.4729\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1076.2903 - val_loss: 1081.4193\n",
      "46/46 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4373, 1458, 1458, sum = 7289\n",
      "In split_to_train_test: dataset_X.shape=(11552, 11, 65), dataset_y.shape=(11552, 65)\n",
      "Epoch 1/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1078.0090 - val_loss: 1100.5668\n",
      "Epoch 2/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1077.8793 - val_loss: 1100.4379\n",
      "Epoch 3/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1077.7506 - val_loss: 1100.3091\n",
      "Epoch 4/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1077.6218 - val_loss: 1100.1802\n",
      "Epoch 5/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1077.4929 - val_loss: 1100.0514\n",
      "Epoch 6/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1077.3643 - val_loss: 1099.9225\n",
      "Epoch 7/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1077.2361 - val_loss: 1099.7939\n",
      "Epoch 8/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1077.1069 - val_loss: 1099.6653\n",
      "Epoch 9/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1076.9786 - val_loss: 1099.5366\n",
      "Epoch 10/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1076.8502 - val_loss: 1099.4080\n",
      "Epoch 11/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1076.7212 - val_loss: 1099.2793\n",
      "Epoch 12/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1076.5924 - val_loss: 1099.1506\n",
      "Epoch 13/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1076.4635 - val_loss: 1099.0219\n",
      "Epoch 14/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1076.3353 - val_loss: 1098.8932\n",
      "Epoch 15/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1076.2065 - val_loss: 1098.7646\n",
      "Epoch 16/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1076.0780 - val_loss: 1098.6357\n",
      "Epoch 17/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1075.9493 - val_loss: 1098.5071\n",
      "Epoch 18/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1075.8207 - val_loss: 1098.3785\n",
      "Epoch 19/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1075.6919 - val_loss: 1098.2499\n",
      "Epoch 20/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1075.5632 - val_loss: 1098.1212\n",
      "Epoch 21/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1075.4349 - val_loss: 1097.9927\n",
      "Epoch 22/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1075.3064 - val_loss: 1097.8639\n",
      "Epoch 23/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1075.1775 - val_loss: 1097.7352\n",
      "Epoch 24/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1075.0488 - val_loss: 1097.6066\n",
      "Epoch 25/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1074.9205 - val_loss: 1097.4778\n",
      "Epoch 26/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1074.7919 - val_loss: 1097.3491\n",
      "Epoch 27/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1074.6631 - val_loss: 1097.2205\n",
      "Epoch 28/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1074.5343 - val_loss: 1097.0917\n",
      "Epoch 29/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1074.4058 - val_loss: 1096.9634\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 3s 31ms/step - loss: 1074.2771 - val_loss: 1096.8348\n",
      "73/73 [==============================] - 1s 11ms/step\n",
      "In calc_results: 6931, 2311, 2310, sum = 11552\n",
      "In split_to_train_test: dataset_X.shape=(64700, 11, 65), dataset_y.shape=(64700, 65)\n",
      "Epoch 1/30\n",
      "607/607 [==============================] - 20s 33ms/step - loss: 762.6290 - val_loss: 752.4374\n",
      "Epoch 2/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 761.8779 - val_loss: 751.6946\n",
      "Epoch 3/30\n",
      "607/607 [==============================] - 20s 32ms/step - loss: 761.1356 - val_loss: 750.9523\n",
      "Epoch 4/30\n",
      "607/607 [==============================] - 20s 32ms/step - loss: 760.3941 - val_loss: 750.2095\n",
      "Epoch 5/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 759.6541 - val_loss: 749.4668\n",
      "Epoch 6/30\n",
      "607/607 [==============================] - 18s 30ms/step - loss: 758.9139 - val_loss: 748.7241\n",
      "Epoch 7/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 758.1718 - val_loss: 747.9423\n",
      "Epoch 8/30\n",
      "607/607 [==============================] - 20s 32ms/step - loss: 757.3269 - val_loss: 747.0809\n",
      "Epoch 9/30\n",
      "607/607 [==============================] - 20s 33ms/step - loss: 756.4992 - val_loss: 746.2642\n",
      "Epoch 10/30\n",
      "607/607 [==============================] - 20s 32ms/step - loss: 755.6899 - val_loss: 745.4593\n",
      "Epoch 11/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 754.8892 - val_loss: 744.6613\n",
      "Epoch 12/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 754.0947 - val_loss: 743.8754\n",
      "Epoch 13/30\n",
      "607/607 [==============================] - 19s 32ms/step - loss: 753.3065 - val_loss: 743.0944\n",
      "Epoch 14/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 752.5207 - val_loss: 742.3169\n",
      "Epoch 15/30\n",
      "607/607 [==============================] - 18s 30ms/step - loss: 751.7357 - val_loss: 741.5426\n",
      "Epoch 16/30\n",
      "607/607 [==============================] - 19s 32ms/step - loss: 750.9522 - val_loss: 740.7709\n",
      "Epoch 17/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 750.1700 - val_loss: 740.0038\n",
      "Epoch 18/30\n",
      "607/607 [==============================] - 19s 32ms/step - loss: 749.3694 - val_loss: 739.1710\n",
      "Epoch 19/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 748.5077 - val_loss: 738.3488\n",
      "Epoch 20/30\n",
      "607/607 [==============================] - 19s 32ms/step - loss: 747.6763 - val_loss: 737.5433\n",
      "Epoch 21/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 746.8541 - val_loss: 736.7444\n",
      "Epoch 22/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 746.0366 - val_loss: 735.9495\n",
      "Epoch 23/30\n",
      "607/607 [==============================] - 20s 32ms/step - loss: 745.2234 - val_loss: 735.1577\n",
      "Epoch 24/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 744.4125 - val_loss: 734.3676\n",
      "Epoch 25/30\n",
      "607/607 [==============================] - 20s 34ms/step - loss: 743.6061 - val_loss: 733.5809\n",
      "Epoch 26/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 742.8014 - val_loss: 732.7959\n",
      "Epoch 27/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 741.9982 - val_loss: 732.0125\n",
      "Epoch 28/30\n",
      "607/607 [==============================] - 18s 30ms/step - loss: 741.1975 - val_loss: 731.2308\n",
      "Epoch 29/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 740.3991 - val_loss: 730.4501\n",
      "Epoch 30/30\n",
      "607/607 [==============================] - 19s 31ms/step - loss: 739.6026 - val_loss: 729.6701\n",
      "405/405 [==============================] - 5s 13ms/step\n",
      "In calc_results: 38820, 12940, 12940, sum = 64700\n",
      "N_clusters=11\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(17205, 11, 65), dataset_y.shape=(17205, 65)\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 5s 33ms/step - loss: 12446613360345088.0000 - val_loss: 12689094328975360.0000\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 5s 33ms/step - loss: 12446615507828736.0000 - val_loss: 12689094328975360.0000\n",
      "Epoch 3/30\n",
      "161/162 [============================>.] - ETA: 0s - loss: 12446594032992256.0000Restoring model weights from the end of the best epoch: 1.\n",
      "162/162 [==============================] - 5s 33ms/step - loss: 12446615507828736.0000 - val_loss: 12689094328975360.0000\n",
      "Epoch 3: early stopping\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10323, 3441, 3441, sum = 17205\n",
      "In split_to_train_test: dataset_X.shape=(7776, 11, 65), dataset_y.shape=(7776, 65)\n",
      "Epoch 1/30\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 223.1904 - val_loss: 222.3721\n",
      "Epoch 2/30\n",
      "73/73 [==============================] - 2s 33ms/step - loss: 223.0360 - val_loss: 222.2402\n",
      "Epoch 3/30\n",
      "73/73 [==============================] - 2s 33ms/step - loss: 222.9025 - val_loss: 222.1238\n",
      "Epoch 4/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.7864 - val_loss: 222.0076\n",
      "Epoch 5/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.6705 - val_loss: 221.8918\n",
      "Epoch 6/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.5548 - val_loss: 221.7763\n",
      "Epoch 7/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.4396 - val_loss: 221.6608\n",
      "Epoch 8/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.3244 - val_loss: 221.5455\n",
      "Epoch 9/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.2096 - val_loss: 221.4304\n",
      "Epoch 10/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 222.0948 - val_loss: 221.3161\n",
      "Epoch 11/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 221.9800 - val_loss: 221.2018\n",
      "Epoch 12/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 221.8654 - val_loss: 221.0875\n",
      "Epoch 13/30\n",
      "73/73 [==============================] - 2s 34ms/step - loss: 221.7507 - val_loss: 220.9733\n",
      "Epoch 14/30\n",
      "73/73 [==============================] - 3s 35ms/step - loss: 221.6361 - val_loss: 220.8592\n",
      "Epoch 15/30\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 221.5216 - val_loss: 220.7450\n",
      "Epoch 16/30\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 221.4071 - val_loss: 220.6309\n",
      "Epoch 17/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 221.2926 - val_loss: 220.5168\n",
      "Epoch 18/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 221.1781 - val_loss: 220.4027\n",
      "Epoch 19/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 221.0636 - val_loss: 220.2887\n",
      "Epoch 20/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.9493 - val_loss: 220.1748\n",
      "Epoch 21/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.8349 - val_loss: 220.0610\n",
      "Epoch 22/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.7205 - val_loss: 219.9471\n",
      "Epoch 23/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.6062 - val_loss: 219.8334\n",
      "Epoch 24/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.4919 - val_loss: 219.7196\n",
      "Epoch 25/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.3776 - val_loss: 219.6059\n",
      "Epoch 26/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.2571 - val_loss: 219.4698\n",
      "Epoch 27/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 220.1197 - val_loss: 219.3382\n",
      "Epoch 28/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 219.9904 - val_loss: 219.2119\n",
      "Epoch 29/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 219.8645 - val_loss: 219.0880\n",
      "Epoch 30/30\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 219.7405 - val_loss: 218.9656\n",
      "49/49 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4666, 1555, 1555, sum = 7776\n",
      "In split_to_train_test: dataset_X.shape=(7423, 11, 65), dataset_y.shape=(7423, 65)\n",
      "Epoch 1/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1169.2721 - val_loss: 1195.1250\n",
      "Epoch 2/30\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1169.1908 - val_loss: 1195.0438\n",
      "Epoch 3/30\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1169.1097 - val_loss: 1194.9625\n",
      "Epoch 4/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1169.0291 - val_loss: 1194.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.9479 - val_loss: 1194.8003\n",
      "Epoch 6/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.8669 - val_loss: 1194.7195\n",
      "Epoch 7/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.7861 - val_loss: 1194.6384\n",
      "Epoch 8/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.7056 - val_loss: 1194.5575\n",
      "Epoch 9/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.6248 - val_loss: 1194.4763\n",
      "Epoch 10/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.5436 - val_loss: 1194.3955\n",
      "Epoch 11/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.4630 - val_loss: 1194.3145\n",
      "Epoch 12/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1168.3818 - val_loss: 1194.2334\n",
      "Epoch 13/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.3013 - val_loss: 1194.1523\n",
      "Epoch 14/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.2205 - val_loss: 1194.0717\n",
      "Epoch 15/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.1395 - val_loss: 1193.9906\n",
      "Epoch 16/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1168.0586 - val_loss: 1193.9095\n",
      "Epoch 17/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1167.9780 - val_loss: 1193.8285\n",
      "Epoch 18/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1167.8971 - val_loss: 1193.7476\n",
      "Epoch 19/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1167.8163 - val_loss: 1193.6665\n",
      "Epoch 20/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1167.7354 - val_loss: 1193.5857\n",
      "Epoch 21/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1167.6544 - val_loss: 1193.5048\n",
      "Epoch 22/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1167.5736 - val_loss: 1193.4236\n",
      "Epoch 23/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1167.4930 - val_loss: 1193.3427\n",
      "Epoch 24/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1167.4117 - val_loss: 1193.2616\n",
      "Epoch 25/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1167.3315 - val_loss: 1193.1808\n",
      "Epoch 26/30\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 1167.2507 - val_loss: 1193.0997\n",
      "Epoch 27/30\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 1167.1698 - val_loss: 1193.0189\n",
      "Epoch 28/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1167.0887 - val_loss: 1192.9379\n",
      "Epoch 29/30\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 1167.0079 - val_loss: 1192.8569\n",
      "Epoch 30/30\n",
      "70/70 [==============================] - 2s 36ms/step - loss: 1166.9274 - val_loss: 1192.7760\n",
      "47/47 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4454, 1484, 1485, sum = 7423\n",
      "In split_to_train_test: dataset_X.shape=(18876, 11, 65), dataset_y.shape=(18876, 65)\n",
      "Epoch 1/30\n",
      "177/177 [==============================] - 6s 33ms/step - loss: 971.0757 - val_loss: 990.1373\n",
      "Epoch 2/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 970.8531 - val_loss: 989.9152\n",
      "Epoch 3/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 970.6317 - val_loss: 989.6931\n",
      "Epoch 4/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 970.4093 - val_loss: 989.4711\n",
      "Epoch 5/30\n",
      "177/177 [==============================] - 6s 31ms/step - loss: 970.1877 - val_loss: 989.2488\n",
      "Epoch 6/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 969.9659 - val_loss: 989.0268\n",
      "Epoch 7/30\n",
      "177/177 [==============================] - 6s 31ms/step - loss: 969.7438 - val_loss: 988.8048\n",
      "Epoch 8/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 969.5223 - val_loss: 988.5825\n",
      "Epoch 9/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 969.3003 - val_loss: 988.3604\n",
      "Epoch 10/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 969.0786 - val_loss: 988.1381\n",
      "Epoch 11/30\n",
      "177/177 [==============================] - 6s 33ms/step - loss: 968.8568 - val_loss: 987.9158\n",
      "Epoch 12/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 968.6354 - val_loss: 987.6937\n",
      "Epoch 13/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 968.4135 - val_loss: 987.4713\n",
      "Epoch 14/30\n",
      "177/177 [==============================] - 6s 33ms/step - loss: 968.1909 - val_loss: 987.2491\n",
      "Epoch 15/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 967.9689 - val_loss: 987.0268\n",
      "Epoch 16/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 967.7475 - val_loss: 986.8047\n",
      "Epoch 17/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 967.5256 - val_loss: 986.5826\n",
      "Epoch 18/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 967.3035 - val_loss: 986.3602\n",
      "Epoch 19/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 967.0822 - val_loss: 986.1381\n",
      "Epoch 20/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 966.8604 - val_loss: 985.9158\n",
      "Epoch 21/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 966.6388 - val_loss: 985.6938\n",
      "Epoch 22/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 966.4169 - val_loss: 985.4716\n",
      "Epoch 23/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 966.1951 - val_loss: 985.2495\n",
      "Epoch 24/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 965.9731 - val_loss: 985.0273\n",
      "Epoch 25/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 965.7513 - val_loss: 984.8051\n",
      "Epoch 26/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 965.5297 - val_loss: 984.5826\n",
      "Epoch 27/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 965.3082 - val_loss: 984.3605\n",
      "Epoch 28/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 965.0865 - val_loss: 984.1384\n",
      "Epoch 29/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 964.8650 - val_loss: 983.9160\n",
      "Epoch 30/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 964.6437 - val_loss: 983.6939\n",
      "118/118 [==============================] - 1s 11ms/step\n",
      "In calc_results: 11326, 3775, 3775, sum = 18876\n",
      "In split_to_train_test: dataset_X.shape=(84810, 11, 65), dataset_y.shape=(84810, 65)\n",
      "Epoch 1/30\n",
      "796/796 [==============================] - 26s 33ms/step - loss: 901.0403 - val_loss: 900.4578\n",
      "Epoch 2/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 899.8322 - val_loss: 899.2421\n",
      "Epoch 3/30\n",
      "796/796 [==============================] - 24s 31ms/step - loss: 898.6389 - val_loss: 898.0320\n",
      "Epoch 4/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 897.4496 - val_loss: 896.8243\n",
      "Epoch 5/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 896.2612 - val_loss: 895.6180\n",
      "Epoch 6/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 895.0756 - val_loss: 894.4134\n",
      "Epoch 7/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 893.8902 - val_loss: 893.2113\n",
      "Epoch 8/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 892.7072 - val_loss: 892.0132\n",
      "Epoch 9/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 891.5282 - val_loss: 890.8221\n",
      "Epoch 10/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 890.3567 - val_loss: 889.6475\n",
      "Epoch 11/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 889.1642 - val_loss: 888.3851\n",
      "Epoch 12/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 887.9027 - val_loss: 887.1469\n",
      "Epoch 13/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 886.6832 - val_loss: 885.9374\n",
      "Epoch 14/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 885.4813 - val_loss: 884.7403\n",
      "Epoch 15/30\n",
      "796/796 [==============================] - 24s 31ms/step - loss: 884.2878 - val_loss: 883.5518\n",
      "Epoch 16/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 883.0978 - val_loss: 882.3708\n",
      "Epoch 17/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 881.9125 - val_loss: 881.1972\n",
      "Epoch 18/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 880.7308 - val_loss: 880.0289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 879.5536 - val_loss: 878.8660\n",
      "Epoch 20/30\n",
      "796/796 [==============================] - 26s 33ms/step - loss: 878.3804 - val_loss: 877.7086\n",
      "Epoch 21/30\n",
      "796/796 [==============================] - 24s 31ms/step - loss: 877.2099 - val_loss: 876.5549\n",
      "Epoch 22/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 876.0428 - val_loss: 875.4045\n",
      "Epoch 23/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 874.8776 - val_loss: 874.2585\n",
      "Epoch 24/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 873.7183 - val_loss: 873.1183\n",
      "Epoch 25/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 872.5695 - val_loss: 871.9904\n",
      "Epoch 26/30\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 871.4323 - val_loss: 870.8683\n",
      "Epoch 27/30\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 870.3004 - val_loss: 869.7485\n",
      "Epoch 28/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 869.1702 - val_loss: 868.6314\n",
      "Epoch 29/30\n",
      "796/796 [==============================] - 26s 33ms/step - loss: 868.0452 - val_loss: 867.5189\n",
      "Epoch 30/30\n",
      "796/796 [==============================] - 25s 31ms/step - loss: 866.9224 - val_loss: 866.4122\n",
      "531/531 [==============================] - 6s 11ms/step\n",
      "In calc_results: 50886, 16962, 16962, sum = 84810\n",
      "In split_to_train_test: dataset_X.shape=(3929, 11, 65), dataset_y.shape=(3929, 65)\n",
      "Epoch 1/30\n",
      "37/37 [==============================] - 1s 37ms/step - loss: 1075.7550 - val_loss: 1071.4288\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1075.6930 - val_loss: 1071.3674\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.6316 - val_loss: 1071.3062\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1075.5702 - val_loss: 1071.2448\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1075.5088 - val_loss: 1071.1835\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.4476 - val_loss: 1071.1221\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1075.3864 - val_loss: 1071.0608\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.3250 - val_loss: 1070.9994\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.2638 - val_loss: 1070.9382\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.2023 - val_loss: 1070.8767\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.1411 - val_loss: 1070.8154\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.0797 - val_loss: 1070.7542\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1075.0188 - val_loss: 1070.6930\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1074.9573 - val_loss: 1070.6317\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1074.8961 - val_loss: 1070.5703\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1074.8346 - val_loss: 1070.5090\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1074.7738 - val_loss: 1070.4478\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1074.7124 - val_loss: 1070.3865\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1074.6509 - val_loss: 1070.3252\n",
      "Epoch 20/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1074.5897 - val_loss: 1070.2639\n",
      "Epoch 21/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1074.5286 - val_loss: 1070.2024\n",
      "Epoch 22/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1074.4674 - val_loss: 1070.1414\n",
      "Epoch 23/30\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1074.4060 - val_loss: 1070.0800\n",
      "Epoch 24/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1074.3448 - val_loss: 1070.0186\n",
      "Epoch 25/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1074.2834 - val_loss: 1069.9573\n",
      "Epoch 26/30\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1074.2222 - val_loss: 1069.8960\n",
      "Epoch 27/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1074.1609 - val_loss: 1069.8347\n",
      "Epoch 28/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1074.0997 - val_loss: 1069.7734\n",
      "Epoch 29/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1074.0385 - val_loss: 1069.7122\n",
      "Epoch 30/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1073.9769 - val_loss: 1069.6506\n",
      "25/25 [==============================] - 0s 12ms/step\n",
      "In calc_results: 2357, 786, 786, sum = 3929\n",
      "In split_to_train_test: dataset_X.shape=(12072, 11, 65), dataset_y.shape=(12072, 65)\n",
      "Epoch 1/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1004.5522 - val_loss: 1030.4340\n",
      "Epoch 2/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1004.4294 - val_loss: 1030.3107\n",
      "Epoch 3/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1004.3059 - val_loss: 1030.1880\n",
      "Epoch 4/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1004.1825 - val_loss: 1030.0651\n",
      "Epoch 5/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1004.0593 - val_loss: 1029.9421\n",
      "Epoch 6/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1003.9365 - val_loss: 1029.8192\n",
      "Epoch 7/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1003.8134 - val_loss: 1029.6964\n",
      "Epoch 8/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1003.6902 - val_loss: 1029.5732\n",
      "Epoch 9/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1003.5671 - val_loss: 1029.4506\n",
      "Epoch 10/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1003.4442 - val_loss: 1029.3276\n",
      "Epoch 11/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1003.3208 - val_loss: 1029.2047\n",
      "Epoch 12/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1003.1978 - val_loss: 1029.0820\n",
      "Epoch 13/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1003.0748 - val_loss: 1028.9594\n",
      "Epoch 14/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1002.9515 - val_loss: 1028.8362\n",
      "Epoch 15/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1002.8282 - val_loss: 1028.7134\n",
      "Epoch 16/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1002.7051 - val_loss: 1028.5906\n",
      "Epoch 17/30\n",
      "114/114 [==============================] - 4s 35ms/step - loss: 1002.5823 - val_loss: 1028.4677\n",
      "Epoch 18/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1002.4592 - val_loss: 1028.3450\n",
      "Epoch 19/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1002.3360 - val_loss: 1028.2223\n",
      "Epoch 20/30\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 1002.2130 - val_loss: 1028.0996\n",
      "Epoch 21/30\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 1002.0896 - val_loss: 1027.9766\n",
      "Epoch 22/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1001.9669 - val_loss: 1027.8540\n",
      "Epoch 23/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1001.8434 - val_loss: 1027.7313\n",
      "Epoch 24/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1001.7206 - val_loss: 1027.6085\n",
      "Epoch 25/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1001.5977 - val_loss: 1027.4857\n",
      "Epoch 26/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1001.4745 - val_loss: 1027.3630\n",
      "Epoch 27/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1001.3513 - val_loss: 1027.2402\n",
      "Epoch 28/30\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 1001.2281 - val_loss: 1027.1176\n",
      "Epoch 29/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1001.1053 - val_loss: 1026.9949\n",
      "Epoch 30/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1000.9824 - val_loss: 1026.8722\n",
      "76/76 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7243, 2415, 2414, sum = 12072\n",
      "In split_to_train_test: dataset_X.shape=(7077, 11, 65), dataset_y.shape=(7077, 65)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "67/67 [==============================] - 2s 33ms/step - loss: 164.2197 - val_loss: 165.2754\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 164.1194 - val_loss: 165.1923\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 164.0140 - val_loss: 165.1098\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.9347 - val_loss: 165.0357\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.8604 - val_loss: 164.9646\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.7889 - val_loss: 164.8953\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.7182 - val_loss: 164.8265\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.6479 - val_loss: 164.7580\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 163.5781 - val_loss: 164.6898\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 163.5084 - val_loss: 164.6221\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.4389 - val_loss: 164.5546\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.3694 - val_loss: 164.4874\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 163.3002 - val_loss: 164.4206\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 163.2309 - val_loss: 164.3545\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 163.1618 - val_loss: 164.2890\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 2s 32ms/step - loss: 163.0929 - val_loss: 164.2239\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 163.0240 - val_loss: 164.1591\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 2s 34ms/step - loss: 162.9552 - val_loss: 164.0948\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.8867 - val_loss: 164.0309\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.8185 - val_loss: 163.9671\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.7503 - val_loss: 163.9033\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.6822 - val_loss: 163.8399\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.6142 - val_loss: 163.7766\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.5461 - val_loss: 163.7133\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.4780 - val_loss: 163.6503\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.4100 - val_loss: 163.5872\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.3419 - val_loss: 163.5242\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.2744 - val_loss: 163.4622\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.2076 - val_loss: 163.4006\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 2s 31ms/step - loss: 162.1408 - val_loss: 163.3398\n",
      "45/45 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4246, 1416, 1415, sum = 7077\n",
      "In split_to_train_test: dataset_X.shape=(74852, 11, 65), dataset_y.shape=(74852, 65)\n",
      "Epoch 1/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 884.7137 - val_loss: 878.7219\n",
      "Epoch 2/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 884.1246 - val_loss: 878.1275\n",
      "Epoch 3/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 883.5375 - val_loss: 877.5334\n",
      "Epoch 4/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 882.9486 - val_loss: 876.9391\n",
      "Epoch 5/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 882.3620 - val_loss: 876.3453\n",
      "Epoch 6/30\n",
      "702/702 [==============================] - 21s 31ms/step - loss: 881.7730 - val_loss: 875.7515\n",
      "Epoch 7/30\n",
      "702/702 [==============================] - 22s 32ms/step - loss: 881.1850 - val_loss: 875.1579\n",
      "Epoch 8/30\n",
      "702/702 [==============================] - 22s 32ms/step - loss: 880.5978 - val_loss: 874.5643\n",
      "Epoch 9/30\n",
      "702/702 [==============================] - 22s 32ms/step - loss: 880.0104 - val_loss: 873.9718\n",
      "Epoch 10/30\n",
      "702/702 [==============================] - 21s 31ms/step - loss: 879.4232 - val_loss: 873.3788\n",
      "Epoch 11/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 878.8354 - val_loss: 872.7863\n",
      "Epoch 12/30\n",
      "702/702 [==============================] - 22s 32ms/step - loss: 878.2496 - val_loss: 872.1950\n",
      "Epoch 13/30\n",
      "702/702 [==============================] - 23s 32ms/step - loss: 877.6619 - val_loss: 871.6037\n",
      "Epoch 14/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 877.0762 - val_loss: 871.0130\n",
      "Epoch 15/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 876.4907 - val_loss: 870.4235\n",
      "Epoch 16/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 875.9050 - val_loss: 869.8347\n",
      "Epoch 17/30\n",
      "702/702 [==============================] - 23s 32ms/step - loss: 875.3204 - val_loss: 869.2475\n",
      "Epoch 18/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 874.7371 - val_loss: 868.6624\n",
      "Epoch 19/30\n",
      "702/702 [==============================] - 23s 33ms/step - loss: 874.1555 - val_loss: 868.0834\n",
      "Epoch 20/30\n",
      "702/702 [==============================] - 23s 32ms/step - loss: 873.5797 - val_loss: 867.5069\n",
      "Epoch 21/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 873.0046 - val_loss: 866.9319\n",
      "Epoch 22/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 872.4294 - val_loss: 866.3584\n",
      "Epoch 23/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 871.8560 - val_loss: 865.7865\n",
      "Epoch 24/30\n",
      "702/702 [==============================] - 21s 30ms/step - loss: 871.2818 - val_loss: 865.2158\n",
      "Epoch 25/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 870.7098 - val_loss: 864.6469\n",
      "Epoch 26/30\n",
      "702/702 [==============================] - 24s 34ms/step - loss: 870.1373 - val_loss: 864.0793\n",
      "Epoch 27/30\n",
      "702/702 [==============================] - 23s 32ms/step - loss: 869.5665 - val_loss: 863.5135\n",
      "Epoch 28/30\n",
      "702/702 [==============================] - 22s 31ms/step - loss: 868.9961 - val_loss: 862.9506\n",
      "Epoch 29/30\n",
      "702/702 [==============================] - 23s 33ms/step - loss: 868.4286 - val_loss: 862.3912\n",
      "Epoch 30/30\n",
      "702/702 [==============================] - 23s 32ms/step - loss: 867.8650 - val_loss: 861.8339\n",
      "468/468 [==============================] - 5s 10ms/step\n",
      "In calc_results: 44911, 14971, 14970, sum = 74852\n",
      "In split_to_train_test: dataset_X.shape=(66093, 11, 65), dataset_y.shape=(66093, 65)\n",
      "Epoch 1/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 827.4998 - val_loss: 824.8449\n",
      "Epoch 2/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 826.5323 - val_loss: 823.8353\n",
      "Epoch 3/30\n",
      "620/620 [==============================] - 19s 30ms/step - loss: 825.5648 - val_loss: 822.8767\n",
      "Epoch 4/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 824.6168 - val_loss: 821.9252\n",
      "Epoch 5/30\n",
      "620/620 [==============================] - 20s 33ms/step - loss: 823.6749 - val_loss: 820.9774\n",
      "Epoch 6/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 822.7368 - val_loss: 820.0323\n",
      "Epoch 7/30\n",
      "620/620 [==============================] - 21s 33ms/step - loss: 821.8022 - val_loss: 819.0900\n",
      "Epoch 8/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 820.8723 - val_loss: 818.1563\n",
      "Epoch 9/30\n",
      "620/620 [==============================] - 19s 30ms/step - loss: 819.9480 - val_loss: 817.2277\n",
      "Epoch 10/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 819.0327 - val_loss: 816.3063\n",
      "Epoch 11/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 818.1251 - val_loss: 815.3915\n",
      "Epoch 12/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 817.2250 - val_loss: 814.4861\n",
      "Epoch 13/30\n",
      "620/620 [==============================] - 21s 33ms/step - loss: 816.3277 - val_loss: 813.5828\n",
      "Epoch 14/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 815.4332 - val_loss: 812.6818\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 19s 31ms/step - loss: 814.5396 - val_loss: 811.7836\n",
      "Epoch 16/30\n",
      "620/620 [==============================] - 20s 33ms/step - loss: 813.6499 - val_loss: 810.8901\n",
      "Epoch 17/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 812.7614 - val_loss: 809.9977\n",
      "Epoch 18/30\n",
      "620/620 [==============================] - 19s 30ms/step - loss: 811.8759 - val_loss: 809.1097\n",
      "Epoch 19/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 810.9940 - val_loss: 808.2285\n",
      "Epoch 20/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 810.1207 - val_loss: 807.3602\n",
      "Epoch 21/30\n",
      "620/620 [==============================] - 21s 33ms/step - loss: 809.2518 - val_loss: 806.4955\n",
      "Epoch 22/30\n",
      "620/620 [==============================] - 20s 33ms/step - loss: 808.3842 - val_loss: 805.6334\n",
      "Epoch 23/30\n",
      "620/620 [==============================] - 21s 33ms/step - loss: 807.5190 - val_loss: 804.7732\n",
      "Epoch 24/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 806.6553 - val_loss: 803.9133\n",
      "Epoch 25/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 805.7947 - val_loss: 803.0543\n",
      "Epoch 26/30\n",
      "620/620 [==============================] - 19s 30ms/step - loss: 804.9362 - val_loss: 802.1946\n",
      "Epoch 27/30\n",
      "620/620 [==============================] - 19s 31ms/step - loss: 804.0790 - val_loss: 801.3354\n",
      "Epoch 28/30\n",
      "620/620 [==============================] - 19s 30ms/step - loss: 803.2249 - val_loss: 800.4775\n",
      "Epoch 29/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 802.3724 - val_loss: 799.6194\n",
      "Epoch 30/30\n",
      "620/620 [==============================] - 20s 32ms/step - loss: 801.5225 - val_loss: 798.7627\n",
      "414/414 [==============================] - 4s 11ms/step\n",
      "In calc_results: 39656, 13218, 13219, sum = 66093\n",
      "In split_to_train_test: dataset_X.shape=(26353, 11, 65), dataset_y.shape=(26353, 65)\n",
      "Epoch 1/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 659.3926 - val_loss: 659.0511\n",
      "Epoch 2/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 658.9967 - val_loss: 658.6555\n",
      "Epoch 3/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 658.6042 - val_loss: 658.2604\n",
      "Epoch 4/30\n",
      "248/248 [==============================] - 8s 30ms/step - loss: 658.2119 - val_loss: 657.8652\n",
      "Epoch 5/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 657.8198 - val_loss: 657.4702\n",
      "Epoch 6/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 657.4278 - val_loss: 657.0760\n",
      "Epoch 7/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 657.0372 - val_loss: 656.6825\n",
      "Epoch 8/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 656.6471 - val_loss: 656.2887\n",
      "Epoch 9/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 656.2562 - val_loss: 655.8948\n",
      "Epoch 10/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 655.8661 - val_loss: 655.5007\n",
      "Epoch 11/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 655.4757 - val_loss: 655.1064\n",
      "Epoch 12/30\n",
      "248/248 [==============================] - 8s 30ms/step - loss: 655.0851 - val_loss: 654.7126\n",
      "Epoch 13/30\n",
      "248/248 [==============================] - 8s 33ms/step - loss: 654.6947 - val_loss: 654.3185\n",
      "Epoch 14/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 654.3038 - val_loss: 653.9247\n",
      "Epoch 15/30\n",
      "248/248 [==============================] - 8s 33ms/step - loss: 653.9136 - val_loss: 653.5309\n",
      "Epoch 16/30\n",
      "248/248 [==============================] - 8s 30ms/step - loss: 653.5232 - val_loss: 653.1380\n",
      "Epoch 17/30\n",
      "248/248 [==============================] - 8s 33ms/step - loss: 653.1328 - val_loss: 652.7460\n",
      "Epoch 18/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 652.7427 - val_loss: 652.3540\n",
      "Epoch 19/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 652.3527 - val_loss: 651.9621\n",
      "Epoch 20/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 651.9625 - val_loss: 651.5705\n",
      "Epoch 21/30\n",
      "248/248 [==============================] - 8s 32ms/step - loss: 651.5723 - val_loss: 651.1796\n",
      "Epoch 22/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 651.1826 - val_loss: 650.7886\n",
      "Epoch 23/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 650.7923 - val_loss: 650.3986\n",
      "Epoch 24/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 650.4025 - val_loss: 650.0087\n",
      "Epoch 25/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 650.0124 - val_loss: 649.6193\n",
      "Epoch 26/30\n",
      "248/248 [==============================] - 8s 32ms/step - loss: 649.6227 - val_loss: 649.2301\n",
      "Epoch 27/30\n",
      "248/248 [==============================] - 8s 32ms/step - loss: 649.2331 - val_loss: 648.8414\n",
      "Epoch 28/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 648.8433 - val_loss: 648.4531\n",
      "Epoch 29/30\n",
      "248/248 [==============================] - 8s 31ms/step - loss: 648.4538 - val_loss: 648.0660\n",
      "Epoch 30/30\n",
      "248/248 [==============================] - 7s 30ms/step - loss: 648.0648 - val_loss: 647.6792\n",
      "165/165 [==============================] - 2s 11ms/step\n",
      "In calc_results: 15812, 5270, 5271, sum = 26353\n",
      "N_clusters=5\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(253887, 11, 65), dataset_y.shape=(253887, 65)\n",
      "Epoch 1/30\n",
      "2381/2381 [==============================] - 73s 30ms/step - loss: 730.5591 - val_loss: 728.7690\n",
      "Epoch 2/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 727.2821 - val_loss: 725.4987\n",
      "Epoch 3/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 724.0735 - val_loss: 722.2841\n",
      "Epoch 4/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 720.9225 - val_loss: 719.1393\n",
      "Epoch 5/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 717.8177 - val_loss: 716.0477\n",
      "Epoch 6/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 714.7419 - val_loss: 712.9963\n",
      "Epoch 7/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 711.6979 - val_loss: 710.0135\n",
      "Epoch 8/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 708.7078 - val_loss: 707.0733\n",
      "Epoch 9/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 705.7448 - val_loss: 704.1518\n",
      "Epoch 10/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 702.8065 - val_loss: 701.2399\n",
      "Epoch 11/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 699.8900 - val_loss: 698.3488\n",
      "Epoch 12/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 696.9999 - val_loss: 695.5010\n",
      "Epoch 13/30\n",
      "2381/2381 [==============================] - 73s 31ms/step - loss: 694.1533 - val_loss: 692.7018\n",
      "Epoch 14/30\n",
      "2381/2381 [==============================] - 75s 31ms/step - loss: 691.3575 - val_loss: 689.9589\n",
      "Epoch 15/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 688.6198 - val_loss: 687.2942\n",
      "Epoch 16/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 685.9442 - val_loss: 684.7234\n",
      "Epoch 17/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 683.3262 - val_loss: 682.2319\n",
      "Epoch 18/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 680.7686 - val_loss: 679.7946\n",
      "Epoch 19/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 678.2617 - val_loss: 677.3828\n",
      "Epoch 20/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 675.7955 - val_loss: 674.9795\n",
      "Epoch 21/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 673.3609 - val_loss: 672.5833\n",
      "Epoch 22/30\n",
      "2381/2381 [==============================] - 73s 31ms/step - loss: 670.9539 - val_loss: 670.2082\n",
      "Epoch 23/30\n",
      "2381/2381 [==============================] - 77s 33ms/step - loss: 668.5782 - val_loss: 667.8553\n",
      "Epoch 24/30\n",
      "2381/2381 [==============================] - 79s 33ms/step - loss: 666.2347 - val_loss: 665.5408\n",
      "Epoch 25/30\n",
      "2381/2381 [==============================] - 75s 31ms/step - loss: 663.9248 - val_loss: 663.2554\n",
      "Epoch 26/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 661.6427 - val_loss: 660.9973\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381/2381 [==============================] - 75s 31ms/step - loss: 659.3871 - val_loss: 658.7567\n",
      "Epoch 28/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 657.1614 - val_loss: 656.5402\n",
      "Epoch 29/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 654.9732 - val_loss: 654.3547\n",
      "Epoch 30/30\n",
      "2381/2381 [==============================] - 77s 33ms/step - loss: 652.8134 - val_loss: 652.1969\n",
      "1587/1587 [==============================] - 17s 10ms/step\n",
      "In calc_results: 152332, 50778, 50777, sum = 253887\n",
      "In split_to_train_test: dataset_X.shape=(17256, 11, 65), dataset_y.shape=(17256, 65)\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 5s 32ms/step - loss: 12448663133487104.0000 - val_loss: 12688444715171840.0000\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12448662059745280.0000 - val_loss: 12688444715171840.0000\n",
      "Epoch 3/30\n",
      "161/162 [============================>.] - ETA: 0s - loss: 12448669575938048.0000Restoring model weights from the end of the best epoch: 1.\n",
      "162/162 [==============================] - 5s 32ms/step - loss: 12448666354712576.0000 - val_loss: 12688444715171840.0000\n",
      "Epoch 3: early stopping\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10354, 3451, 3451, sum = 17256\n",
      "In split_to_train_test: dataset_X.shape=(11968, 11, 65), dataset_y.shape=(11968, 65)\n",
      "Epoch 1/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.1761 - val_loss: 162.2295\n",
      "Epoch 2/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 164.0868 - val_loss: 162.1507\n",
      "Epoch 3/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 164.0042 - val_loss: 162.0743\n",
      "Epoch 4/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.9226 - val_loss: 162.0000\n",
      "Epoch 5/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 163.8424 - val_loss: 161.9215\n",
      "Epoch 6/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.7342 - val_loss: 161.8201\n",
      "Epoch 7/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.6328 - val_loss: 161.7359\n",
      "Epoch 8/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 163.5412 - val_loss: 161.6568\n",
      "Epoch 9/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 163.4525 - val_loss: 161.5794\n",
      "Epoch 10/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.3648 - val_loss: 161.5032\n",
      "Epoch 11/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.2779 - val_loss: 161.4277\n",
      "Epoch 12/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.1916 - val_loss: 161.3529\n",
      "Epoch 13/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 163.1055 - val_loss: 161.2784\n",
      "Epoch 14/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 163.0197 - val_loss: 161.2044\n",
      "Epoch 15/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.9342 - val_loss: 161.1308\n",
      "Epoch 16/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.8490 - val_loss: 161.0577\n",
      "Epoch 17/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.7640 - val_loss: 160.9849\n",
      "Epoch 18/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 162.6790 - val_loss: 160.9122\n",
      "Epoch 19/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.5944 - val_loss: 160.8395\n",
      "Epoch 20/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 162.5099 - val_loss: 160.7672\n",
      "Epoch 21/30\n",
      "113/113 [==============================] - 3s 30ms/step - loss: 162.4255 - val_loss: 160.6951\n",
      "Epoch 22/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 162.3411 - val_loss: 160.6232\n",
      "Epoch 23/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.2567 - val_loss: 160.5516\n",
      "Epoch 24/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 162.1722 - val_loss: 160.4803\n",
      "Epoch 25/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 162.0879 - val_loss: 160.4088\n",
      "Epoch 26/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 162.0036 - val_loss: 160.3376\n",
      "Epoch 27/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 161.9193 - val_loss: 160.2666\n",
      "Epoch 28/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 161.8351 - val_loss: 160.1958\n",
      "Epoch 29/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 161.7510 - val_loss: 160.1253\n",
      "Epoch 30/30\n",
      "113/113 [==============================] - 4s 35ms/step - loss: 161.6671 - val_loss: 160.0550\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7181, 2393, 2394, sum = 11968\n",
      "In split_to_train_test: dataset_X.shape=(30881, 11, 65), dataset_y.shape=(30881, 65)\n",
      "Epoch 1/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 977.7682 - val_loss: 1002.6434\n",
      "Epoch 2/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 977.4968 - val_loss: 1002.3748\n",
      "Epoch 3/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 977.2276 - val_loss: 1002.1067\n",
      "Epoch 4/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 976.9584 - val_loss: 1001.8386\n",
      "Epoch 5/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 976.6892 - val_loss: 1001.5705\n",
      "Epoch 6/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 976.4202 - val_loss: 1001.3023\n",
      "Epoch 7/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 976.1511 - val_loss: 1001.0344\n",
      "Epoch 8/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 975.8823 - val_loss: 1000.7664\n",
      "Epoch 9/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 975.6130 - val_loss: 1000.4988\n",
      "Epoch 10/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 975.3447 - val_loss: 1000.2307\n",
      "Epoch 11/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 975.0751 - val_loss: 999.9633\n",
      "Epoch 12/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 974.8066 - val_loss: 999.6953\n",
      "Epoch 13/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 974.5370 - val_loss: 999.4279\n",
      "Epoch 14/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 974.2682 - val_loss: 999.1603\n",
      "Epoch 15/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 973.9999 - val_loss: 998.8923\n",
      "Epoch 16/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 973.7303 - val_loss: 998.6252\n",
      "Epoch 17/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 973.4620 - val_loss: 998.3575\n",
      "Epoch 18/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 973.1934 - val_loss: 998.0898\n",
      "Epoch 19/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 972.9240 - val_loss: 997.8221\n",
      "Epoch 20/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 972.6554 - val_loss: 997.5548\n",
      "Epoch 21/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 972.3861 - val_loss: 997.2872\n",
      "Epoch 22/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 972.1181 - val_loss: 997.0197\n",
      "Epoch 23/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 971.8490 - val_loss: 996.7519\n",
      "Epoch 24/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 971.5802 - val_loss: 996.4844\n",
      "Epoch 25/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 971.3115 - val_loss: 996.2167\n",
      "Epoch 26/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 971.0426 - val_loss: 995.9492\n",
      "Epoch 27/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 970.7745 - val_loss: 995.6820\n",
      "Epoch 28/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 970.5061 - val_loss: 995.4144\n",
      "Epoch 29/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 970.2372 - val_loss: 995.1471\n",
      "Epoch 30/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 969.9695 - val_loss: 994.8797\n",
      "193/193 [==============================] - 2s 11ms/step\n",
      "In calc_results: 18529, 6176, 6176, sum = 30881\n",
      "In split_to_train_test: dataset_X.shape=(12474, 11, 65), dataset_y.shape=(12474, 65)\n",
      "Epoch 1/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1093.6432 - val_loss: 1103.8014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1093.4952 - val_loss: 1103.6543\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1093.3481 - val_loss: 1103.5070\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1093.2010 - val_loss: 1103.3599\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1093.0542 - val_loss: 1103.2129\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.9071 - val_loss: 1103.0657\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.7606 - val_loss: 1102.9189\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.6139 - val_loss: 1102.7719\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.4670 - val_loss: 1102.6246\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1092.3203 - val_loss: 1102.4780\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.1731 - val_loss: 1102.3307\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1092.0266 - val_loss: 1102.1837\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1091.8796 - val_loss: 1102.0369\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1091.7330 - val_loss: 1101.8898\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1091.5859 - val_loss: 1101.7428\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1091.4393 - val_loss: 1101.5958\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1091.2925 - val_loss: 1101.4486\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1091.1459 - val_loss: 1101.3018\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1090.9989 - val_loss: 1101.1547\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.8519 - val_loss: 1101.0076\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1090.7051 - val_loss: 1100.8608\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1090.5586 - val_loss: 1100.7134\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.4115 - val_loss: 1100.5663\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1090.2651 - val_loss: 1100.4196\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1090.1183 - val_loss: 1100.2723\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1089.9712 - val_loss: 1100.1252\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1089.8243 - val_loss: 1099.9785\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1089.6776 - val_loss: 1099.8314\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1089.5310 - val_loss: 1099.6846\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1089.3840 - val_loss: 1099.5375\n",
      "78/78 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7484, 2495, 2495, sum = 12474\n",
      "N_clusters=7\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(92847, 11, 65), dataset_y.shape=(92847, 65)\n",
      "Epoch 1/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 811.9642 - val_loss: 810.7276\n",
      "Epoch 2/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 811.0971 - val_loss: 809.8612\n",
      "Epoch 3/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 810.2239 - val_loss: 808.9327\n",
      "Epoch 4/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 809.2725 - val_loss: 807.8896\n",
      "Epoch 5/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 808.1853 - val_loss: 806.8600\n",
      "Epoch 6/30\n",
      "871/871 [==============================] - 28s 33ms/step - loss: 807.1913 - val_loss: 805.8730\n",
      "Epoch 7/30\n",
      "871/871 [==============================] - 27s 32ms/step - loss: 806.2189 - val_loss: 804.8970\n",
      "Epoch 8/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 805.2548 - val_loss: 803.9304\n",
      "Epoch 9/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 804.3015 - val_loss: 802.9766\n",
      "Epoch 10/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 803.3602 - val_loss: 802.0284\n",
      "Epoch 11/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 802.4265 - val_loss: 801.0892\n",
      "Epoch 12/30\n",
      "871/871 [==============================] - 28s 33ms/step - loss: 801.4966 - val_loss: 800.1543\n",
      "Epoch 13/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 800.5698 - val_loss: 799.2221\n",
      "Epoch 14/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 799.6458 - val_loss: 798.2923\n",
      "Epoch 15/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 798.7228 - val_loss: 797.3651\n",
      "Epoch 16/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 797.8022 - val_loss: 796.4420\n",
      "Epoch 17/30\n",
      "871/871 [==============================] - 26s 30ms/step - loss: 796.8847 - val_loss: 795.5221\n",
      "Epoch 18/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 795.9691 - val_loss: 794.6052\n",
      "Epoch 19/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 795.0571 - val_loss: 793.6933\n",
      "Epoch 20/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 794.1514 - val_loss: 792.7916\n",
      "Epoch 21/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 793.2531 - val_loss: 791.8961\n",
      "Epoch 22/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 792.3562 - val_loss: 791.0035\n",
      "Epoch 23/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 791.4623 - val_loss: 790.1132\n",
      "Epoch 24/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 790.5696 - val_loss: 789.2238\n",
      "Epoch 25/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 789.6788 - val_loss: 788.3358\n",
      "Epoch 26/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 788.7901 - val_loss: 787.4490\n",
      "Epoch 27/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 787.9042 - val_loss: 786.5625\n",
      "Epoch 28/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 787.0209 - val_loss: 785.6772\n",
      "Epoch 29/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 786.1415 - val_loss: 784.7916\n",
      "Epoch 30/30\n",
      "871/871 [==============================] - 27s 32ms/step - loss: 785.2644 - val_loss: 783.9066\n",
      "581/581 [==============================] - 6s 11ms/step\n",
      "In calc_results: 55708, 18570, 18569, sum = 92847\n",
      "In split_to_train_test: dataset_X.shape=(24579, 11, 65), dataset_y.shape=(24579, 65)\n",
      "Epoch 1/30\n",
      "231/231 [==============================] - 8s 34ms/step - loss: 1019.7856 - val_loss: 1051.1488\n",
      "Epoch 2/30\n",
      "231/231 [==============================] - 8s 33ms/step - loss: 1019.4595 - val_loss: 1050.8251\n",
      "Epoch 3/30\n",
      "231/231 [==============================] - 8s 34ms/step - loss: 1019.1354 - val_loss: 1050.5009\n",
      "Epoch 4/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 1018.8107 - val_loss: 1050.1774\n",
      "Epoch 5/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 1018.4863 - val_loss: 1049.8534\n",
      "Epoch 6/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 1018.1619 - val_loss: 1049.5300\n",
      "Epoch 7/30\n",
      "231/231 [==============================] - 8s 33ms/step - loss: 1017.8373 - val_loss: 1049.2064\n",
      "Epoch 8/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 1017.5133 - val_loss: 1048.8828\n",
      "Epoch 9/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 1017.1888 - val_loss: 1048.5599\n",
      "Epoch 10/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 1016.8651 - val_loss: 1048.2363\n",
      "Epoch 11/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 1016.5409 - val_loss: 1047.9136\n",
      "Epoch 12/30\n",
      "231/231 [==============================] - 8s 34ms/step - loss: 1016.2169 - val_loss: 1047.5903\n",
      "Epoch 13/30\n",
      "231/231 [==============================] - 8s 33ms/step - loss: 1015.8932 - val_loss: 1047.2677\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 7s 30ms/step - loss: 1015.5687 - val_loss: 1046.9446\n",
      "Epoch 15/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 1015.2450 - val_loss: 1046.6218\n",
      "Epoch 16/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1014.9207 - val_loss: 1046.2991\n",
      "Epoch 17/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1014.5970 - val_loss: 1045.9762\n",
      "Epoch 18/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1014.2732 - val_loss: 1045.6532\n",
      "Epoch 19/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1013.9491 - val_loss: 1045.3303\n",
      "Epoch 20/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1013.6255 - val_loss: 1045.0076\n",
      "Epoch 21/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1013.3016 - val_loss: 1044.6846\n",
      "Epoch 22/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1012.9778 - val_loss: 1044.3617\n",
      "Epoch 23/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1012.6544 - val_loss: 1044.0394\n",
      "Epoch 24/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1012.3305 - val_loss: 1043.7167\n",
      "Epoch 25/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1012.0069 - val_loss: 1043.3942\n",
      "Epoch 26/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1011.6835 - val_loss: 1043.0713\n",
      "Epoch 27/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1011.3602 - val_loss: 1042.7489\n",
      "Epoch 28/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1011.0371 - val_loss: 1042.4261\n",
      "Epoch 29/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 1010.7137 - val_loss: 1042.1038\n",
      "Epoch 30/30\n",
      "231/231 [==============================] - 8s 33ms/step - loss: 1010.3911 - val_loss: 1041.7812\n",
      "154/154 [==============================] - 2s 11ms/step\n",
      "In calc_results: 14747, 4916, 4916, sum = 24579\n",
      "In split_to_train_test: dataset_X.shape=(17215, 11, 65), dataset_y.shape=(17215, 65)\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12446906491863040.0000 - val_loss: 12689506645835776.0000\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12446902196895744.0000 - val_loss: 12689506645835776.0000\n",
      "Epoch 3/30\n",
      "161/162 [============================>.] - ETA: 0s - loss: 12446980580048896.0000Restoring model weights from the end of the best epoch: 1.\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12446906491863040.0000 - val_loss: 12689506645835776.0000\n",
      "Epoch 3: early stopping\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10329, 3443, 3443, sum = 17215\n",
      "In split_to_train_test: dataset_X.shape=(32112, 11, 65), dataset_y.shape=(32112, 65)\n",
      "Epoch 1/30\n",
      "302/302 [==============================] - 10s 35ms/step - loss: 392.4417 - val_loss: 389.6812\n",
      "Epoch 2/30\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 391.9917 - val_loss: 389.2404\n",
      "Epoch 3/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 391.5516 - val_loss: 388.8025\n",
      "Epoch 4/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 391.0795 - val_loss: 388.3075\n",
      "Epoch 5/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 390.5992 - val_loss: 387.8381\n",
      "Epoch 6/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 390.1322 - val_loss: 387.3748\n",
      "Epoch 7/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 389.6693 - val_loss: 386.9139\n",
      "Epoch 8/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 389.2078 - val_loss: 386.4542\n",
      "Epoch 9/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 388.7474 - val_loss: 385.9953\n",
      "Epoch 10/30\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 388.2875 - val_loss: 385.5372\n",
      "Epoch 11/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 387.8282 - val_loss: 385.0794\n",
      "Epoch 12/30\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 387.3692 - val_loss: 384.6224\n",
      "Epoch 13/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 386.9109 - val_loss: 384.1667\n",
      "Epoch 14/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 386.4533 - val_loss: 383.7141\n",
      "Epoch 15/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 385.9962 - val_loss: 383.2621\n",
      "Epoch 16/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 385.5395 - val_loss: 382.8116\n",
      "Epoch 17/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 385.0828 - val_loss: 382.3627\n",
      "Epoch 18/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 384.6264 - val_loss: 381.9145\n",
      "Epoch 19/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 384.1697 - val_loss: 381.4671\n",
      "Epoch 20/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 383.7135 - val_loss: 381.0201\n",
      "Epoch 21/30\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 383.2570 - val_loss: 380.5738\n",
      "Epoch 22/30\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 382.8010 - val_loss: 380.1281\n",
      "Epoch 23/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 382.3454 - val_loss: 379.6830\n",
      "Epoch 24/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 381.8899 - val_loss: 379.2385\n",
      "Epoch 25/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 381.4347 - val_loss: 378.7944\n",
      "Epoch 26/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 380.9798 - val_loss: 378.3508\n",
      "Epoch 27/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 380.5251 - val_loss: 377.9083\n",
      "Epoch 28/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 380.0705 - val_loss: 377.4664\n",
      "Epoch 29/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 379.6164 - val_loss: 377.0251\n",
      "Epoch 30/30\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 379.1624 - val_loss: 376.5846\n",
      "201/201 [==============================] - 2s 11ms/step\n",
      "In calc_results: 19267, 6423, 6422, sum = 32112\n",
      "In split_to_train_test: dataset_X.shape=(141694, 11, 65), dataset_y.shape=(141694, 65)\n",
      "Epoch 1/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 862.6328 - val_loss: 861.6233\n",
      "Epoch 2/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 861.2861 - val_loss: 860.2598\n",
      "Epoch 3/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 859.9445 - val_loss: 858.8974\n",
      "Epoch 4/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 858.6031 - val_loss: 857.5367\n",
      "Epoch 5/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 857.2635 - val_loss: 856.1765\n",
      "Epoch 6/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 855.9246 - val_loss: 854.8193\n",
      "Epoch 7/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 854.5878 - val_loss: 853.4662\n",
      "Epoch 8/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 853.2557 - val_loss: 852.1269\n",
      "Epoch 9/30\n",
      "1329/1329 [==============================] - 43s 33ms/step - loss: 851.9371 - val_loss: 850.8068\n",
      "Epoch 10/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 850.6262 - val_loss: 849.4937\n",
      "Epoch 11/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 849.3184 - val_loss: 848.1885\n",
      "Epoch 12/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 848.0146 - val_loss: 846.8940\n",
      "Epoch 13/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 846.7211 - val_loss: 845.6174\n",
      "Epoch 14/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 845.4368 - val_loss: 844.3502\n",
      "Epoch 15/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 844.1572 - val_loss: 843.0910\n",
      "Epoch 16/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 842.8831 - val_loss: 841.8380\n",
      "Epoch 17/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 841.6142 - val_loss: 840.5916\n",
      "Epoch 18/30\n",
      "1329/1329 [==============================] - 43s 33ms/step - loss: 840.3504 - val_loss: 839.3506\n",
      "Epoch 19/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 839.0927 - val_loss: 838.1119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 837.8385 - val_loss: 836.8759\n",
      "Epoch 21/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 836.5891 - val_loss: 835.6414\n",
      "Epoch 22/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 835.3432 - val_loss: 834.4106\n",
      "Epoch 23/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 834.1007 - val_loss: 833.1876\n",
      "Epoch 24/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 832.8679 - val_loss: 831.9756\n",
      "Epoch 25/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 831.6443 - val_loss: 830.7658\n",
      "Epoch 26/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 830.4228 - val_loss: 829.5584\n",
      "Epoch 27/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 829.2037 - val_loss: 828.3536\n",
      "Epoch 28/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 827.9882 - val_loss: 827.1541\n",
      "Epoch 29/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 826.7770 - val_loss: 825.9599\n",
      "Epoch 30/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 825.5712 - val_loss: 824.7725\n",
      "886/886 [==============================] - 9s 11ms/step\n",
      "In calc_results: 85016, 28339, 28339, sum = 141694\n",
      "In split_to_train_test: dataset_X.shape=(7178, 11, 65), dataset_y.shape=(7178, 65)\n",
      "Epoch 1/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 161.2176 - val_loss: 162.2989\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 2s 33ms/step - loss: 161.1477 - val_loss: 162.2271\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 161.0709 - val_loss: 162.1521\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 161.0000 - val_loss: 162.0845\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.9339 - val_loss: 162.0198\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.8705 - val_loss: 161.9577\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.8085 - val_loss: 161.8967\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.7473 - val_loss: 161.8375\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.6868 - val_loss: 161.7793\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 160.6267 - val_loss: 161.7218\n",
      "Epoch 11/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.5668 - val_loss: 161.6647\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.5070 - val_loss: 161.6077\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 2s 30ms/step - loss: 160.4474 - val_loss: 161.5514\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.3880 - val_loss: 161.4956\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.3288 - val_loss: 161.4401\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.2695 - val_loss: 161.3849\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 160.2104 - val_loss: 161.3301\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.1513 - val_loss: 161.2752\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.0922 - val_loss: 161.2206\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.0334 - val_loss: 161.1659\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.9747 - val_loss: 161.1110\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 159.9160 - val_loss: 161.0563\n",
      "Epoch 23/30\n",
      "68/68 [==============================] - 2s 35ms/step - loss: 159.8574 - val_loss: 161.0015\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 2s 35ms/step - loss: 159.7989 - val_loss: 160.9468\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 159.7405 - val_loss: 160.8923\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.6822 - val_loss: 160.8377\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.6238 - val_loss: 160.7833\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.5655 - val_loss: 160.7294\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.5075 - val_loss: 160.6756\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.4498 - val_loss: 160.6228\n",
      "45/45 [==============================] - 0s 10ms/step\n",
      "In calc_results: 4307, 1435, 1436, sum = 7178\n",
      "In split_to_train_test: dataset_X.shape=(10841, 11, 65), dataset_y.shape=(10841, 65)\n",
      "Epoch 1/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.5710 - val_loss: 1134.4924\n",
      "Epoch 2/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.4224 - val_loss: 1134.3439\n",
      "Epoch 3/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.2739 - val_loss: 1134.1952\n",
      "Epoch 4/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.1252 - val_loss: 1134.0469\n",
      "Epoch 5/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1125.9768 - val_loss: 1133.8982\n",
      "Epoch 6/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1125.8285 - val_loss: 1133.7498\n",
      "Epoch 7/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1125.6804 - val_loss: 1133.6012\n",
      "Epoch 8/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1125.5319 - val_loss: 1133.4528\n",
      "Epoch 9/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1125.3839 - val_loss: 1133.3042\n",
      "Epoch 10/30\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 1125.2352 - val_loss: 1133.1556\n",
      "Epoch 11/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1125.0873 - val_loss: 1133.0071\n",
      "Epoch 12/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1124.9391 - val_loss: 1132.8586\n",
      "Epoch 13/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1124.7906 - val_loss: 1132.7100\n",
      "Epoch 14/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1124.6425 - val_loss: 1132.5614\n",
      "Epoch 15/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1124.4940 - val_loss: 1132.4128\n",
      "Epoch 16/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1124.3455 - val_loss: 1132.2642\n",
      "Epoch 17/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1124.1975 - val_loss: 1132.1156\n",
      "Epoch 18/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1124.0488 - val_loss: 1131.9672\n",
      "Epoch 19/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1123.9006 - val_loss: 1131.8186\n",
      "Epoch 20/30\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 1123.7524 - val_loss: 1131.6700\n",
      "Epoch 21/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1123.6040 - val_loss: 1131.5216\n",
      "Epoch 22/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1123.4556 - val_loss: 1131.3730\n",
      "Epoch 23/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1123.3075 - val_loss: 1131.2244\n",
      "Epoch 24/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1123.1591 - val_loss: 1131.0758\n",
      "Epoch 25/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1123.0109 - val_loss: 1130.9271\n",
      "Epoch 26/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1122.8627 - val_loss: 1130.7786\n",
      "Epoch 27/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1122.7145 - val_loss: 1130.6301\n",
      "Epoch 28/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1122.5662 - val_loss: 1130.4817\n",
      "Epoch 29/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1122.4176 - val_loss: 1130.3329\n",
      "Epoch 30/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1122.2695 - val_loss: 1130.1843\n",
      "68/68 [==============================] - 1s 11ms/step\n",
      "In calc_results: 6505, 2168, 2168, sum = 10841\n",
      "N_clusters=9\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(22117, 11, 65), dataset_y.shape=(22117, 65)\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 7s 35ms/step - loss: 993.9601 - val_loss: 1015.5467\n",
      "Epoch 2/30\n",
      "208/208 [==============================] - 7s 34ms/step - loss: 993.8076 - val_loss: 1015.3956\n",
      "Epoch 3/30\n",
      "208/208 [==============================] - 7s 35ms/step - loss: 993.6561 - val_loss: 1015.2443\n",
      "Epoch 4/30\n",
      "208/208 [==============================] - 7s 34ms/step - loss: 993.5048 - val_loss: 1015.0937\n",
      "Epoch 5/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 993.3529 - val_loss: 1014.9426\n",
      "Epoch 6/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 993.2018 - val_loss: 1014.7919\n",
      "Epoch 7/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 993.0499 - val_loss: 1014.6409\n",
      "Epoch 8/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 992.8984 - val_loss: 1014.4901\n",
      "Epoch 9/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 992.7474 - val_loss: 1014.3395\n",
      "Epoch 10/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 992.5958 - val_loss: 1014.1885\n",
      "Epoch 11/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 992.4441 - val_loss: 1014.0378\n",
      "Epoch 12/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 992.2934 - val_loss: 1013.8871\n",
      "Epoch 13/30\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 992.1420 - val_loss: 1013.7366\n",
      "Epoch 14/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.9904 - val_loss: 1013.5860\n",
      "Epoch 15/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.8387 - val_loss: 1013.4354\n",
      "Epoch 16/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.6875 - val_loss: 1013.2846\n",
      "Epoch 17/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.5366 - val_loss: 1013.1340\n",
      "Epoch 18/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.3844 - val_loss: 1012.9837\n",
      "Epoch 19/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.2336 - val_loss: 1012.8329\n",
      "Epoch 20/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 991.0820 - val_loss: 1012.6826\n",
      "Epoch 21/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 990.9310 - val_loss: 1012.5317\n",
      "Epoch 22/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 990.7796 - val_loss: 1012.3815\n",
      "Epoch 23/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 990.6277 - val_loss: 1012.2311\n",
      "Epoch 24/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 990.4763 - val_loss: 1012.0804\n",
      "Epoch 25/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 990.3255 - val_loss: 1011.9302\n",
      "Epoch 26/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 990.1736 - val_loss: 1011.7793\n",
      "Epoch 27/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 990.0222 - val_loss: 1011.6290\n",
      "Epoch 28/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 989.8712 - val_loss: 1011.4784\n",
      "Epoch 29/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 989.7197 - val_loss: 1011.3279\n",
      "Epoch 30/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 989.5687 - val_loss: 1011.1776\n",
      "139/139 [==============================] - 1s 11ms/step\n",
      "In calc_results: 13270, 4424, 4423, sum = 22117\n",
      "In split_to_train_test: dataset_X.shape=(64938, 11, 65), dataset_y.shape=(64938, 65)\n",
      "Epoch 1/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 729.8817 - val_loss: 720.0047\n",
      "Epoch 2/30\n",
      "609/609 [==============================] - 19s 32ms/step - loss: 729.1343 - val_loss: 719.2532\n",
      "Epoch 3/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 728.3882 - val_loss: 718.5025\n",
      "Epoch 4/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 727.6423 - val_loss: 717.7515\n",
      "Epoch 5/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 726.8962 - val_loss: 717.0006\n",
      "Epoch 6/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 726.1511 - val_loss: 716.2508\n",
      "Epoch 7/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 725.4053 - val_loss: 715.5013\n",
      "Epoch 8/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 724.6597 - val_loss: 714.7527\n",
      "Epoch 9/30\n",
      "609/609 [==============================] - 19s 30ms/step - loss: 723.9152 - val_loss: 714.0052\n",
      "Epoch 10/30\n",
      "609/609 [==============================] - 19s 30ms/step - loss: 723.1711 - val_loss: 713.2591\n",
      "Epoch 11/30\n",
      "609/609 [==============================] - 19s 30ms/step - loss: 722.4279 - val_loss: 712.5146\n",
      "Epoch 12/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 721.6848 - val_loss: 711.7731\n",
      "Epoch 13/30\n",
      "609/609 [==============================] - 20s 32ms/step - loss: 720.9459 - val_loss: 711.0403\n",
      "Epoch 14/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 720.2116 - val_loss: 710.3130\n",
      "Epoch 15/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 719.4792 - val_loss: 709.5878\n",
      "Epoch 16/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 718.7474 - val_loss: 708.8655\n",
      "Epoch 17/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 718.0159 - val_loss: 708.1456\n",
      "Epoch 18/30\n",
      "609/609 [==============================] - 19s 30ms/step - loss: 717.2864 - val_loss: 707.4283\n",
      "Epoch 19/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 716.5588 - val_loss: 706.7158\n",
      "Epoch 20/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 715.8333 - val_loss: 706.0092\n",
      "Epoch 21/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 715.1120 - val_loss: 705.3052\n",
      "Epoch 22/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 714.3901 - val_loss: 704.6025\n",
      "Epoch 23/30\n",
      "609/609 [==============================] - 20s 33ms/step - loss: 713.6707 - val_loss: 703.9017\n",
      "Epoch 24/30\n",
      "609/609 [==============================] - 20s 33ms/step - loss: 712.9519 - val_loss: 703.2025\n",
      "Epoch 25/30\n",
      "609/609 [==============================] - 18s 30ms/step - loss: 712.2017 - val_loss: 702.4227\n",
      "Epoch 26/30\n",
      "609/609 [==============================] - 19s 32ms/step - loss: 711.4037 - val_loss: 701.6659\n",
      "Epoch 27/30\n",
      "609/609 [==============================] - 19s 30ms/step - loss: 710.6371 - val_loss: 700.9224\n",
      "Epoch 28/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 709.8786 - val_loss: 700.1840\n",
      "Epoch 29/30\n",
      "609/609 [==============================] - 20s 33ms/step - loss: 709.1240 - val_loss: 699.4495\n",
      "Epoch 30/30\n",
      "609/609 [==============================] - 19s 31ms/step - loss: 708.3727 - val_loss: 698.7171\n",
      "406/406 [==============================] - 4s 11ms/step\n",
      "In calc_results: 38963, 12987, 12988, sum = 64938\n",
      "In split_to_train_test: dataset_X.shape=(104480, 11, 65), dataset_y.shape=(104480, 65)\n",
      "Epoch 1/30\n",
      "980/980 [==============================] - 33s 34ms/step - loss: 912.3962 - val_loss: 912.1332\n",
      "Epoch 2/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 910.8724 - val_loss: 910.5999\n",
      "Epoch 3/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 909.3582 - val_loss: 909.0652\n",
      "Epoch 4/30\n",
      "980/980 [==============================] - 31s 31ms/step - loss: 907.8471 - val_loss: 907.5318\n",
      "Epoch 5/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 906.3366 - val_loss: 906.0001\n",
      "Epoch 6/30\n",
      "980/980 [==============================] - 30s 31ms/step - loss: 904.8277 - val_loss: 904.4697\n",
      "Epoch 7/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 903.3228 - val_loss: 902.9492\n",
      "Epoch 8/30\n",
      "980/980 [==============================] - 30s 30ms/step - loss: 901.8331 - val_loss: 901.4527\n",
      "Epoch 9/30\n",
      "980/980 [==============================] - 32s 32ms/step - loss: 900.3561 - val_loss: 899.9662\n",
      "Epoch 10/30\n",
      "980/980 [==============================] - 32s 33ms/step - loss: 898.8840 - val_loss: 898.4861\n",
      "Epoch 11/30\n",
      "980/980 [==============================] - 32s 33ms/step - loss: 897.4156 - val_loss: 897.0163\n",
      "Epoch 12/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 895.9591 - val_loss: 895.5633\n",
      "Epoch 13/30\n",
      "980/980 [==============================] - 31s 31ms/step - loss: 894.5132 - val_loss: 894.1176\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980/980 [==============================] - 30s 30ms/step - loss: 893.0716 - val_loss: 892.6799\n",
      "Epoch 15/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 891.6348 - val_loss: 891.2515\n",
      "Epoch 16/30\n",
      "980/980 [==============================] - 31s 31ms/step - loss: 890.2039 - val_loss: 889.8319\n",
      "Epoch 17/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 888.7805 - val_loss: 888.4205\n",
      "Epoch 18/30\n",
      "980/980 [==============================] - 30s 31ms/step - loss: 887.3611 - val_loss: 887.0164\n",
      "Epoch 19/30\n",
      "980/980 [==============================] - 31s 31ms/step - loss: 885.9473 - val_loss: 885.6198\n",
      "Epoch 20/30\n",
      "980/980 [==============================] - 32s 33ms/step - loss: 884.5402 - val_loss: 884.2340\n",
      "Epoch 21/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 883.1477 - val_loss: 882.8649\n",
      "Epoch 22/30\n",
      "980/980 [==============================] - 32s 33ms/step - loss: 881.7689 - val_loss: 881.5015\n",
      "Epoch 23/30\n",
      "980/980 [==============================] - 30s 31ms/step - loss: 880.3953 - val_loss: 880.1400\n",
      "Epoch 24/30\n",
      "980/980 [==============================] - 30s 30ms/step - loss: 879.0251 - val_loss: 878.7841\n",
      "Epoch 25/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 877.6588 - val_loss: 877.4352\n",
      "Epoch 26/30\n",
      "980/980 [==============================] - 31s 31ms/step - loss: 876.2988 - val_loss: 876.0963\n",
      "Epoch 27/30\n",
      "980/980 [==============================] - 31s 32ms/step - loss: 874.9463 - val_loss: 874.7686\n",
      "Epoch 28/30\n",
      "980/980 [==============================] - 32s 32ms/step - loss: 873.6003 - val_loss: 873.4518\n",
      "Epoch 29/30\n",
      "980/980 [==============================] - 30s 31ms/step - loss: 872.2644 - val_loss: 872.1442\n",
      "Epoch 30/30\n",
      "980/980 [==============================] - 33s 34ms/step - loss: 870.9344 - val_loss: 870.8430\n",
      "653/653 [==============================] - 7s 11ms/step\n",
      "In calc_results: 62688, 20896, 20896, sum = 104480\n",
      "In split_to_train_test: dataset_X.shape=(72969, 11, 65), dataset_y.shape=(72969, 65)\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 794.4142 - val_loss: 792.1356\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 22s 31ms/step - loss: 793.5295 - val_loss: 791.2484\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 792.6448 - val_loss: 790.3617\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 791.7614 - val_loss: 789.4755\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 790.7918 - val_loss: 788.4080\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 789.7682 - val_loss: 787.4308\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 788.8065 - val_loss: 786.4710\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 787.8578 - val_loss: 785.5202\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 23s 33ms/step - loss: 786.9194 - val_loss: 784.5837\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 21s 30ms/step - loss: 785.9955 - val_loss: 783.6566\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 785.0825 - val_loss: 782.7410\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 21s 30ms/step - loss: 784.1750 - val_loss: 781.8280\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 783.2686 - val_loss: 780.9174\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 782.3655 - val_loss: 780.0096\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 23s 33ms/step - loss: 781.4653 - val_loss: 779.1051\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 22s 33ms/step - loss: 780.5661 - val_loss: 778.2037\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 779.6700 - val_loss: 777.3062\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 778.7776 - val_loss: 776.4150\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 777.8938 - val_loss: 775.5363\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 777.0149 - val_loss: 774.6609\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 776.1383 - val_loss: 773.7881\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 775.2615 - val_loss: 772.9160\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 22s 31ms/step - loss: 774.3881 - val_loss: 772.0447\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 773.5167 - val_loss: 771.1743\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 23s 33ms/step - loss: 772.6470 - val_loss: 770.3038\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 22s 32ms/step - loss: 771.7802 - val_loss: 769.4348\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 21s 31ms/step - loss: 770.9163 - val_loss: 768.5659\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 23s 33ms/step - loss: 770.0545 - val_loss: 767.6974\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 23s 34ms/step - loss: 769.1947 - val_loss: 766.8298\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 23s 33ms/step - loss: 768.3372 - val_loss: 765.9627\n",
      "457/457 [==============================] - 5s 11ms/step\n",
      "In calc_results: 43781, 14594, 14594, sum = 72969\n",
      "In split_to_train_test: dataset_X.shape=(11544, 11, 65), dataset_y.shape=(11544, 65)\n",
      "Epoch 1/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1089.3765 - val_loss: 1111.8895\n",
      "Epoch 2/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1089.2520 - val_loss: 1111.7654\n",
      "Epoch 3/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1089.1281 - val_loss: 1111.6410\n",
      "Epoch 4/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1089.0038 - val_loss: 1111.5171\n",
      "Epoch 5/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1088.8794 - val_loss: 1111.3931\n",
      "Epoch 6/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1088.7556 - val_loss: 1111.2688\n",
      "Epoch 7/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1088.6313 - val_loss: 1111.1449\n",
      "Epoch 8/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1088.5077 - val_loss: 1111.0209\n",
      "Epoch 9/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1088.3838 - val_loss: 1110.8970\n",
      "Epoch 10/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1088.2598 - val_loss: 1110.7728\n",
      "Epoch 11/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1088.1357 - val_loss: 1110.6488\n",
      "Epoch 12/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1088.0121 - val_loss: 1110.5249\n",
      "Epoch 13/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.8879 - val_loss: 1110.4009\n",
      "Epoch 14/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.7638 - val_loss: 1110.2770\n",
      "Epoch 15/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.6400 - val_loss: 1110.1528\n",
      "Epoch 16/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.5159 - val_loss: 1110.0289\n",
      "Epoch 17/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.3922 - val_loss: 1109.9049\n",
      "Epoch 18/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1087.2681 - val_loss: 1109.7811\n",
      "Epoch 19/30\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 1087.1442 - val_loss: 1109.6569\n",
      "Epoch 20/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1087.0203 - val_loss: 1109.5331\n",
      "Epoch 21/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1086.8964 - val_loss: 1109.4091\n",
      "Epoch 22/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1086.7723 - val_loss: 1109.2853\n",
      "Epoch 23/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1086.6482 - val_loss: 1109.1609\n",
      "Epoch 24/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1086.5245 - val_loss: 1109.0372\n",
      "Epoch 25/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1086.4006 - val_loss: 1108.9132\n",
      "Epoch 26/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1086.2766 - val_loss: 1108.7893\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 3s 32ms/step - loss: 1086.1527 - val_loss: 1108.6652\n",
      "Epoch 28/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1086.0287 - val_loss: 1108.5414\n",
      "Epoch 29/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1085.9045 - val_loss: 1108.4172\n",
      "Epoch 30/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1085.7808 - val_loss: 1108.2935\n",
      "73/73 [==============================] - 1s 11ms/step\n",
      "In calc_results: 6926, 2309, 2309, sum = 11544\n",
      "In split_to_train_test: dataset_X.shape=(7310, 11, 65), dataset_y.shape=(7310, 65)\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1108.9122 - val_loss: 1114.8068\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.8409 - val_loss: 1114.7368\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.7716 - val_loss: 1114.6674\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1108.7020 - val_loss: 1114.5978\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.6324 - val_loss: 1114.5280\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 30ms/step - loss: 1108.5629 - val_loss: 1114.4585\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.4935 - val_loss: 1114.3888\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.4238 - val_loss: 1114.3191\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1108.3546 - val_loss: 1114.2495\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.2848 - val_loss: 1114.1799\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.2152 - val_loss: 1114.1104\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1108.1459 - val_loss: 1114.0406\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1108.0764 - val_loss: 1113.9709\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1108.0066 - val_loss: 1113.9012\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.9375 - val_loss: 1113.8315\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.8679 - val_loss: 1113.7620\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.7983 - val_loss: 1113.6924\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.7292 - val_loss: 1113.6227\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.6594 - val_loss: 1113.5531\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.5900 - val_loss: 1113.4836\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1107.5205 - val_loss: 1113.4138\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1107.4509 - val_loss: 1113.3442\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.3816 - val_loss: 1113.2748\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1107.3120 - val_loss: 1113.2051\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1107.2426 - val_loss: 1113.1351\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.1729 - val_loss: 1113.0658\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.1034 - val_loss: 1112.9960\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1107.0339 - val_loss: 1112.9263\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1106.9644 - val_loss: 1112.8569\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1106.8950 - val_loss: 1112.7872\n",
      "46/46 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4386, 1462, 1462, sum = 7310\n",
      "In split_to_train_test: dataset_X.shape=(9805, 11, 65), dataset_y.shape=(9805, 65)\n",
      "Epoch 1/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.8066 - val_loss: 185.6779\n",
      "Epoch 2/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.7056 - val_loss: 185.5753\n",
      "Epoch 3/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.6108 - val_loss: 185.4734\n",
      "Epoch 4/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.5195 - val_loss: 185.3753\n",
      "Epoch 5/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.4325 - val_loss: 185.2811\n",
      "Epoch 6/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.3506 - val_loss: 185.1906\n",
      "Epoch 7/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.2707 - val_loss: 185.1016\n",
      "Epoch 8/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.1913 - val_loss: 185.0147\n",
      "Epoch 9/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 153.1147 - val_loss: 184.9307\n",
      "Epoch 10/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 153.0392 - val_loss: 184.8473\n",
      "Epoch 11/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.9637 - val_loss: 184.7638\n",
      "Epoch 12/30\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 152.8884 - val_loss: 184.6807\n",
      "Epoch 13/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.8130 - val_loss: 184.5968\n",
      "Epoch 14/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.7377 - val_loss: 184.5135\n",
      "Epoch 15/30\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 152.6624 - val_loss: 184.4306\n",
      "Epoch 16/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.5872 - val_loss: 184.3475\n",
      "Epoch 17/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.5120 - val_loss: 184.2647\n",
      "Epoch 18/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.4368 - val_loss: 184.1823\n",
      "Epoch 19/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.3484 - val_loss: 184.0810\n",
      "Epoch 20/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 152.2542 - val_loss: 183.9892\n",
      "Epoch 21/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.1698 - val_loss: 183.9010\n",
      "Epoch 22/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 152.0846 - val_loss: 183.8129\n",
      "Epoch 23/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.9964 - val_loss: 183.7309\n",
      "Epoch 24/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 151.9114 - val_loss: 183.6492\n",
      "Epoch 25/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.8288 - val_loss: 183.5663\n",
      "Epoch 26/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 151.7476 - val_loss: 183.4826\n",
      "Epoch 27/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 151.6676 - val_loss: 183.3990\n",
      "Epoch 28/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 151.5883 - val_loss: 183.3168\n",
      "Epoch 29/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 151.5102 - val_loss: 183.2363\n",
      "Epoch 30/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 151.4347 - val_loss: 183.1588\n",
      "62/62 [==============================] - 1s 12ms/step\n",
      "In calc_results: 5883, 1961, 1961, sum = 9805\n",
      "In split_to_train_test: dataset_X.shape=(18822, 11, 65), dataset_y.shape=(18822, 65)\n",
      "Epoch 1/30\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 334.5207 - val_loss: 331.5529\n",
      "Epoch 2/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 334.2911 - val_loss: 331.3426\n",
      "Epoch 3/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 334.0736 - val_loss: 331.1303\n",
      "Epoch 4/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 333.8478 - val_loss: 330.9207\n",
      "Epoch 5/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 333.6291 - val_loss: 330.7130\n",
      "Epoch 6/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 333.4118 - val_loss: 330.5063\n",
      "Epoch 7/30\n",
      "177/177 [==============================] - 6s 31ms/step - loss: 333.1953 - val_loss: 330.3001\n",
      "Epoch 8/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 332.9793 - val_loss: 330.0943\n",
      "Epoch 9/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 332.7639 - val_loss: 329.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 332.5500 - val_loss: 329.6855\n",
      "Epoch 11/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 332.3362 - val_loss: 329.4813\n",
      "Epoch 12/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 332.1225 - val_loss: 329.2773\n",
      "Epoch 13/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 331.9090 - val_loss: 329.0733\n",
      "Epoch 14/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 331.6955 - val_loss: 328.8695\n",
      "Epoch 15/30\n",
      "177/177 [==============================] - 6s 31ms/step - loss: 331.4822 - val_loss: 328.6656\n",
      "Epoch 16/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 331.2688 - val_loss: 328.4618\n",
      "Epoch 17/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 331.0555 - val_loss: 328.2582\n",
      "Epoch 18/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 330.8425 - val_loss: 328.0549\n",
      "Epoch 19/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 330.6293 - val_loss: 327.8515\n",
      "Epoch 20/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 330.4164 - val_loss: 327.6481\n",
      "Epoch 21/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 330.2033 - val_loss: 327.4446\n",
      "Epoch 22/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 329.9902 - val_loss: 327.2414\n",
      "Epoch 23/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 329.7775 - val_loss: 327.0385\n",
      "Epoch 24/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 329.5647 - val_loss: 326.8356\n",
      "Epoch 25/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 329.3520 - val_loss: 326.6329\n",
      "Epoch 26/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 329.1393 - val_loss: 326.4301\n",
      "Epoch 27/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 328.9267 - val_loss: 326.2274\n",
      "Epoch 28/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 328.7141 - val_loss: 326.0247\n",
      "Epoch 29/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 328.5016 - val_loss: 325.8221\n",
      "Epoch 30/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 328.2889 - val_loss: 325.6198\n",
      "118/118 [==============================] - 1s 10ms/step\n",
      "In calc_results: 11293, 3765, 3764, sum = 18822\n",
      "In split_to_train_test: dataset_X.shape=(14481, 11, 65), dataset_y.shape=(14481, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515785956130816.0000 - val_loss: 12546002124800000.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515783808647168.0000 - val_loss: 12546002124800000.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515793472323584.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515778439938048.0000 - val_loss: 12546002124800000.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8689, 2896, 2896, sum = 14481\n",
      "N_clusters=11\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(7203, 11, 65), dataset_y.shape=(7203, 65)\n",
      "Epoch 1/30\n",
      "68/68 [==============================] - 2s 33ms/step - loss: 161.9105 - val_loss: 163.3390\n",
      "Epoch 2/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 161.8319 - val_loss: 163.2759\n",
      "Epoch 3/30\n",
      "68/68 [==============================] - 2s 33ms/step - loss: 161.7687 - val_loss: 163.2141\n",
      "Epoch 4/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 161.7063 - val_loss: 163.1534\n",
      "Epoch 5/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 161.6406 - val_loss: 163.0811\n",
      "Epoch 6/30\n",
      "68/68 [==============================] - 2s 35ms/step - loss: 161.5622 - val_loss: 163.0123\n",
      "Epoch 7/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 161.4942 - val_loss: 162.9482\n",
      "Epoch 8/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 161.4280 - val_loss: 162.8856\n",
      "Epoch 9/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 161.3517 - val_loss: 162.8029\n",
      "Epoch 10/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 161.2665 - val_loss: 162.7273\n",
      "Epoch 11/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 161.1903 - val_loss: 162.6553\n",
      "Epoch 12/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 161.1161 - val_loss: 162.5857\n",
      "Epoch 13/30\n",
      "68/68 [==============================] - 2s 32ms/step - loss: 161.0431 - val_loss: 162.5167\n",
      "Epoch 14/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.9710 - val_loss: 162.4491\n",
      "Epoch 15/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.8995 - val_loss: 162.3823\n",
      "Epoch 16/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.8284 - val_loss: 162.3159\n",
      "Epoch 17/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 160.7578 - val_loss: 162.2502\n",
      "Epoch 18/30\n",
      "68/68 [==============================] - 2s 33ms/step - loss: 160.6875 - val_loss: 162.1852\n",
      "Epoch 19/30\n",
      "68/68 [==============================] - 2s 34ms/step - loss: 160.6174 - val_loss: 162.1203\n",
      "Epoch 20/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.5476 - val_loss: 162.0558\n",
      "Epoch 21/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.4783 - val_loss: 161.9919\n",
      "Epoch 22/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.4095 - val_loss: 161.9282\n",
      "Epoch 23/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.3408 - val_loss: 161.8648\n",
      "Epoch 24/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.2723 - val_loss: 161.8015\n",
      "Epoch 25/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.2038 - val_loss: 161.7385\n",
      "Epoch 26/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.1356 - val_loss: 161.6756\n",
      "Epoch 27/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 160.0672 - val_loss: 161.6128\n",
      "Epoch 28/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.9990 - val_loss: 161.5503\n",
      "Epoch 29/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.9311 - val_loss: 161.4881\n",
      "Epoch 30/30\n",
      "68/68 [==============================] - 2s 31ms/step - loss: 159.8637 - val_loss: 161.4262\n",
      "46/46 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4322, 1440, 1441, sum = 7203\n",
      "In split_to_train_test: dataset_X.shape=(14525, 11, 65), dataset_y.shape=(14525, 65)\n",
      "Epoch 1/30\n",
      "137/137 [==============================] - 4s 32ms/step - loss: 12516598778691584.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 2/30\n",
      "137/137 [==============================] - 4s 31ms/step - loss: 12516598778691584.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 3/30\n",
      "137/137 [==============================] - ETA: 0s - loss: 12516598778691584.0000Restoring model weights from the end of the best epoch: 1.\n",
      "137/137 [==============================] - 4s 31ms/step - loss: 12516598778691584.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 10ms/step\n",
      "In calc_results: 8715, 2905, 2905, sum = 14525\n",
      "In split_to_train_test: dataset_X.shape=(2615, 11, 65), dataset_y.shape=(2615, 65)\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48073534584389632.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 48073534584389632.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 48073534584389632.0000Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 48073534584389632.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3: early stopping\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "In calc_results: 1569, 523, 523, sum = 2615\n",
      "In split_to_train_test: dataset_X.shape=(43528, 11, 65), dataset_y.shape=(43528, 65)\n",
      "Epoch 1/30\n",
      "409/409 [==============================] - 14s 33ms/step - loss: 720.5639 - val_loss: 709.7283\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 14s 33ms/step - loss: 719.8797 - val_loss: 709.0475\n",
      "Epoch 3/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 719.1987 - val_loss: 708.3665\n",
      "Epoch 4/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 718.5175 - val_loss: 707.6864\n",
      "Epoch 5/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 717.8370 - val_loss: 707.0059\n",
      "Epoch 6/30\n",
      "409/409 [==============================] - 12s 30ms/step - loss: 717.1559 - val_loss: 706.3264\n",
      "Epoch 7/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 716.4752 - val_loss: 705.6476\n",
      "Epoch 8/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 715.7950 - val_loss: 704.9702\n",
      "Epoch 9/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 715.1149 - val_loss: 704.2938\n",
      "Epoch 10/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 714.4345 - val_loss: 703.6202\n",
      "Epoch 11/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 713.7559 - val_loss: 702.9517\n",
      "Epoch 12/30\n",
      "409/409 [==============================] - 14s 34ms/step - loss: 713.0796 - val_loss: 702.2925\n",
      "Epoch 13/30\n",
      "409/409 [==============================] - 14s 34ms/step - loss: 712.4052 - val_loss: 701.6365\n",
      "Epoch 14/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 711.7305 - val_loss: 700.9828\n",
      "Epoch 15/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 711.0574 - val_loss: 700.3314\n",
      "Epoch 16/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 710.3840 - val_loss: 699.6815\n",
      "Epoch 17/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 709.7115 - val_loss: 699.0339\n",
      "Epoch 18/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 709.0402 - val_loss: 698.3891\n",
      "Epoch 19/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 708.3691 - val_loss: 697.7494\n",
      "Epoch 20/30\n",
      "409/409 [==============================] - 12s 31ms/step - loss: 707.7000 - val_loss: 697.1161\n",
      "Epoch 21/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 707.0327 - val_loss: 696.4849\n",
      "Epoch 22/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 706.3663 - val_loss: 695.8549\n",
      "Epoch 23/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 705.7004 - val_loss: 695.2259\n",
      "Epoch 24/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 705.0360 - val_loss: 694.5984\n",
      "Epoch 25/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 704.3719 - val_loss: 693.9729\n",
      "Epoch 26/30\n",
      "409/409 [==============================] - 12s 30ms/step - loss: 703.7093 - val_loss: 693.3485\n",
      "Epoch 27/30\n",
      "409/409 [==============================] - 12s 30ms/step - loss: 703.0479 - val_loss: 692.7252\n",
      "Epoch 28/30\n",
      "409/409 [==============================] - 13s 31ms/step - loss: 702.3873 - val_loss: 692.1022\n",
      "Epoch 29/30\n",
      "409/409 [==============================] - 12s 30ms/step - loss: 701.7277 - val_loss: 691.4808\n",
      "Epoch 30/30\n",
      "409/409 [==============================] - 13s 32ms/step - loss: 701.0700 - val_loss: 690.8593\n",
      "273/273 [==============================] - 3s 11ms/step\n",
      "In calc_results: 26117, 8705, 8706, sum = 43528\n",
      "In split_to_train_test: dataset_X.shape=(9342, 11, 65), dataset_y.shape=(9342, 65)\n",
      "Epoch 1/30\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1153.8683 - val_loss: 1177.9286\n",
      "Epoch 2/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.8055 - val_loss: 1177.8668\n",
      "Epoch 3/30\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 1153.7439 - val_loss: 1177.8046\n",
      "Epoch 4/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.6819 - val_loss: 1177.7426\n",
      "Epoch 5/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.6195 - val_loss: 1177.6805\n",
      "Epoch 6/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.5575 - val_loss: 1177.6187\n",
      "Epoch 7/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.4956 - val_loss: 1177.5569\n",
      "Epoch 8/30\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 1153.4340 - val_loss: 1177.4945\n",
      "Epoch 9/30\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1153.3719 - val_loss: 1177.4329\n",
      "Epoch 10/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.3102 - val_loss: 1177.3707\n",
      "Epoch 11/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.2482 - val_loss: 1177.3088\n",
      "Epoch 12/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.1863 - val_loss: 1177.2466\n",
      "Epoch 13/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.1241 - val_loss: 1177.1848\n",
      "Epoch 14/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1153.0619 - val_loss: 1177.1230\n",
      "Epoch 15/30\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 1153.0004 - val_loss: 1177.0609\n",
      "Epoch 16/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.9385 - val_loss: 1176.9989\n",
      "Epoch 17/30\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 1152.8770 - val_loss: 1176.9370\n",
      "Epoch 18/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.8146 - val_loss: 1176.8751\n",
      "Epoch 19/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.7528 - val_loss: 1176.8129\n",
      "Epoch 20/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.6908 - val_loss: 1176.7511\n",
      "Epoch 21/30\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 1152.6290 - val_loss: 1176.6890\n",
      "Epoch 22/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.5670 - val_loss: 1176.6271\n",
      "Epoch 23/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.5051 - val_loss: 1176.5651\n",
      "Epoch 24/30\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 1152.4432 - val_loss: 1176.5031\n",
      "Epoch 25/30\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 1152.3815 - val_loss: 1176.4413\n",
      "Epoch 26/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.3196 - val_loss: 1176.3790\n",
      "Epoch 27/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.2573 - val_loss: 1176.3169\n",
      "Epoch 28/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.1958 - val_loss: 1176.2552\n",
      "Epoch 29/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.1337 - val_loss: 1176.1931\n",
      "Epoch 30/30\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1152.0717 - val_loss: 1176.1312\n",
      "59/59 [==============================] - 1s 11ms/step\n",
      "In calc_results: 5605, 1869, 1868, sum = 9342\n",
      "In split_to_train_test: dataset_X.shape=(14497, 11, 65), dataset_y.shape=(14497, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 301.3587 - val_loss: 295.8178\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 301.1783 - val_loss: 295.6527\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 301.0053 - val_loss: 295.4893\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 300.8333 - val_loss: 295.3270\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 300.6616 - val_loss: 295.1655\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 300.4905 - val_loss: 295.0054\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 300.3199 - val_loss: 294.8455\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 300.1493 - val_loss: 294.6857\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 299.9787 - val_loss: 294.5262\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 299.8081 - val_loss: 294.3667\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 299.6375 - val_loss: 294.2072\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 299.4669 - val_loss: 294.0477\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 299.2962 - val_loss: 293.8882\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 299.1256 - val_loss: 293.7286\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 298.9549 - val_loss: 293.5678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 298.7543 - val_loss: 293.3589\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 298.5459 - val_loss: 293.1713\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 298.3492 - val_loss: 292.9887\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 298.1572 - val_loss: 292.8082\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 297.9687 - val_loss: 292.6288\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 297.7819 - val_loss: 292.4476\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 297.5961 - val_loss: 292.2642\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 297.4106 - val_loss: 292.0719\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 297.2261 - val_loss: 291.8974\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 297.0419 - val_loss: 291.7231\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 296.8583 - val_loss: 291.5494\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 296.6748 - val_loss: 291.3759\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 296.4916 - val_loss: 291.2027\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 296.3087 - val_loss: 291.0295\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 296.1258 - val_loss: 290.8567\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8698, 2900, 2899, sum = 14497\n",
      "In split_to_train_test: dataset_X.shape=(107674, 11, 65), dataset_y.shape=(107674, 65)\n",
      "Epoch 1/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 878.9034 - val_loss: 879.4559\n",
      "Epoch 2/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 877.4025 - val_loss: 877.9625\n",
      "Epoch 3/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 875.9313 - val_loss: 876.4752\n",
      "Epoch 4/30\n",
      "1010/1010 [==============================] - 31s 30ms/step - loss: 874.4650 - val_loss: 874.9893\n",
      "Epoch 5/30\n",
      "1010/1010 [==============================] - 33s 33ms/step - loss: 873.0007 - val_loss: 873.5050\n",
      "Epoch 6/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 871.5379 - val_loss: 872.0245\n",
      "Epoch 7/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 870.0782 - val_loss: 870.5498\n",
      "Epoch 8/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 868.6252 - val_loss: 869.0930\n",
      "Epoch 9/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 867.1897 - val_loss: 867.6525\n",
      "Epoch 10/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 865.7587 - val_loss: 866.2186\n",
      "Epoch 11/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 864.3311 - val_loss: 864.7920\n",
      "Epoch 12/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 862.9093 - val_loss: 863.3763\n",
      "Epoch 13/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 861.4977 - val_loss: 861.9788\n",
      "Epoch 14/30\n",
      "1010/1010 [==============================] - 30s 30ms/step - loss: 860.0964 - val_loss: 860.5919\n",
      "Epoch 15/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 858.7004 - val_loss: 859.2142\n",
      "Epoch 16/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 857.3121 - val_loss: 857.8454\n",
      "Epoch 17/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 855.9301 - val_loss: 856.4827\n",
      "Epoch 18/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 854.5533 - val_loss: 855.1263\n",
      "Epoch 19/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 853.1823 - val_loss: 853.7749\n",
      "Epoch 20/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 851.8174 - val_loss: 852.4252\n",
      "Epoch 21/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 850.4575 - val_loss: 851.0834\n",
      "Epoch 22/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 849.1092 - val_loss: 849.7565\n",
      "Epoch 23/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 847.7736 - val_loss: 848.4346\n",
      "Epoch 24/30\n",
      "1010/1010 [==============================] - 31s 31ms/step - loss: 846.4409 - val_loss: 847.1142\n",
      "Epoch 25/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 845.1124 - val_loss: 845.7984\n",
      "Epoch 26/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 843.7869 - val_loss: 844.4879\n",
      "Epoch 27/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 842.4672 - val_loss: 843.1853\n",
      "Epoch 28/30\n",
      "1010/1010 [==============================] - 31s 30ms/step - loss: 841.1543 - val_loss: 841.8923\n",
      "Epoch 29/30\n",
      "1010/1010 [==============================] - 32s 31ms/step - loss: 839.8486 - val_loss: 840.6082\n",
      "Epoch 30/30\n",
      "1010/1010 [==============================] - 32s 32ms/step - loss: 838.5493 - val_loss: 839.3320\n",
      "673/673 [==============================] - 7s 11ms/step\n",
      "In calc_results: 64604, 21535, 21535, sum = 107674\n",
      "In split_to_train_test: dataset_X.shape=(83949, 11, 65), dataset_y.shape=(83949, 65)\n",
      "Epoch 1/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 863.0754 - val_loss: 862.5328\n",
      "Epoch 2/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 862.3298 - val_loss: 861.7852\n",
      "Epoch 3/30\n",
      "788/788 [==============================] - 25s 32ms/step - loss: 861.5888 - val_loss: 861.0392\n",
      "Epoch 4/30\n",
      "788/788 [==============================] - 24s 30ms/step - loss: 860.8488 - val_loss: 860.2939\n",
      "Epoch 5/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 860.1110 - val_loss: 859.5486\n",
      "Epoch 6/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 859.3730 - val_loss: 858.8040\n",
      "Epoch 7/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 858.6356 - val_loss: 858.0599\n",
      "Epoch 8/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 857.8996 - val_loss: 857.3162\n",
      "Epoch 9/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 857.1633 - val_loss: 856.5731\n",
      "Epoch 10/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 856.4285 - val_loss: 855.8311\n",
      "Epoch 11/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 855.6968 - val_loss: 855.0906\n",
      "Epoch 12/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 854.9657 - val_loss: 854.3531\n",
      "Epoch 13/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 854.2380 - val_loss: 853.6208\n",
      "Epoch 14/30\n",
      "788/788 [==============================] - 24s 30ms/step - loss: 853.5151 - val_loss: 852.8952\n",
      "Epoch 15/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 852.8004 - val_loss: 852.1735\n",
      "Epoch 16/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 852.0894 - val_loss: 851.4545\n",
      "Epoch 17/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 851.3816 - val_loss: 850.7415\n",
      "Epoch 18/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 850.6769 - val_loss: 850.0304\n",
      "Epoch 19/30\n",
      "788/788 [==============================] - 25s 32ms/step - loss: 849.9733 - val_loss: 849.3218\n",
      "Epoch 20/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 849.2720 - val_loss: 848.6144\n",
      "Epoch 21/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 848.5718 - val_loss: 847.9089\n",
      "Epoch 22/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 847.8718 - val_loss: 847.2052\n",
      "Epoch 23/30\n",
      "788/788 [==============================] - 25s 31ms/step - loss: 847.1736 - val_loss: 846.5036\n",
      "Epoch 24/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 846.4764 - val_loss: 845.8043\n",
      "Epoch 25/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 845.7804 - val_loss: 845.1058\n",
      "Epoch 26/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 845.0855 - val_loss: 844.4094\n",
      "Epoch 27/30\n",
      "788/788 [==============================] - 24s 31ms/step - loss: 844.3921 - val_loss: 843.7146\n",
      "Epoch 28/30\n",
      "788/788 [==============================] - 24s 30ms/step - loss: 843.6991 - val_loss: 843.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "788/788 [==============================] - 26s 33ms/step - loss: 843.0076 - val_loss: 842.3289\n",
      "Epoch 30/30\n",
      "788/788 [==============================] - 24s 30ms/step - loss: 842.3176 - val_loss: 841.6376\n",
      "525/525 [==============================] - 6s 10ms/step\n",
      "In calc_results: 50369, 16790, 16790, sum = 83949\n",
      "In split_to_train_test: dataset_X.shape=(5852, 11, 65), dataset_y.shape=(5852, 65)\n",
      "Epoch 1/30\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 1097.8651 - val_loss: 1101.9937\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.8071 - val_loss: 1101.9358\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.7496 - val_loss: 1101.8779\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.6917 - val_loss: 1101.8201\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.6339 - val_loss: 1101.7622\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 1097.5759 - val_loss: 1101.7043\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.5182 - val_loss: 1101.6465\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.4603 - val_loss: 1101.5887\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.4026 - val_loss: 1101.5308\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.3448 - val_loss: 1101.4729\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.2869 - val_loss: 1101.4152\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.2294 - val_loss: 1101.3573\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.1714 - val_loss: 1101.2993\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1097.1135 - val_loss: 1101.2415\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 1097.0557 - val_loss: 1101.1836\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 1096.9980 - val_loss: 1101.1259\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.9403 - val_loss: 1101.0677\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.8823 - val_loss: 1101.0101\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.8248 - val_loss: 1100.9523\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.7670 - val_loss: 1100.8944\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.7091 - val_loss: 1100.8364\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.6511 - val_loss: 1100.7784\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.5933 - val_loss: 1100.7208\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.5354 - val_loss: 1100.6631\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.4780 - val_loss: 1100.6050\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.4202 - val_loss: 1100.5471\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.3623 - val_loss: 1100.4894\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1096.3046 - val_loss: 1100.4315\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 1096.2468 - val_loss: 1100.3737\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 1096.1890 - val_loss: 1100.3157\n",
      "37/37 [==============================] - 0s 11ms/step\n",
      "In calc_results: 3511, 1171, 1170, sum = 5852\n",
      "In split_to_train_test: dataset_X.shape=(19813, 11, 65), dataset_y.shape=(19813, 65)\n",
      "Epoch 1/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 747.3904 - val_loss: 747.2590\n",
      "Epoch 2/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 747.2670 - val_loss: 747.1230\n",
      "Epoch 3/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 747.1379 - val_loss: 746.9982\n",
      "Epoch 4/30\n",
      "186/186 [==============================] - 6s 32ms/step - loss: 747.0142 - val_loss: 746.8751\n",
      "Epoch 5/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 746.8916 - val_loss: 746.7531\n",
      "Epoch 6/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 746.7694 - val_loss: 746.6312\n",
      "Epoch 7/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 746.6479 - val_loss: 746.5098\n",
      "Epoch 8/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 746.5263 - val_loss: 746.3885\n",
      "Epoch 9/30\n",
      "186/186 [==============================] - 6s 33ms/step - loss: 746.4048 - val_loss: 746.2672\n",
      "Epoch 10/30\n",
      "186/186 [==============================] - 6s 32ms/step - loss: 746.2834 - val_loss: 746.1462\n",
      "Epoch 11/30\n",
      "186/186 [==============================] - 6s 33ms/step - loss: 746.1624 - val_loss: 746.0251\n",
      "Epoch 12/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 746.0416 - val_loss: 745.9042\n",
      "Epoch 13/30\n",
      "186/186 [==============================] - 6s 32ms/step - loss: 745.9206 - val_loss: 745.7833\n",
      "Epoch 14/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 745.7994 - val_loss: 745.6626\n",
      "Epoch 15/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 745.6784 - val_loss: 745.5417\n",
      "Epoch 16/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 745.5576 - val_loss: 745.4209\n",
      "Epoch 17/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 745.4364 - val_loss: 745.3001\n",
      "Epoch 18/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 745.3158 - val_loss: 745.1794\n",
      "Epoch 19/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 745.1951 - val_loss: 745.0586\n",
      "Epoch 20/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 745.0743 - val_loss: 744.9380\n",
      "Epoch 21/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 744.9531 - val_loss: 744.8174\n",
      "Epoch 22/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 744.8327 - val_loss: 744.6966\n",
      "Epoch 23/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 744.7117 - val_loss: 744.5759\n",
      "Epoch 24/30\n",
      "186/186 [==============================] - 6s 31ms/step - loss: 744.5910 - val_loss: 744.4551\n",
      "Epoch 25/30\n",
      "186/186 [==============================] - 6s 32ms/step - loss: 744.4702 - val_loss: 744.3344\n",
      "Epoch 26/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 744.3494 - val_loss: 744.2137\n",
      "Epoch 27/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 744.2288 - val_loss: 744.0930\n",
      "Epoch 28/30\n",
      "186/186 [==============================] - 6s 30ms/step - loss: 744.1080 - val_loss: 743.9724\n",
      "Epoch 29/30\n",
      "186/186 [==============================] - 6s 34ms/step - loss: 743.9871 - val_loss: 743.8516\n",
      "Epoch 30/30\n",
      "186/186 [==============================] - 6s 34ms/step - loss: 743.8663 - val_loss: 743.7310\n",
      "124/124 [==============================] - 1s 11ms/step\n",
      "In calc_results: 11888, 3962, 3963, sum = 19813\n",
      "In split_to_train_test: dataset_X.shape=(17468, 11, 65), dataset_y.shape=(17468, 65)\n",
      "Epoch 1/30\n",
      "164/164 [==============================] - 6s 35ms/step - loss: 1031.7743 - val_loss: 1064.5627\n",
      "Epoch 2/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1031.5726 - val_loss: 1064.3607\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1031.3704 - val_loss: 1064.1588\n",
      "Epoch 4/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1031.1685 - val_loss: 1063.9569\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1030.9663 - val_loss: 1063.7552\n",
      "Epoch 6/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1030.7648 - val_loss: 1063.5533\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1030.5620 - val_loss: 1063.3518\n",
      "Epoch 8/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1030.3602 - val_loss: 1063.1499\n",
      "Epoch 9/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1030.1581 - val_loss: 1062.9485\n",
      "Epoch 10/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1029.9565 - val_loss: 1062.7467\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 5s 32ms/step - loss: 1029.7542 - val_loss: 1062.5450\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1029.5524 - val_loss: 1062.3438\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1029.3502 - val_loss: 1062.1425\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1029.1486 - val_loss: 1061.9408\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1028.9463 - val_loss: 1061.7393\n",
      "Epoch 16/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1028.7443 - val_loss: 1061.5380\n",
      "Epoch 17/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1028.5426 - val_loss: 1061.3365\n",
      "Epoch 18/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1028.3409 - val_loss: 1061.1350\n",
      "Epoch 19/30\n",
      "164/164 [==============================] - 5s 33ms/step - loss: 1028.1384 - val_loss: 1060.9335\n",
      "Epoch 20/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1027.9368 - val_loss: 1060.7321\n",
      "Epoch 21/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1027.7346 - val_loss: 1060.5306\n",
      "Epoch 22/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1027.5330 - val_loss: 1060.3293\n",
      "Epoch 23/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1027.3313 - val_loss: 1060.1279\n",
      "Epoch 24/30\n",
      "164/164 [==============================] - 5s 30ms/step - loss: 1027.1292 - val_loss: 1059.9268\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1026.9272 - val_loss: 1059.7255\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1026.7258 - val_loss: 1059.5243\n",
      "Epoch 27/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1026.5236 - val_loss: 1059.3231\n",
      "Epoch 28/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1026.3217 - val_loss: 1059.1217\n",
      "Epoch 29/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1026.1199 - val_loss: 1058.9205\n",
      "Epoch 30/30\n",
      "164/164 [==============================] - 5s 31ms/step - loss: 1025.9176 - val_loss: 1058.7196\n",
      "110/110 [==============================] - 1s 10ms/step\n",
      "In calc_results: 10481, 3493, 3494, sum = 17468\n",
      "N_clusters=5\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(12430, 11, 65), dataset_y.shape=(12430, 65)\n",
      "Epoch 1/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1091.5762 - val_loss: 1101.8511\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1091.4677 - val_loss: 1101.7419\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1091.3585 - val_loss: 1101.6327\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1091.2495 - val_loss: 1101.5238\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1091.1403 - val_loss: 1101.4147\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 4s 34ms/step - loss: 1091.0315 - val_loss: 1101.3057\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1090.9225 - val_loss: 1101.1964\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1090.8136 - val_loss: 1101.0872\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.7045 - val_loss: 1100.9780\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.5957 - val_loss: 1100.8689\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.4865 - val_loss: 1100.7598\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1090.3781 - val_loss: 1100.6505\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1090.2686 - val_loss: 1100.5415\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1090.1598 - val_loss: 1100.4324\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1090.0507 - val_loss: 1100.3231\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1089.9418 - val_loss: 1100.2140\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 4s 33ms/step - loss: 1089.8328 - val_loss: 1100.1047\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1089.7236 - val_loss: 1099.9957\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1089.6147 - val_loss: 1099.8866\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1089.5057 - val_loss: 1099.7775\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1089.3970 - val_loss: 1099.6683\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1089.2878 - val_loss: 1099.5591\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 4s 32ms/step - loss: 1089.1790 - val_loss: 1099.4501\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1089.0698 - val_loss: 1099.3411\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1088.9609 - val_loss: 1099.2318\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1088.8519 - val_loss: 1099.1224\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1088.7426 - val_loss: 1099.0134\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1088.6338 - val_loss: 1098.9042\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 4s 30ms/step - loss: 1088.5249 - val_loss: 1098.7949\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 4s 31ms/step - loss: 1088.4158 - val_loss: 1098.6858\n",
      "78/78 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7458, 2486, 2486, sum = 12430\n",
      "In split_to_train_test: dataset_X.shape=(30905, 11, 65), dataset_y.shape=(30905, 65)\n",
      "Epoch 1/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 1005.9836 - val_loss: 1031.3289\n",
      "Epoch 2/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1005.6550 - val_loss: 1031.0021\n",
      "Epoch 3/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 1005.3261 - val_loss: 1030.6749\n",
      "Epoch 4/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1004.9982 - val_loss: 1030.3479\n",
      "Epoch 5/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1004.6696 - val_loss: 1030.0208\n",
      "Epoch 6/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1004.3414 - val_loss: 1029.6938\n",
      "Epoch 7/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1004.0126 - val_loss: 1029.3671\n",
      "Epoch 8/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 1003.6849 - val_loss: 1029.0402\n",
      "Epoch 9/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 1003.3573 - val_loss: 1028.7136\n",
      "Epoch 10/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 1003.0279 - val_loss: 1028.3870\n",
      "Epoch 11/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 1002.7000 - val_loss: 1028.0604\n",
      "Epoch 12/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1002.3716 - val_loss: 1027.7338\n",
      "Epoch 13/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 1002.0433 - val_loss: 1027.4072\n",
      "Epoch 14/30\n",
      "290/290 [==============================] - 10s 34ms/step - loss: 1001.7154 - val_loss: 1027.0808\n",
      "Epoch 15/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 1001.3873 - val_loss: 1026.7545\n",
      "Epoch 16/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 1001.0595 - val_loss: 1026.4280\n",
      "Epoch 17/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 1000.7310 - val_loss: 1026.1016\n",
      "Epoch 18/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1000.4030 - val_loss: 1025.7751\n",
      "Epoch 19/30\n",
      "290/290 [==============================] - 10s 33ms/step - loss: 1000.0751 - val_loss: 1025.4487\n",
      "Epoch 20/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 999.7474 - val_loss: 1025.1224\n",
      "Epoch 21/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 999.4193 - val_loss: 1024.7960\n",
      "Epoch 22/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 999.0915 - val_loss: 1024.4698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 998.7637 - val_loss: 1024.1436\n",
      "Epoch 24/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 998.4362 - val_loss: 1023.8171\n",
      "Epoch 25/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 998.1085 - val_loss: 1023.4910\n",
      "Epoch 26/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 997.7805 - val_loss: 1023.1649\n",
      "Epoch 27/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 997.4542 - val_loss: 1022.8389\n",
      "Epoch 28/30\n",
      "290/290 [==============================] - 9s 32ms/step - loss: 997.1271 - val_loss: 1022.5128\n",
      "Epoch 29/30\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 996.8002 - val_loss: 1022.1868\n",
      "Epoch 30/30\n",
      "290/290 [==============================] - 9s 30ms/step - loss: 996.4730 - val_loss: 1021.8611\n",
      "194/194 [==============================] - 2s 11ms/step\n",
      "In calc_results: 18543, 6181, 6181, sum = 30905\n",
      "In split_to_train_test: dataset_X.shape=(253887, 11, 65), dataset_y.shape=(253887, 65)\n",
      "Epoch 1/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 718.8926 - val_loss: 717.7216\n",
      "Epoch 2/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 716.8277 - val_loss: 715.5403\n",
      "Epoch 3/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 714.5831 - val_loss: 713.2607\n",
      "Epoch 4/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 712.3561 - val_loss: 711.0203\n",
      "Epoch 5/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 710.1613 - val_loss: 708.8278\n",
      "Epoch 6/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 707.9915 - val_loss: 706.6628\n",
      "Epoch 7/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 705.8471 - val_loss: 704.5239\n",
      "Epoch 8/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 703.7160 - val_loss: 702.4014\n",
      "Epoch 9/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 701.5967 - val_loss: 700.3000\n",
      "Epoch 10/30\n",
      "2381/2381 [==============================] - 74s 31ms/step - loss: 699.4958 - val_loss: 698.2344\n",
      "Epoch 11/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 697.4277 - val_loss: 696.1954\n",
      "Epoch 12/30\n",
      "2381/2381 [==============================] - 80s 34ms/step - loss: 695.3729 - val_loss: 694.1694\n",
      "Epoch 13/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 693.3265 - val_loss: 692.1514\n",
      "Epoch 14/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 691.2933 - val_loss: 690.1387\n",
      "Epoch 15/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 689.2708 - val_loss: 688.1301\n",
      "Epoch 16/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 687.2583 - val_loss: 686.1328\n",
      "Epoch 17/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 685.1696 - val_loss: 683.8818\n",
      "Epoch 18/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 682.8838 - val_loss: 681.7130\n",
      "Epoch 19/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 680.7226 - val_loss: 679.5892\n",
      "Epoch 20/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 678.5997 - val_loss: 677.5050\n",
      "Epoch 21/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 676.5134 - val_loss: 675.4714\n",
      "Epoch 22/30\n",
      "2381/2381 [==============================] - 79s 33ms/step - loss: 674.4656 - val_loss: 673.4946\n",
      "Epoch 23/30\n",
      "2381/2381 [==============================] - 78s 33ms/step - loss: 672.4527 - val_loss: 671.5717\n",
      "Epoch 24/30\n",
      "2381/2381 [==============================] - 76s 32ms/step - loss: 670.4769 - val_loss: 669.6926\n",
      "Epoch 25/30\n",
      "2381/2381 [==============================] - 81s 34ms/step - loss: 668.5370 - val_loss: 667.8398\n",
      "Epoch 26/30\n",
      "2381/2381 [==============================] - 81s 34ms/step - loss: 666.6274 - val_loss: 666.0033\n",
      "Epoch 27/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 664.7426 - val_loss: 664.1681\n",
      "Epoch 28/30\n",
      "2381/2381 [==============================] - 77s 32ms/step - loss: 662.8775 - val_loss: 662.3343\n",
      "Epoch 29/30\n",
      "2381/2381 [==============================] - 75s 32ms/step - loss: 661.0267 - val_loss: 660.5082\n",
      "Epoch 30/30\n",
      "2381/2381 [==============================] - 77s 33ms/step - loss: 659.1938 - val_loss: 658.6934\n",
      "1587/1587 [==============================] - 17s 11ms/step\n",
      "In calc_results: 152332, 50778, 50777, sum = 253887\n",
      "In split_to_train_test: dataset_X.shape=(11994, 11, 65), dataset_y.shape=(11994, 65)\n",
      "Epoch 1/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 165.3590 - val_loss: 163.4991\n",
      "Epoch 2/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 165.2447 - val_loss: 163.3825\n",
      "Epoch 3/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 165.1271 - val_loss: 163.2761\n",
      "Epoch 4/30\n",
      "113/113 [==============================] - 4s 35ms/step - loss: 165.0117 - val_loss: 163.1778\n",
      "Epoch 5/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.9063 - val_loss: 163.0855\n",
      "Epoch 6/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.8059 - val_loss: 162.9975\n",
      "Epoch 7/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.7072 - val_loss: 162.9111\n",
      "Epoch 8/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.6095 - val_loss: 162.8262\n",
      "Epoch 9/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.5126 - val_loss: 162.7422\n",
      "Epoch 10/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.4161 - val_loss: 162.6587\n",
      "Epoch 11/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.3199 - val_loss: 162.5758\n",
      "Epoch 12/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 164.2238 - val_loss: 162.4933\n",
      "Epoch 13/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.1280 - val_loss: 162.4111\n",
      "Epoch 14/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 164.0322 - val_loss: 162.3292\n",
      "Epoch 15/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.9366 - val_loss: 162.2479\n",
      "Epoch 16/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.8411 - val_loss: 162.1666\n",
      "Epoch 17/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 163.7457 - val_loss: 162.0852\n",
      "Epoch 18/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.6504 - val_loss: 162.0045\n",
      "Epoch 19/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 163.5551 - val_loss: 161.9239\n",
      "Epoch 20/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.4600 - val_loss: 161.8435\n",
      "Epoch 21/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.3649 - val_loss: 161.7636\n",
      "Epoch 22/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.2701 - val_loss: 161.6839\n",
      "Epoch 23/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.1760 - val_loss: 161.6052\n",
      "Epoch 24/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 163.0828 - val_loss: 161.5268\n",
      "Epoch 25/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 162.9897 - val_loss: 161.4484\n",
      "Epoch 26/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 162.8965 - val_loss: 161.3702\n",
      "Epoch 27/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 162.8033 - val_loss: 161.2920\n",
      "Epoch 28/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 162.7103 - val_loss: 161.2139\n",
      "Epoch 29/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 162.6172 - val_loss: 161.1360\n",
      "Epoch 30/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 162.5246 - val_loss: 161.0585\n",
      "75/75 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7196, 2399, 2399, sum = 11994\n",
      "In split_to_train_test: dataset_X.shape=(17250, 11, 65), dataset_y.shape=(17250, 65)\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 5s 32ms/step - loss: 12448368928227328.0000 - val_loss: 12688559605547008.0000\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 5s 31ms/step - loss: 12448370001969152.0000 - val_loss: 12688559605547008.0000\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/162 [============================>.] - ETA: 0s - loss: 12448511735889920.0000Restoring model weights from the end of the best epoch: 1.\n",
      "162/162 [==============================] - 5s 32ms/step - loss: 12448368928227328.0000 - val_loss: 12688559605547008.0000\n",
      "Epoch 3: early stopping\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10350, 3450, 3450, sum = 17250\n",
      "N_clusters=7\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(174947, 11, 65), dataset_y.shape=(174947, 65)\n",
      "Epoch 1/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 796.5589 - val_loss: 795.8486\n",
      "Epoch 2/30\n",
      "1641/1641 [==============================] - 54s 33ms/step - loss: 795.1807 - val_loss: 794.4849\n",
      "Epoch 3/30\n",
      "1641/1641 [==============================] - 54s 33ms/step - loss: 793.8420 - val_loss: 793.1320\n",
      "Epoch 4/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 792.5062 - val_loss: 791.7815\n",
      "Epoch 5/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 791.1743 - val_loss: 790.4327\n",
      "Epoch 6/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 789.8455 - val_loss: 789.0877\n",
      "Epoch 7/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 788.5219 - val_loss: 787.7558\n",
      "Epoch 8/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 787.2164 - val_loss: 786.4435\n",
      "Epoch 9/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 785.9197 - val_loss: 785.1385\n",
      "Epoch 10/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 784.6335 - val_loss: 783.8484\n",
      "Epoch 11/30\n",
      "1641/1641 [==============================] - 55s 33ms/step - loss: 783.3574 - val_loss: 782.5624\n",
      "Epoch 12/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 782.0845 - val_loss: 781.2812\n",
      "Epoch 13/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 780.8153 - val_loss: 780.0065\n",
      "Epoch 14/30\n",
      "1641/1641 [==============================] - 54s 33ms/step - loss: 779.5507 - val_loss: 778.7394\n",
      "Epoch 15/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 778.2902 - val_loss: 777.4794\n",
      "Epoch 16/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 777.0379 - val_loss: 776.2301\n",
      "Epoch 17/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 775.7986 - val_loss: 774.9995\n",
      "Epoch 18/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 774.5709 - val_loss: 773.7726\n",
      "Epoch 19/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 773.3457 - val_loss: 772.5508\n",
      "Epoch 20/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 772.1257 - val_loss: 771.3317\n",
      "Epoch 21/30\n",
      "1641/1641 [==============================] - 51s 31ms/step - loss: 770.9121 - val_loss: 770.1149\n",
      "Epoch 22/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 769.7011 - val_loss: 768.9006\n",
      "Epoch 23/30\n",
      "1641/1641 [==============================] - 54s 33ms/step - loss: 768.4954 - val_loss: 767.6888\n",
      "Epoch 24/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 767.2921 - val_loss: 766.4780\n",
      "Epoch 25/30\n",
      "1641/1641 [==============================] - 55s 33ms/step - loss: 766.0903 - val_loss: 765.2725\n",
      "Epoch 26/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 764.8939 - val_loss: 764.0724\n",
      "Epoch 27/30\n",
      "1641/1641 [==============================] - 53s 32ms/step - loss: 763.6990 - val_loss: 762.8806\n",
      "Epoch 28/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 762.5105 - val_loss: 761.6984\n",
      "Epoch 29/30\n",
      "1641/1641 [==============================] - 52s 32ms/step - loss: 761.3306 - val_loss: 760.5237\n",
      "Epoch 30/30\n",
      "1641/1641 [==============================] - 56s 34ms/step - loss: 760.1568 - val_loss: 759.3578\n",
      "1094/1094 [==============================] - 12s 11ms/step\n",
      "In calc_results: 104968, 34990, 34989, sum = 174947\n",
      "In split_to_train_test: dataset_X.shape=(12027, 11, 65), dataset_y.shape=(12027, 65)\n",
      "Epoch 1/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1102.6021 - val_loss: 1112.4309\n",
      "Epoch 2/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1102.4659 - val_loss: 1112.2966\n",
      "Epoch 3/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1102.3323 - val_loss: 1112.1628\n",
      "Epoch 4/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1102.1980 - val_loss: 1112.0286\n",
      "Epoch 5/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1102.0642 - val_loss: 1111.8948\n",
      "Epoch 6/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1101.9304 - val_loss: 1111.7609\n",
      "Epoch 7/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1101.7969 - val_loss: 1111.6271\n",
      "Epoch 8/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1101.6627 - val_loss: 1111.4933\n",
      "Epoch 9/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1101.5292 - val_loss: 1111.3593\n",
      "Epoch 10/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1101.3956 - val_loss: 1111.2255\n",
      "Epoch 11/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1101.2616 - val_loss: 1111.0916\n",
      "Epoch 12/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1101.1279 - val_loss: 1110.9578\n",
      "Epoch 13/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1100.9946 - val_loss: 1110.8237\n",
      "Epoch 14/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1100.8608 - val_loss: 1110.6896\n",
      "Epoch 15/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1100.7269 - val_loss: 1110.5559\n",
      "Epoch 16/30\n",
      "113/113 [==============================] - 4s 35ms/step - loss: 1100.5933 - val_loss: 1110.4218\n",
      "Epoch 17/30\n",
      "113/113 [==============================] - 4s 35ms/step - loss: 1100.4597 - val_loss: 1110.2881\n",
      "Epoch 18/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1100.3262 - val_loss: 1110.1542\n",
      "Epoch 19/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1100.1924 - val_loss: 1110.0203\n",
      "Epoch 20/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1100.0586 - val_loss: 1109.8862\n",
      "Epoch 21/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1099.9246 - val_loss: 1109.7523\n",
      "Epoch 22/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1099.7908 - val_loss: 1109.6184\n",
      "Epoch 23/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1099.6572 - val_loss: 1109.4844\n",
      "Epoch 24/30\n",
      "113/113 [==============================] - 4s 34ms/step - loss: 1099.5234 - val_loss: 1109.3506\n",
      "Epoch 25/30\n",
      "113/113 [==============================] - 4s 33ms/step - loss: 1099.3900 - val_loss: 1109.2167\n",
      "Epoch 26/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1099.2560 - val_loss: 1109.0828\n",
      "Epoch 27/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1099.1227 - val_loss: 1108.9487\n",
      "Epoch 28/30\n",
      "113/113 [==============================] - 4s 31ms/step - loss: 1098.9886 - val_loss: 1108.8147\n",
      "Epoch 29/30\n",
      "113/113 [==============================] - 3s 31ms/step - loss: 1098.8547 - val_loss: 1108.6809\n",
      "Epoch 30/30\n",
      "113/113 [==============================] - 4s 32ms/step - loss: 1098.7214 - val_loss: 1108.5470\n",
      "76/76 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7216, 2406, 2405, sum = 12027\n",
      "In split_to_train_test: dataset_X.shape=(8013, 11, 65), dataset_y.shape=(8013, 65)\n",
      "Epoch 1/30\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 149.3812 - val_loss: 150.4457\n",
      "Epoch 2/30\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 149.2338 - val_loss: 150.3206\n",
      "Epoch 3/30\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 149.1076 - val_loss: 150.2066\n",
      "Epoch 4/30\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 148.9828 - val_loss: 150.0941\n",
      "Epoch 5/30\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 148.8640 - val_loss: 149.9876\n",
      "Epoch 6/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 148.7494 - val_loss: 149.8824\n",
      "Epoch 7/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 148.6360 - val_loss: 149.7781\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 2s 31ms/step - loss: 148.5235 - val_loss: 149.6743\n",
      "Epoch 9/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 148.4113 - val_loss: 149.5710\n",
      "Epoch 10/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 148.2997 - val_loss: 149.4681\n",
      "Epoch 11/30\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 148.1887 - val_loss: 149.3664\n",
      "Epoch 12/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 148.0782 - val_loss: 149.2653\n",
      "Epoch 13/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.9678 - val_loss: 149.1644\n",
      "Epoch 14/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.8577 - val_loss: 149.0635\n",
      "Epoch 15/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.7479 - val_loss: 148.9628\n",
      "Epoch 16/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.6381 - val_loss: 148.8618\n",
      "Epoch 17/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.5286 - val_loss: 148.7614\n",
      "Epoch 18/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.4197 - val_loss: 148.6617\n",
      "Epoch 19/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.3121 - val_loss: 148.5642\n",
      "Epoch 20/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.2053 - val_loss: 148.4664\n",
      "Epoch 21/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 147.0990 - val_loss: 148.3694\n",
      "Epoch 22/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.9934 - val_loss: 148.2731\n",
      "Epoch 23/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.8887 - val_loss: 148.1783\n",
      "Epoch 24/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.7854 - val_loss: 148.0846\n",
      "Epoch 25/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.6828 - val_loss: 147.9913\n",
      "Epoch 26/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.5807 - val_loss: 147.8979\n",
      "Epoch 27/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.4787 - val_loss: 147.8050\n",
      "Epoch 28/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.3774 - val_loss: 147.7120\n",
      "Epoch 29/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.2761 - val_loss: 147.6192\n",
      "Epoch 30/30\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 146.1749 - val_loss: 147.5266\n",
      "51/51 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4808, 1602, 1603, sum = 8013\n",
      "In split_to_train_test: dataset_X.shape=(85366, 11, 65), dataset_y.shape=(85366, 65)\n",
      "Epoch 1/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 571.4907 - val_loss: 568.5402\n",
      "Epoch 2/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 570.3742 - val_loss: 567.4279\n",
      "Epoch 3/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 569.2606 - val_loss: 566.3171\n",
      "Epoch 4/30\n",
      "801/801 [==============================] - 25s 32ms/step - loss: 568.1489 - val_loss: 565.2083\n",
      "Epoch 5/30\n",
      "801/801 [==============================] - 25s 32ms/step - loss: 567.0385 - val_loss: 564.1011\n",
      "Epoch 6/30\n",
      "801/801 [==============================] - 25s 32ms/step - loss: 565.9297 - val_loss: 562.9968\n",
      "Epoch 7/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 564.8209 - val_loss: 561.8975\n",
      "Epoch 8/30\n",
      "801/801 [==============================] - 24s 30ms/step - loss: 563.7156 - val_loss: 560.8118\n",
      "Epoch 9/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 562.6173 - val_loss: 559.7365\n",
      "Epoch 10/30\n",
      "801/801 [==============================] - 25s 32ms/step - loss: 561.5208 - val_loss: 558.6674\n",
      "Epoch 11/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 560.4260 - val_loss: 557.6058\n",
      "Epoch 12/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 559.3331 - val_loss: 556.5561\n",
      "Epoch 13/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 558.2458 - val_loss: 555.5151\n",
      "Epoch 14/30\n",
      "801/801 [==============================] - 27s 33ms/step - loss: 557.1610 - val_loss: 554.4782\n",
      "Epoch 15/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 556.0782 - val_loss: 553.4444\n",
      "Epoch 16/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 554.9979 - val_loss: 552.4148\n",
      "Epoch 17/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 553.9216 - val_loss: 551.3912\n",
      "Epoch 18/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 552.8486 - val_loss: 550.3765\n",
      "Epoch 19/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 551.7830 - val_loss: 549.3740\n",
      "Epoch 20/30\n",
      "801/801 [==============================] - 26s 32ms/step - loss: 550.6189 - val_loss: 548.0434\n",
      "Epoch 21/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 549.2937 - val_loss: 547.0223\n",
      "Epoch 22/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 548.2168 - val_loss: 546.0096\n",
      "Epoch 23/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 547.1473 - val_loss: 545.0026\n",
      "Epoch 24/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 546.0828 - val_loss: 543.9993\n",
      "Epoch 25/30\n",
      "801/801 [==============================] - 26s 33ms/step - loss: 545.0244 - val_loss: 542.9988\n",
      "Epoch 26/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 543.9702 - val_loss: 542.0033\n",
      "Epoch 27/30\n",
      "801/801 [==============================] - 25s 32ms/step - loss: 542.9217 - val_loss: 541.0130\n",
      "Epoch 28/30\n",
      "801/801 [==============================] - 27s 34ms/step - loss: 541.8788 - val_loss: 540.0308\n",
      "Epoch 29/30\n",
      "801/801 [==============================] - 27s 34ms/step - loss: 540.8464 - val_loss: 539.0571\n",
      "Epoch 30/30\n",
      "801/801 [==============================] - 25s 31ms/step - loss: 539.8203 - val_loss: 538.0909\n",
      "534/534 [==============================] - 6s 11ms/step\n",
      "In calc_results: 51220, 17073, 17073, sum = 85366\n",
      "In split_to_train_test: dataset_X.shape=(14524, 11, 65), dataset_y.shape=(14524, 65)\n",
      "Epoch 1/30\n",
      "137/137 [==============================] - 5s 33ms/step - loss: 12516307794657280.0000 - val_loss: 12548832508248064.0000\n",
      "Epoch 2/30\n",
      "137/137 [==============================] - 5s 34ms/step - loss: 12516309942140928.0000 - val_loss: 12548832508248064.0000\n",
      "Epoch 3/30\n",
      "137/137 [==============================] - ETA: 0s - loss: 12516305647173632.0000Restoring model weights from the end of the best epoch: 1.\n",
      "137/137 [==============================] - 5s 35ms/step - loss: 12516305647173632.0000 - val_loss: 12548832508248064.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8714, 2905, 2905, sum = 14524\n",
      "In split_to_train_test: dataset_X.shape=(2614, 11, 65), dataset_y.shape=(2614, 65)\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 48073530289422336.0000Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3: early stopping\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "In calc_results: 1568, 523, 523, sum = 2614\n",
      "In split_to_train_test: dataset_X.shape=(28975, 11, 65), dataset_y.shape=(28975, 65)\n",
      "Epoch 1/30\n",
      "272/272 [==============================] - 9s 31ms/step - loss: 1021.8694 - val_loss: 1047.5970\n",
      "Epoch 2/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1021.4333 - val_loss: 1047.1626\n",
      "Epoch 3/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 1020.9967 - val_loss: 1046.7279\n",
      "Epoch 4/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 1020.5609 - val_loss: 1046.2936\n",
      "Epoch 5/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1020.1246 - val_loss: 1045.8593\n",
      "Epoch 6/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1019.6886 - val_loss: 1045.4250\n",
      "Epoch 7/30\n",
      "272/272 [==============================] - 9s 34ms/step - loss: 1019.2524 - val_loss: 1044.9907\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 9s 31ms/step - loss: 1018.8165 - val_loss: 1044.5573\n",
      "Epoch 9/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1018.3806 - val_loss: 1044.1237\n",
      "Epoch 10/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1017.9446 - val_loss: 1043.6901\n",
      "Epoch 11/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1017.5081 - val_loss: 1043.2565\n",
      "Epoch 12/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1017.0724 - val_loss: 1042.8230\n",
      "Epoch 13/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1016.6364 - val_loss: 1042.3890\n",
      "Epoch 14/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1016.2009 - val_loss: 1041.9557\n",
      "Epoch 15/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1015.7651 - val_loss: 1041.5222\n",
      "Epoch 16/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1015.3294 - val_loss: 1041.0894\n",
      "Epoch 17/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 1014.8939 - val_loss: 1040.6556\n",
      "Epoch 18/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1014.4579 - val_loss: 1040.2227\n",
      "Epoch 19/30\n",
      "272/272 [==============================] - 9s 34ms/step - loss: 1014.0225 - val_loss: 1039.7894\n",
      "Epoch 20/30\n",
      "272/272 [==============================] - 9s 34ms/step - loss: 1013.5871 - val_loss: 1039.3560\n",
      "Epoch 21/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1013.1517 - val_loss: 1038.9233\n",
      "Epoch 22/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1012.7173 - val_loss: 1038.4907\n",
      "Epoch 23/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1012.2822 - val_loss: 1038.0579\n",
      "Epoch 24/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1011.8481 - val_loss: 1037.6255\n",
      "Epoch 25/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1011.4139 - val_loss: 1037.1929\n",
      "Epoch 26/30\n",
      "272/272 [==============================] - 8s 30ms/step - loss: 1010.9798 - val_loss: 1036.7606\n",
      "Epoch 27/30\n",
      "272/272 [==============================] - 8s 31ms/step - loss: 1010.5462 - val_loss: 1036.3287\n",
      "Epoch 28/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1010.1130 - val_loss: 1035.8969\n",
      "Epoch 29/30\n",
      "272/272 [==============================] - 9s 33ms/step - loss: 1009.6829 - val_loss: 1035.4690\n",
      "Epoch 30/30\n",
      "272/272 [==============================] - 9s 32ms/step - loss: 1009.2562 - val_loss: 1035.0419\n",
      "182/182 [==============================] - 2s 11ms/step\n",
      "In calc_results: 17385, 5795, 5795, sum = 28975\n",
      "N_clusters=9\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(23000, 11, 65), dataset_y.shape=(23000, 65)\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 1018.6982 - val_loss: 1042.7926\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1018.4138 - val_loss: 1042.5110\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1018.1320 - val_loss: 1042.2302\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1017.8505 - val_loss: 1041.9492\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1017.5688 - val_loss: 1041.6683\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 1017.2866 - val_loss: 1041.3875\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 1017.0062 - val_loss: 1041.1067\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1016.7246 - val_loss: 1040.8259\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1016.4434 - val_loss: 1040.5452\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 1016.1625 - val_loss: 1040.2648\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1015.8813 - val_loss: 1039.9840\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1015.6005 - val_loss: 1039.7034\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1015.3193 - val_loss: 1039.4226\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1015.0381 - val_loss: 1039.1421\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1014.7576 - val_loss: 1038.8615\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1014.4759 - val_loss: 1038.5807\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1014.1951 - val_loss: 1038.2998\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1013.9145 - val_loss: 1038.0189\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 1013.6328 - val_loss: 1037.7382\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 1013.3517 - val_loss: 1037.4575\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 1013.0709 - val_loss: 1037.1769\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 1012.7898 - val_loss: 1036.8960\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 1012.5090 - val_loss: 1036.6152\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 1012.2279 - val_loss: 1036.3346\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1011.9471 - val_loss: 1036.0541\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1011.6663 - val_loss: 1035.7732\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1011.3856 - val_loss: 1035.4926\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1011.1042 - val_loss: 1035.2120\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 7s 30ms/step - loss: 1010.8238 - val_loss: 1034.9312\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 1010.5428 - val_loss: 1034.6508\n",
      "144/144 [==============================] - 2s 11ms/step\n",
      "In calc_results: 13800, 4600, 4600, sum = 23000\n",
      "In split_to_train_test: dataset_X.shape=(14522, 11, 65), dataset_y.shape=(14522, 65)\n",
      "Epoch 1/30\n",
      "137/137 [==============================] - 5s 35ms/step - loss: 12516291688529920.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 2/30\n",
      "137/137 [==============================] - 4s 33ms/step - loss: 12516291688529920.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 3/30\n",
      "137/137 [==============================] - ETA: 0s - loss: 12516290614788096.0000Restoring model weights from the end of the best epoch: 1.\n",
      "137/137 [==============================] - 4s 33ms/step - loss: 12516290614788096.0000 - val_loss: 12548767009996800.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8713, 2905, 2904, sum = 14522\n",
      "In split_to_train_test: dataset_X.shape=(109412, 11, 65), dataset_y.shape=(109412, 65)\n",
      "Epoch 1/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 799.0066 - val_loss: 797.6556\n",
      "Epoch 2/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 798.0684 - val_loss: 796.7087\n",
      "Epoch 3/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 797.1320 - val_loss: 795.7626\n",
      "Epoch 4/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 796.1959 - val_loss: 794.8163\n",
      "Epoch 5/30\n",
      "1026/1026 [==============================] - 34s 33ms/step - loss: 795.2352 - val_loss: 793.7558\n",
      "Epoch 6/30\n",
      "1026/1026 [==============================] - 31s 30ms/step - loss: 794.1384 - val_loss: 792.6734\n",
      "Epoch 7/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 793.0888 - val_loss: 791.6222\n",
      "Epoch 8/30\n",
      "1026/1026 [==============================] - 32s 32ms/step - loss: 792.0563 - val_loss: 790.5852\n",
      "Epoch 9/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 791.0369 - val_loss: 789.5620\n",
      "Epoch 10/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 790.0297 - val_loss: 788.5468\n",
      "Epoch 11/30\n",
      "1026/1026 [==============================] - 31s 31ms/step - loss: 789.0326 - val_loss: 787.5416\n",
      "Epoch 12/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 788.0392 - val_loss: 786.5392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 787.0482 - val_loss: 785.5403\n",
      "Epoch 14/30\n",
      "1026/1026 [==============================] - 35s 34ms/step - loss: 786.0608 - val_loss: 784.5441\n",
      "Epoch 15/30\n",
      "1026/1026 [==============================] - 34s 33ms/step - loss: 785.0757 - val_loss: 783.5515\n",
      "Epoch 16/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 784.0912 - val_loss: 782.5626\n",
      "Epoch 17/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 783.1080 - val_loss: 781.5772\n",
      "Epoch 18/30\n",
      "1026/1026 [==============================] - 34s 34ms/step - loss: 782.1290 - val_loss: 780.5968\n",
      "Epoch 19/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 781.1575 - val_loss: 779.6268\n",
      "Epoch 20/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 780.0842 - val_loss: 778.1632\n",
      "Epoch 21/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 778.6688 - val_loss: 777.0763\n",
      "Epoch 22/30\n",
      "1026/1026 [==============================] - 31s 30ms/step - loss: 777.5983 - val_loss: 776.0197\n",
      "Epoch 23/30\n",
      "1026/1026 [==============================] - 34s 33ms/step - loss: 776.5457 - val_loss: 774.9716\n",
      "Epoch 24/30\n",
      "1026/1026 [==============================] - 34s 34ms/step - loss: 775.4997 - val_loss: 773.9272\n",
      "Epoch 25/30\n",
      "1026/1026 [==============================] - 35s 34ms/step - loss: 774.4589 - val_loss: 772.8849\n",
      "Epoch 26/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 773.4231 - val_loss: 771.8442\n",
      "Epoch 27/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 772.3901 - val_loss: 770.8043\n",
      "Epoch 28/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 771.3627 - val_loss: 769.7641\n",
      "Epoch 29/30\n",
      "1026/1026 [==============================] - 33s 32ms/step - loss: 770.3364 - val_loss: 768.7256\n",
      "Epoch 30/30\n",
      "1026/1026 [==============================] - 32s 31ms/step - loss: 769.3138 - val_loss: 767.6897\n",
      "684/684 [==============================] - 8s 11ms/step\n",
      "In calc_results: 65647, 21883, 21882, sum = 109412\n",
      "In split_to_train_test: dataset_X.shape=(2614, 11, 65), dataset_y.shape=(2614, 65)\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 48073530289422336.0000Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 48073530289422336.0000 - val_loss: 48073534584389632.0000\n",
      "Epoch 3: early stopping\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "In calc_results: 1568, 523, 523, sum = 2614\n",
      "In split_to_train_test: dataset_X.shape=(124337, 11, 65), dataset_y.shape=(124337, 65)\n",
      "Epoch 1/30\n",
      "1166/1166 [==============================] - 38s 33ms/step - loss: 780.9401 - val_loss: 779.3756\n",
      "Epoch 2/30\n",
      "1166/1166 [==============================] - 38s 32ms/step - loss: 779.9302 - val_loss: 778.3586\n",
      "Epoch 3/30\n",
      "1166/1166 [==============================] - 39s 33ms/step - loss: 778.9263 - val_loss: 777.3423\n",
      "Epoch 4/30\n",
      "1166/1166 [==============================] - 39s 33ms/step - loss: 777.9223 - val_loss: 776.3273\n",
      "Epoch 5/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 776.9190 - val_loss: 775.3121\n",
      "Epoch 6/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 775.9160 - val_loss: 774.2982\n",
      "Epoch 7/30\n",
      "1166/1166 [==============================] - 38s 32ms/step - loss: 774.9142 - val_loss: 773.2855\n",
      "Epoch 8/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 773.9133 - val_loss: 772.2758\n",
      "Epoch 9/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 772.9141 - val_loss: 771.2686\n",
      "Epoch 10/30\n",
      "1166/1166 [==============================] - 38s 33ms/step - loss: 771.9204 - val_loss: 770.2755\n",
      "Epoch 11/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 770.9347 - val_loss: 769.2912\n",
      "Epoch 12/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 769.9530 - val_loss: 768.3104\n",
      "Epoch 13/30\n",
      "1166/1166 [==============================] - 38s 32ms/step - loss: 768.9740 - val_loss: 767.3333\n",
      "Epoch 14/30\n",
      "1166/1166 [==============================] - 38s 32ms/step - loss: 767.9951 - val_loss: 766.3590\n",
      "Epoch 15/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 767.0189 - val_loss: 765.3889\n",
      "Epoch 16/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 766.0443 - val_loss: 764.4246\n",
      "Epoch 17/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 765.0743 - val_loss: 763.4705\n",
      "Epoch 18/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 764.1134 - val_loss: 762.5216\n",
      "Epoch 19/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 763.1531 - val_loss: 761.5770\n",
      "Epoch 20/30\n",
      "1166/1166 [==============================] - 39s 33ms/step - loss: 762.1964 - val_loss: 760.6371\n",
      "Epoch 21/30\n",
      "1166/1166 [==============================] - 37s 31ms/step - loss: 761.2424 - val_loss: 759.7010\n",
      "Epoch 22/30\n",
      "1166/1166 [==============================] - 40s 34ms/step - loss: 760.2924 - val_loss: 758.7676\n",
      "Epoch 23/30\n",
      "1166/1166 [==============================] - 37s 31ms/step - loss: 759.3458 - val_loss: 757.8370\n",
      "Epoch 24/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 758.4022 - val_loss: 756.9094\n",
      "Epoch 25/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 757.4601 - val_loss: 755.9841\n",
      "Epoch 26/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 756.5213 - val_loss: 755.0599\n",
      "Epoch 27/30\n",
      "1166/1166 [==============================] - 37s 31ms/step - loss: 755.5851 - val_loss: 754.1387\n",
      "Epoch 28/30\n",
      "1166/1166 [==============================] - 36s 31ms/step - loss: 754.6508 - val_loss: 753.2187\n",
      "Epoch 29/30\n",
      "1166/1166 [==============================] - 37s 32ms/step - loss: 753.7188 - val_loss: 752.3043\n",
      "Epoch 30/30\n",
      "1166/1166 [==============================] - 35s 30ms/step - loss: 752.7933 - val_loss: 751.3975\n",
      "778/778 [==============================] - 9s 11ms/step\n",
      "In calc_results: 74602, 24868, 24867, sum = 124337\n",
      "In split_to_train_test: dataset_X.shape=(7283, 11, 65), dataset_y.shape=(7283, 65)\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 159.7697 - val_loss: 161.0819\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.6576 - val_loss: 160.9766\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.5510 - val_loss: 160.8787\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.4507 - val_loss: 160.7852\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.3520 - val_loss: 160.6924\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.2546 - val_loss: 160.6018\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 159.1560 - val_loss: 160.4925\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 159.0350 - val_loss: 160.3820\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.9248 - val_loss: 160.2823\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.8203 - val_loss: 160.1850\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.7172 - val_loss: 160.0892\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.6149 - val_loss: 159.9940\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 158.5134 - val_loss: 159.8995\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.4125 - val_loss: 159.8064\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 158.3128 - val_loss: 159.7148\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.2136 - val_loss: 159.6234\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 158.1147 - val_loss: 159.5327\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 158.0159 - val_loss: 159.4423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 30ms/step - loss: 157.9173 - val_loss: 159.3523\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.8190 - val_loss: 159.2623\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 157.7206 - val_loss: 159.1725\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 157.6228 - val_loss: 159.0843\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.5264 - val_loss: 158.9973\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.4306 - val_loss: 158.9107\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.3358 - val_loss: 158.8251\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.2416 - val_loss: 158.7390\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.1475 - val_loss: 158.6535\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 157.0533 - val_loss: 158.5678\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 156.9593 - val_loss: 158.4824\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 156.8653 - val_loss: 158.3969\n",
      "46/46 [==============================] - 1s 10ms/step\n",
      "In calc_results: 4370, 1456, 1457, sum = 7283\n",
      "In split_to_train_test: dataset_X.shape=(11823, 11, 65), dataset_y.shape=(11823, 65)\n",
      "Epoch 1/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 1050.5986 - val_loss: 1071.0974\n",
      "Epoch 2/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1050.4126 - val_loss: 1070.9126\n",
      "Epoch 3/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1050.2279 - val_loss: 1070.7281\n",
      "Epoch 4/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1050.0436 - val_loss: 1070.5436\n",
      "Epoch 5/30\n",
      "111/111 [==============================] - 4s 33ms/step - loss: 1049.8590 - val_loss: 1070.3593\n",
      "Epoch 6/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1049.6750 - val_loss: 1070.1746\n",
      "Epoch 7/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1049.4906 - val_loss: 1069.9902\n",
      "Epoch 8/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1049.3060 - val_loss: 1069.8058\n",
      "Epoch 9/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 1049.1217 - val_loss: 1069.6215\n",
      "Epoch 10/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 1048.9377 - val_loss: 1069.4369\n",
      "Epoch 11/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 1048.7528 - val_loss: 1069.2527\n",
      "Epoch 12/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1048.5685 - val_loss: 1069.0681\n",
      "Epoch 13/30\n",
      "111/111 [==============================] - 4s 32ms/step - loss: 1048.3844 - val_loss: 1068.8838\n",
      "Epoch 14/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1048.2000 - val_loss: 1068.6996\n",
      "Epoch 15/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1048.0154 - val_loss: 1068.5149\n",
      "Epoch 16/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1047.8314 - val_loss: 1068.3307\n",
      "Epoch 17/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1047.6467 - val_loss: 1068.1462\n",
      "Epoch 18/30\n",
      "111/111 [==============================] - 4s 33ms/step - loss: 1047.4628 - val_loss: 1067.9622\n",
      "Epoch 19/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 1047.2783 - val_loss: 1067.7775\n",
      "Epoch 20/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1047.0941 - val_loss: 1067.5930\n",
      "Epoch 21/30\n",
      "111/111 [==============================] - 4s 33ms/step - loss: 1046.9099 - val_loss: 1067.4088\n",
      "Epoch 22/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1046.7251 - val_loss: 1067.2246\n",
      "Epoch 23/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1046.5406 - val_loss: 1067.0404\n",
      "Epoch 24/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1046.3567 - val_loss: 1066.8558\n",
      "Epoch 25/30\n",
      "111/111 [==============================] - 4s 35ms/step - loss: 1046.1729 - val_loss: 1066.6716\n",
      "Epoch 26/30\n",
      "111/111 [==============================] - 4s 34ms/step - loss: 1045.9880 - val_loss: 1066.4873\n",
      "Epoch 27/30\n",
      "111/111 [==============================] - 3s 30ms/step - loss: 1045.8041 - val_loss: 1066.3031\n",
      "Epoch 28/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1045.6198 - val_loss: 1066.1189\n",
      "Epoch 29/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1045.4355 - val_loss: 1065.9344\n",
      "Epoch 30/30\n",
      "111/111 [==============================] - 3s 31ms/step - loss: 1045.2512 - val_loss: 1065.7501\n",
      "74/74 [==============================] - 1s 10ms/step\n",
      "In calc_results: 7094, 2364, 2365, sum = 11823\n",
      "In split_to_train_test: dataset_X.shape=(7450, 11, 65), dataset_y.shape=(7450, 65)\n",
      "Epoch 1/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1143.6677 - val_loss: 1150.2726\n",
      "Epoch 2/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.6130 - val_loss: 1150.2179\n",
      "Epoch 3/30\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 1143.5583 - val_loss: 1150.1633\n",
      "Epoch 4/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1143.5035 - val_loss: 1150.1086\n",
      "Epoch 5/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1143.4493 - val_loss: 1150.0541\n",
      "Epoch 6/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.3948 - val_loss: 1149.9994\n",
      "Epoch 7/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.3402 - val_loss: 1149.9447\n",
      "Epoch 8/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.2858 - val_loss: 1149.8900\n",
      "Epoch 9/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.2316 - val_loss: 1149.8353\n",
      "Epoch 10/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.1768 - val_loss: 1149.7808\n",
      "Epoch 11/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1143.1222 - val_loss: 1149.7262\n",
      "Epoch 12/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1143.0675 - val_loss: 1149.6715\n",
      "Epoch 13/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1143.0134 - val_loss: 1149.6168\n",
      "Epoch 14/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1142.9585 - val_loss: 1149.5621\n",
      "Epoch 15/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.9039 - val_loss: 1149.5074\n",
      "Epoch 16/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.8495 - val_loss: 1149.4529\n",
      "Epoch 17/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.7949 - val_loss: 1149.3983\n",
      "Epoch 18/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1142.7400 - val_loss: 1149.3436\n",
      "Epoch 19/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.6857 - val_loss: 1149.2889\n",
      "Epoch 20/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1142.6313 - val_loss: 1149.2343\n",
      "Epoch 21/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.5768 - val_loss: 1149.1797\n",
      "Epoch 22/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1142.5222 - val_loss: 1149.1250\n",
      "Epoch 23/30\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1142.4674 - val_loss: 1149.0704\n",
      "Epoch 24/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.4133 - val_loss: 1149.0157\n",
      "Epoch 25/30\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1142.3584 - val_loss: 1148.9611\n",
      "Epoch 26/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.3042 - val_loss: 1148.9064\n",
      "Epoch 27/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.2494 - val_loss: 1148.8518\n",
      "Epoch 28/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.1949 - val_loss: 1148.7971\n",
      "Epoch 29/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.1404 - val_loss: 1148.7426\n",
      "Epoch 30/30\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1142.0859 - val_loss: 1148.6879\n",
      "47/47 [==============================] - 1s 10ms/step\n",
      "In calc_results: 4470, 1490, 1490, sum = 7450\n",
      "In split_to_train_test: dataset_X.shape=(26025, 11, 65), dataset_y.shape=(26025, 65)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 396.2044 - val_loss: 390.1817\n",
      "Epoch 2/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 395.9874 - val_loss: 389.9691\n",
      "Epoch 3/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 395.7726 - val_loss: 389.7587\n",
      "Epoch 4/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 395.5429 - val_loss: 389.5046\n",
      "Epoch 5/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 395.2752 - val_loss: 389.2576\n",
      "Epoch 6/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 395.0255 - val_loss: 389.0194\n",
      "Epoch 7/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 394.7814 - val_loss: 388.7845\n",
      "Epoch 8/30\n",
      "244/244 [==============================] - 8s 34ms/step - loss: 394.5398 - val_loss: 388.5512\n",
      "Epoch 9/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 394.2997 - val_loss: 388.3189\n",
      "Epoch 10/30\n",
      "244/244 [==============================] - 8s 32ms/step - loss: 394.0603 - val_loss: 388.0873\n",
      "Epoch 11/30\n",
      "244/244 [==============================] - 8s 34ms/step - loss: 393.8216 - val_loss: 387.8563\n",
      "Epoch 12/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 393.5835 - val_loss: 387.6255\n",
      "Epoch 13/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 393.3455 - val_loss: 387.3948\n",
      "Epoch 14/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 393.1078 - val_loss: 387.1645\n",
      "Epoch 15/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 392.8700 - val_loss: 386.9344\n",
      "Epoch 16/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 392.6328 - val_loss: 386.7046\n",
      "Epoch 17/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 392.3966 - val_loss: 386.4756\n",
      "Epoch 18/30\n",
      "244/244 [==============================] - 8s 33ms/step - loss: 392.1605 - val_loss: 386.2465\n",
      "Epoch 19/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 391.9247 - val_loss: 386.0174\n",
      "Epoch 20/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 391.6885 - val_loss: 385.7885\n",
      "Epoch 21/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 391.4525 - val_loss: 385.5600\n",
      "Epoch 22/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 391.2167 - val_loss: 385.3314\n",
      "Epoch 23/30\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 390.9809 - val_loss: 385.1027\n",
      "Epoch 24/30\n",
      "244/244 [==============================] - 8s 32ms/step - loss: 390.7450 - val_loss: 384.8741\n",
      "Epoch 25/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 390.5091 - val_loss: 384.6454\n",
      "Epoch 26/30\n",
      "244/244 [==============================] - 8s 31ms/step - loss: 390.2734 - val_loss: 384.4166\n",
      "Epoch 27/30\n",
      "244/244 [==============================] - 8s 34ms/step - loss: 390.0375 - val_loss: 384.1881\n",
      "Epoch 28/30\n",
      "244/244 [==============================] - 8s 34ms/step - loss: 389.8016 - val_loss: 383.9595\n",
      "Epoch 29/30\n",
      "244/244 [==============================] - 7s 31ms/step - loss: 389.5657 - val_loss: 383.7309\n",
      "Epoch 30/30\n",
      "244/244 [==============================] - 8s 32ms/step - loss: 389.3300 - val_loss: 383.5024\n",
      "163/163 [==============================] - 2s 11ms/step\n",
      "In calc_results: 15615, 5205, 5205, sum = 26025\n",
      "N_clusters=11\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(14482, 11, 65), dataset_y.shape=(14482, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515418736427008.0000 - val_loss: 12547231559188480.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515423031394304.0000 - val_loss: 12547231559188480.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515614157438976.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 12515419810168832.0000 - val_loss: 12547231559188480.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8689, 2897, 2896, sum = 14482\n",
      "In split_to_train_test: dataset_X.shape=(22472, 11, 65), dataset_y.shape=(22472, 65)\n",
      "Epoch 1/30\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 652.6160 - val_loss: 641.0762\n",
      "Epoch 2/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 652.3768 - val_loss: 640.8588\n",
      "Epoch 3/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 652.1385 - val_loss: 640.6426\n",
      "Epoch 4/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 651.9003 - val_loss: 640.4269\n",
      "Epoch 5/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 651.6623 - val_loss: 640.2115\n",
      "Epoch 6/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 651.4240 - val_loss: 639.9960\n",
      "Epoch 7/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 651.1858 - val_loss: 639.7808\n",
      "Epoch 8/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 650.9477 - val_loss: 639.5654\n",
      "Epoch 9/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 650.7098 - val_loss: 639.3501\n",
      "Epoch 10/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 650.4715 - val_loss: 639.1346\n",
      "Epoch 11/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 650.2286 - val_loss: 638.8703\n",
      "Epoch 12/30\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 649.9325 - val_loss: 638.6469\n",
      "Epoch 13/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 649.6861 - val_loss: 638.4251\n",
      "Epoch 14/30\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 649.4415 - val_loss: 638.2046\n",
      "Epoch 15/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 649.1973 - val_loss: 637.9843\n",
      "Epoch 16/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 648.9538 - val_loss: 637.7642\n",
      "Epoch 17/30\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 648.7109 - val_loss: 637.5449\n",
      "Epoch 18/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 648.4680 - val_loss: 637.3254\n",
      "Epoch 19/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 648.2254 - val_loss: 637.1059\n",
      "Epoch 20/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 647.9828 - val_loss: 636.8864\n",
      "Epoch 21/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 647.7408 - val_loss: 636.6674\n",
      "Epoch 22/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 647.4987 - val_loss: 636.4482\n",
      "Epoch 23/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 647.2562 - val_loss: 636.2290\n",
      "Epoch 24/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 647.0147 - val_loss: 636.0103\n",
      "Epoch 25/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 646.7728 - val_loss: 635.7913\n",
      "Epoch 26/30\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 646.5309 - val_loss: 635.5724\n",
      "Epoch 27/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 646.2893 - val_loss: 635.3539\n",
      "Epoch 28/30\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 646.0479 - val_loss: 635.1351\n",
      "Epoch 29/30\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 645.8066 - val_loss: 634.9169\n",
      "Epoch 30/30\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 645.5649 - val_loss: 634.6983\n",
      "141/141 [==============================] - 1s 10ms/step\n",
      "In calc_results: 13483, 4495, 4494, sum = 22472\n",
      "In split_to_train_test: dataset_X.shape=(68667, 11, 65), dataset_y.shape=(68667, 65)\n",
      "Epoch 1/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 727.4998 - val_loss: 717.6927\n",
      "Epoch 2/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 726.4093 - val_loss: 716.5946\n",
      "Epoch 3/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 725.3180 - val_loss: 715.4957\n",
      "Epoch 4/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 724.2269 - val_loss: 714.3727\n",
      "Epoch 5/30\n",
      "644/644 [==============================] - 20s 30ms/step - loss: 723.0555 - val_loss: 713.1746\n",
      "Epoch 6/30\n",
      "644/644 [==============================] - 20s 30ms/step - loss: 721.8955 - val_loss: 712.0176\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 21s 32ms/step - loss: 720.7521 - val_loss: 710.8698\n",
      "Epoch 8/30\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 719.6152 - val_loss: 709.7273\n",
      "Epoch 9/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 718.4821 - val_loss: 708.5903\n",
      "Epoch 10/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 717.3551 - val_loss: 707.4630\n",
      "Epoch 11/30\n",
      "644/644 [==============================] - 21s 33ms/step - loss: 716.2404 - val_loss: 706.3461\n",
      "Epoch 12/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 715.1312 - val_loss: 705.2338\n",
      "Epoch 13/30\n",
      "644/644 [==============================] - 22s 34ms/step - loss: 714.0264 - val_loss: 704.1283\n",
      "Epoch 14/30\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 712.9283 - val_loss: 703.0328\n",
      "Epoch 15/30\n",
      "644/644 [==============================] - 21s 33ms/step - loss: 711.8372 - val_loss: 701.9407\n",
      "Epoch 16/30\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 710.7480 - val_loss: 700.8519\n",
      "Epoch 17/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 709.6621 - val_loss: 699.7671\n",
      "Epoch 18/30\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 708.5805 - val_loss: 698.6871\n",
      "Epoch 19/30\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 707.5034 - val_loss: 697.6116\n",
      "Epoch 20/30\n",
      "644/644 [==============================] - 21s 33ms/step - loss: 706.4297 - val_loss: 696.5390\n",
      "Epoch 21/30\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 705.3600 - val_loss: 695.4698\n",
      "Epoch 22/30\n",
      "644/644 [==============================] - 22s 34ms/step - loss: 704.2928 - val_loss: 694.4034\n",
      "Epoch 23/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 703.2281 - val_loss: 693.3393\n",
      "Epoch 24/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 702.1666 - val_loss: 692.2774\n",
      "Epoch 25/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 701.1073 - val_loss: 691.2176\n",
      "Epoch 26/30\n",
      "644/644 [==============================] - 20s 30ms/step - loss: 700.0500 - val_loss: 690.1616\n",
      "Epoch 27/30\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 698.9962 - val_loss: 689.1111\n",
      "Epoch 28/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 697.9454 - val_loss: 688.0657\n",
      "Epoch 29/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 696.9014 - val_loss: 687.0297\n",
      "Epoch 30/30\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 695.8644 - val_loss: 685.9977\n",
      "430/430 [==============================] - 5s 10ms/step\n",
      "In calc_results: 41200, 13734, 13733, sum = 68667\n",
      "In split_to_train_test: dataset_X.shape=(3943, 11, 65), dataset_y.shape=(3943, 65)\n",
      "Epoch 1/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1101.0746 - val_loss: 1097.8954\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1101.0262 - val_loss: 1097.8470\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.9779 - val_loss: 1097.7988\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.9296 - val_loss: 1097.7506\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.8813 - val_loss: 1097.7023\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.8333 - val_loss: 1097.6541\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.7849 - val_loss: 1097.6057\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1100.7366 - val_loss: 1097.5575\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.6843 - val_loss: 1097.5039\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1100.6340 - val_loss: 1097.4539\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1100.5840 - val_loss: 1097.4041\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1100.5343 - val_loss: 1097.3542\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1100.4845 - val_loss: 1097.3044\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1100.4347 - val_loss: 1097.2548\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1100.3851 - val_loss: 1097.2052\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1100.3357 - val_loss: 1097.1556\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1100.2863 - val_loss: 1097.1061\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1100.2366 - val_loss: 1097.0565\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1100.1871 - val_loss: 1097.0073\n",
      "Epoch 20/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1100.1377 - val_loss: 1096.9578\n",
      "Epoch 21/30\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 1100.0885 - val_loss: 1096.9083\n",
      "Epoch 22/30\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1100.0391 - val_loss: 1096.8590\n",
      "Epoch 23/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1099.9897 - val_loss: 1096.8094\n",
      "Epoch 24/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1099.9403 - val_loss: 1096.7603\n",
      "Epoch 25/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1099.8911 - val_loss: 1096.7109\n",
      "Epoch 26/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1099.8418 - val_loss: 1096.6616\n",
      "Epoch 27/30\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 1099.7924 - val_loss: 1096.6122\n",
      "Epoch 28/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1099.7433 - val_loss: 1096.5629\n",
      "Epoch 29/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1099.6937 - val_loss: 1096.5135\n",
      "Epoch 30/30\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1099.6447 - val_loss: 1096.4645\n",
      "25/25 [==============================] - 0s 10ms/step\n",
      "In calc_results: 2366, 788, 789, sum = 3943\n",
      "In split_to_train_test: dataset_X.shape=(7473, 11, 65), dataset_y.shape=(7473, 65)\n",
      "Epoch 1/30\n",
      "71/71 [==============================] - 2s 32ms/step - loss: 1187.0076 - val_loss: 1212.4672\n",
      "Epoch 2/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.8896 - val_loss: 1212.3492\n",
      "Epoch 3/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.7716 - val_loss: 1212.2310\n",
      "Epoch 4/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.6534 - val_loss: 1212.1133\n",
      "Epoch 5/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.5354 - val_loss: 1211.9951\n",
      "Epoch 6/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.4175 - val_loss: 1211.8773\n",
      "Epoch 7/30\n",
      "71/71 [==============================] - 2s 32ms/step - loss: 1186.2997 - val_loss: 1211.7594\n",
      "Epoch 8/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1186.1824 - val_loss: 1211.6414\n",
      "Epoch 9/30\n",
      "71/71 [==============================] - 2s 32ms/step - loss: 1186.0647 - val_loss: 1211.5238\n",
      "Epoch 10/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.9468 - val_loss: 1211.4059\n",
      "Epoch 11/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.8295 - val_loss: 1211.2878\n",
      "Epoch 12/30\n",
      "71/71 [==============================] - 2s 32ms/step - loss: 1185.7118 - val_loss: 1211.1703\n",
      "Epoch 13/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.5944 - val_loss: 1211.0522\n",
      "Epoch 14/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.4764 - val_loss: 1210.9342\n",
      "Epoch 15/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.3588 - val_loss: 1210.8167\n",
      "Epoch 16/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.2412 - val_loss: 1210.6985\n",
      "Epoch 17/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.1233 - val_loss: 1210.5807\n",
      "Epoch 18/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1185.0059 - val_loss: 1210.4629\n",
      "Epoch 19/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.8879 - val_loss: 1210.3448\n",
      "Epoch 20/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.7705 - val_loss: 1210.2272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.6527 - val_loss: 1210.1090\n",
      "Epoch 22/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.5353 - val_loss: 1209.9911\n",
      "Epoch 23/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.4171 - val_loss: 1209.8734\n",
      "Epoch 24/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.2997 - val_loss: 1209.7551\n",
      "Epoch 25/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.1819 - val_loss: 1209.6375\n",
      "Epoch 26/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1184.0645 - val_loss: 1209.5195\n",
      "Epoch 27/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1183.9465 - val_loss: 1209.4015\n",
      "Epoch 28/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1183.8291 - val_loss: 1209.2839\n",
      "Epoch 29/30\n",
      "71/71 [==============================] - 2s 31ms/step - loss: 1183.7109 - val_loss: 1209.1656\n",
      "Epoch 30/30\n",
      "71/71 [==============================] - 2s 32ms/step - loss: 1183.5936 - val_loss: 1209.0480\n",
      "47/47 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4484, 1494, 1495, sum = 7473\n",
      "In split_to_train_test: dataset_X.shape=(6222, 11, 65), dataset_y.shape=(6222, 65)\n",
      "Epoch 1/30\n",
      "59/59 [==============================] - 2s 35ms/step - loss: 217.5846 - val_loss: 219.4237\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.4893 - val_loss: 219.3445\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.4054 - val_loss: 219.2578\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.3263 - val_loss: 219.1795\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 217.2492 - val_loss: 219.1016\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.1726 - val_loss: 219.0245\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.0924 - val_loss: 218.9289\n",
      "Epoch 8/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 217.0005 - val_loss: 218.8386\n",
      "Epoch 9/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.9132 - val_loss: 218.7515\n",
      "Epoch 10/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.8283 - val_loss: 218.6654\n",
      "Epoch 11/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.7391 - val_loss: 218.5733\n",
      "Epoch 12/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.6536 - val_loss: 218.4874\n",
      "Epoch 13/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 216.5696 - val_loss: 218.4019\n",
      "Epoch 14/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 216.4862 - val_loss: 218.3169\n",
      "Epoch 15/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 216.4030 - val_loss: 218.2322\n",
      "Epoch 16/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.3201 - val_loss: 218.1480\n",
      "Epoch 17/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 216.2375 - val_loss: 218.0637\n",
      "Epoch 18/30\n",
      "59/59 [==============================] - 2s 35ms/step - loss: 216.1551 - val_loss: 217.9795\n",
      "Epoch 19/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 216.0729 - val_loss: 217.8957\n",
      "Epoch 20/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 215.9908 - val_loss: 217.8120\n",
      "Epoch 21/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 215.9089 - val_loss: 217.7284\n",
      "Epoch 22/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 215.8271 - val_loss: 217.6448\n",
      "Epoch 23/30\n",
      "59/59 [==============================] - 2s 32ms/step - loss: 215.7453 - val_loss: 217.5614\n",
      "Epoch 24/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 215.6637 - val_loss: 217.4779\n",
      "Epoch 25/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 215.5821 - val_loss: 217.3946\n",
      "Epoch 26/30\n",
      "59/59 [==============================] - 2s 31ms/step - loss: 215.5006 - val_loss: 217.3114\n",
      "Epoch 27/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 215.4192 - val_loss: 217.2282\n",
      "Epoch 28/30\n",
      "59/59 [==============================] - 2s 33ms/step - loss: 215.3378 - val_loss: 217.1449\n",
      "Epoch 29/30\n",
      "59/59 [==============================] - 2s 34ms/step - loss: 215.2564 - val_loss: 217.0618\n",
      "Epoch 30/30\n",
      "59/59 [==============================] - 2s 34ms/step - loss: 215.1751 - val_loss: 216.9786\n",
      "39/39 [==============================] - 0s 11ms/step\n",
      "In calc_results: 3733, 1245, 1244, sum = 6222\n",
      "In split_to_train_test: dataset_X.shape=(18927, 11, 65), dataset_y.shape=(18927, 65)\n",
      "Epoch 1/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1046.0431 - val_loss: 1066.2302\n",
      "Epoch 2/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1045.8425 - val_loss: 1066.0288\n",
      "Epoch 3/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1045.6411 - val_loss: 1065.8278\n",
      "Epoch 4/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1045.4399 - val_loss: 1065.6262\n",
      "Epoch 5/30\n",
      "178/178 [==============================] - 6s 34ms/step - loss: 1045.2386 - val_loss: 1065.4249\n",
      "Epoch 6/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1045.0377 - val_loss: 1065.2236\n",
      "Epoch 7/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1044.8364 - val_loss: 1065.0220\n",
      "Epoch 8/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1044.6349 - val_loss: 1064.8206\n",
      "Epoch 9/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1044.4341 - val_loss: 1064.6191\n",
      "Epoch 10/30\n",
      "178/178 [==============================] - 6s 34ms/step - loss: 1044.2330 - val_loss: 1064.4178\n",
      "Epoch 11/30\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 1044.0315 - val_loss: 1064.2164\n",
      "Epoch 12/30\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 1043.8304 - val_loss: 1064.0148\n",
      "Epoch 13/30\n",
      "178/178 [==============================] - 5s 30ms/step - loss: 1043.6293 - val_loss: 1063.8134\n",
      "Epoch 14/30\n",
      "178/178 [==============================] - 5s 30ms/step - loss: 1043.4281 - val_loss: 1063.6119\n",
      "Epoch 15/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1043.2268 - val_loss: 1063.4103\n",
      "Epoch 16/30\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 1043.0258 - val_loss: 1063.2092\n",
      "Epoch 17/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1042.8242 - val_loss: 1063.0076\n",
      "Epoch 18/30\n",
      "178/178 [==============================] - 5s 30ms/step - loss: 1042.6237 - val_loss: 1062.8064\n",
      "Epoch 19/30\n",
      "178/178 [==============================] - 6s 31ms/step - loss: 1042.4221 - val_loss: 1062.6050\n",
      "Epoch 20/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1042.2209 - val_loss: 1062.4031\n",
      "Epoch 21/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1042.0204 - val_loss: 1062.2020\n",
      "Epoch 22/30\n",
      "178/178 [==============================] - 6s 31ms/step - loss: 1041.8192 - val_loss: 1062.0005\n",
      "Epoch 23/30\n",
      "178/178 [==============================] - 6s 32ms/step - loss: 1041.6178 - val_loss: 1061.7988\n",
      "Epoch 24/30\n",
      "178/178 [==============================] - 6s 31ms/step - loss: 1041.4170 - val_loss: 1061.5977\n",
      "Epoch 25/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1041.2158 - val_loss: 1061.3961\n",
      "Epoch 26/30\n",
      "178/178 [==============================] - 5s 30ms/step - loss: 1041.0146 - val_loss: 1061.1947\n",
      "Epoch 27/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1040.8137 - val_loss: 1060.9933\n",
      "Epoch 28/30\n",
      "178/178 [==============================] - 5s 31ms/step - loss: 1040.6122 - val_loss: 1060.7917\n",
      "Epoch 29/30\n",
      "178/178 [==============================] - 6s 31ms/step - loss: 1040.4114 - val_loss: 1060.5902\n",
      "Epoch 30/30\n",
      "178/178 [==============================] - 5s 30ms/step - loss: 1040.2101 - val_loss: 1060.3889\n",
      "119/119 [==============================] - 1s 11ms/step\n",
      "In calc_results: 11356, 3786, 3785, sum = 18927\n",
      "In split_to_train_test: dataset_X.shape=(92897, 11, 65), dataset_y.shape=(92897, 65)\n",
      "Epoch 1/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 894.0886 - val_loss: 894.3799\n",
      "Epoch 2/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 892.9487 - val_loss: 893.1857\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 26s 30ms/step - loss: 891.7603 - val_loss: 891.9998\n",
      "Epoch 4/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 890.5989 - val_loss: 890.8228\n",
      "Epoch 5/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 889.4418 - val_loss: 889.6491\n",
      "Epoch 6/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 888.2886 - val_loss: 888.4775\n",
      "Epoch 7/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 887.1363 - val_loss: 887.3081\n",
      "Epoch 8/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 885.9867 - val_loss: 886.1409\n",
      "Epoch 9/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 884.8380 - val_loss: 884.9783\n",
      "Epoch 10/30\n",
      "871/871 [==============================] - 26s 30ms/step - loss: 883.6972 - val_loss: 883.8334\n",
      "Epoch 11/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 882.5698 - val_loss: 882.6968\n",
      "Epoch 12/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 881.4442 - val_loss: 881.5653\n",
      "Epoch 13/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 880.3230 - val_loss: 880.4410\n",
      "Epoch 14/30\n",
      "871/871 [==============================] - 27s 32ms/step - loss: 879.2092 - val_loss: 879.3269\n",
      "Epoch 15/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 878.1037 - val_loss: 878.2185\n",
      "Epoch 16/30\n",
      "871/871 [==============================] - 26s 30ms/step - loss: 877.0010 - val_loss: 877.1151\n",
      "Epoch 17/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 875.9003 - val_loss: 876.0164\n",
      "Epoch 18/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 874.8024 - val_loss: 874.9233\n",
      "Epoch 19/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 873.7081 - val_loss: 873.8356\n",
      "Epoch 20/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 872.6177 - val_loss: 872.7529\n",
      "Epoch 21/30\n",
      "871/871 [==============================] - 28s 33ms/step - loss: 871.5292 - val_loss: 871.6744\n",
      "Epoch 22/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 870.4449 - val_loss: 870.6008\n",
      "Epoch 23/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 869.3643 - val_loss: 869.5312\n",
      "Epoch 24/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 868.2861 - val_loss: 868.4657\n",
      "Epoch 25/30\n",
      "871/871 [==============================] - 27s 30ms/step - loss: 867.2099 - val_loss: 867.4036\n",
      "Epoch 26/30\n",
      "871/871 [==============================] - 26s 30ms/step - loss: 866.1395 - val_loss: 866.3448\n",
      "Epoch 27/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 865.0726 - val_loss: 865.2921\n",
      "Epoch 28/30\n",
      "871/871 [==============================] - 27s 31ms/step - loss: 864.0168 - val_loss: 864.2512\n",
      "Epoch 29/30\n",
      "871/871 [==============================] - 29s 33ms/step - loss: 862.9677 - val_loss: 863.2128\n",
      "Epoch 30/30\n",
      "871/871 [==============================] - 28s 32ms/step - loss: 861.9207 - val_loss: 862.1762\n",
      "581/581 [==============================] - 6s 10ms/step\n",
      "In calc_results: 55738, 18580, 18579, sum = 92897\n",
      "In split_to_train_test: dataset_X.shape=(9642, 11, 65), dataset_y.shape=(9642, 65)\n",
      "Epoch 1/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 155.2602 - val_loss: 188.3167\n",
      "Epoch 2/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 155.2001 - val_loss: 188.2621\n",
      "Epoch 3/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 155.1484 - val_loss: 188.2106\n",
      "Epoch 4/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 155.0993 - val_loss: 188.1599\n",
      "Epoch 5/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 155.0509 - val_loss: 188.1089\n",
      "Epoch 6/30\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 155.0031 - val_loss: 188.0577\n",
      "Epoch 7/30\n",
      "91/91 [==============================] - 3s 34ms/step - loss: 154.9557 - val_loss: 188.0065\n",
      "Epoch 8/30\n",
      "91/91 [==============================] - 3s 33ms/step - loss: 154.9085 - val_loss: 187.9556\n",
      "Epoch 9/30\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 154.8618 - val_loss: 187.9051\n",
      "Epoch 10/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.8151 - val_loss: 187.8541\n",
      "Epoch 11/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.7684 - val_loss: 187.8040\n",
      "Epoch 12/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.7163 - val_loss: 187.7430\n",
      "Epoch 13/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.6648 - val_loss: 187.6857\n",
      "Epoch 14/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.6143 - val_loss: 187.6299\n",
      "Epoch 15/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.5647 - val_loss: 187.5743\n",
      "Epoch 16/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.5153 - val_loss: 187.5204\n",
      "Epoch 17/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.4665 - val_loss: 187.4669\n",
      "Epoch 18/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.4179 - val_loss: 187.4143\n",
      "Epoch 19/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.3694 - val_loss: 187.3610\n",
      "Epoch 20/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.3211 - val_loss: 187.3086\n",
      "Epoch 21/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.2730 - val_loss: 187.2556\n",
      "Epoch 22/30\n",
      "91/91 [==============================] - 3s 33ms/step - loss: 154.2250 - val_loss: 187.2032\n",
      "Epoch 23/30\n",
      "91/91 [==============================] - 3s 33ms/step - loss: 154.1772 - val_loss: 187.1505\n",
      "Epoch 24/30\n",
      "91/91 [==============================] - 3s 33ms/step - loss: 154.1294 - val_loss: 187.0979\n",
      "Epoch 25/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.0817 - val_loss: 187.0452\n",
      "Epoch 26/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 154.0341 - val_loss: 186.9929\n",
      "Epoch 27/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 153.9865 - val_loss: 186.9406\n",
      "Epoch 28/30\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 153.9390 - val_loss: 186.8883\n",
      "Epoch 29/30\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 153.8879 - val_loss: 186.8363\n",
      "Epoch 30/30\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 153.8162 - val_loss: 186.7864\n",
      "61/61 [==============================] - 1s 11ms/step\n",
      "In calc_results: 5785, 1929, 1928, sum = 9642\n",
      "In split_to_train_test: dataset_X.shape=(12141, 11, 65), dataset_y.shape=(12141, 65)\n",
      "Epoch 1/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1013.5017 - val_loss: 1038.6595\n",
      "Epoch 2/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1013.4161 - val_loss: 1038.5739\n",
      "Epoch 3/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1013.3302 - val_loss: 1038.4884\n",
      "Epoch 4/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1013.2443 - val_loss: 1038.4030\n",
      "Epoch 5/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1013.1586 - val_loss: 1038.3174\n",
      "Epoch 6/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1013.0730 - val_loss: 1038.2317\n",
      "Epoch 7/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1012.9873 - val_loss: 1038.1460\n",
      "Epoch 8/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1012.9015 - val_loss: 1038.0605\n",
      "Epoch 9/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1012.8160 - val_loss: 1037.9750\n",
      "Epoch 10/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1012.7299 - val_loss: 1037.8895\n",
      "Epoch 11/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1012.6443 - val_loss: 1037.8040\n",
      "Epoch 12/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1012.5589 - val_loss: 1037.7184\n",
      "Epoch 13/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1012.4734 - val_loss: 1037.6329\n",
      "Epoch 14/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1012.3874 - val_loss: 1037.5474\n",
      "Epoch 15/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1012.3016 - val_loss: 1037.4618\n",
      "Epoch 16/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1012.2159 - val_loss: 1037.3763\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 3s 31ms/step - loss: 1012.1302 - val_loss: 1037.2908\n",
      "Epoch 18/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1012.0445 - val_loss: 1037.2053\n",
      "Epoch 19/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1011.9590 - val_loss: 1037.1199\n",
      "Epoch 20/30\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 1011.8730 - val_loss: 1037.0345\n",
      "Epoch 21/30\n",
      "114/114 [==============================] - 3s 30ms/step - loss: 1011.7877 - val_loss: 1036.9489\n",
      "Epoch 22/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1011.7017 - val_loss: 1036.8633\n",
      "Epoch 23/30\n",
      "114/114 [==============================] - 3s 31ms/step - loss: 1011.6160 - val_loss: 1036.7778\n",
      "Epoch 24/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1011.5303 - val_loss: 1036.6924\n",
      "Epoch 25/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1011.4446 - val_loss: 1036.6072\n",
      "Epoch 26/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1011.3591 - val_loss: 1036.5216\n",
      "Epoch 27/30\n",
      "114/114 [==============================] - 4s 31ms/step - loss: 1011.2732 - val_loss: 1036.4362\n",
      "Epoch 28/30\n",
      "114/114 [==============================] - 4s 32ms/step - loss: 1011.1876 - val_loss: 1036.3507\n",
      "Epoch 29/30\n",
      "114/114 [==============================] - 4s 33ms/step - loss: 1011.1020 - val_loss: 1036.2654\n",
      "Epoch 30/30\n",
      "114/114 [==============================] - 4s 34ms/step - loss: 1011.0162 - val_loss: 1036.1799\n",
      "76/76 [==============================] - 1s 11ms/step\n",
      "In calc_results: 7285, 2428, 2428, sum = 12141\n",
      "In split_to_train_test: dataset_X.shape=(69600, 11, 65), dataset_y.shape=(69600, 65)\n",
      "Epoch 1/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 772.6854 - val_loss: 770.6588\n",
      "Epoch 2/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 771.9832 - val_loss: 769.9525\n",
      "Epoch 3/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 771.2819 - val_loss: 769.2468\n",
      "Epoch 4/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 770.5807 - val_loss: 768.5406\n",
      "Epoch 5/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 769.8801 - val_loss: 767.8351\n",
      "Epoch 6/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 769.1797 - val_loss: 767.1302\n",
      "Epoch 7/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 768.4813 - val_loss: 766.4252\n",
      "Epoch 8/30\n",
      "653/653 [==============================] - 20s 30ms/step - loss: 767.7828 - val_loss: 765.7207\n",
      "Epoch 9/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 767.0854 - val_loss: 765.0174\n",
      "Epoch 10/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 766.3902 - val_loss: 764.3156\n",
      "Epoch 11/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 765.6974 - val_loss: 763.6165\n",
      "Epoch 12/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 765.0117 - val_loss: 762.9281\n",
      "Epoch 13/30\n",
      "653/653 [==============================] - 21s 31ms/step - loss: 764.3328 - val_loss: 762.2415\n",
      "Epoch 14/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 763.6565 - val_loss: 761.5577\n",
      "Epoch 15/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 762.9844 - val_loss: 760.8812\n",
      "Epoch 16/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 762.3153 - val_loss: 760.2071\n",
      "Epoch 17/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 761.6472 - val_loss: 759.5347\n",
      "Epoch 18/30\n",
      "653/653 [==============================] - 22s 33ms/step - loss: 760.9811 - val_loss: 758.8633\n",
      "Epoch 19/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 760.3154 - val_loss: 758.1944\n",
      "Epoch 20/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 759.6520 - val_loss: 757.5265\n",
      "Epoch 21/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 758.9888 - val_loss: 756.8602\n",
      "Epoch 22/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 758.3275 - val_loss: 756.1961\n",
      "Epoch 23/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 757.6669 - val_loss: 755.5338\n",
      "Epoch 24/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 757.0088 - val_loss: 754.8743\n",
      "Epoch 25/30\n",
      "653/653 [==============================] - 22s 33ms/step - loss: 756.3537 - val_loss: 754.2195\n",
      "Epoch 26/30\n",
      "653/653 [==============================] - 21s 33ms/step - loss: 755.7043 - val_loss: 753.5739\n",
      "Epoch 27/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 755.0565 - val_loss: 752.9291\n",
      "Epoch 28/30\n",
      "653/653 [==============================] - 20s 31ms/step - loss: 754.4100 - val_loss: 752.2869\n",
      "Epoch 29/30\n",
      "653/653 [==============================] - 21s 33ms/step - loss: 753.7646 - val_loss: 751.6450\n",
      "Epoch 30/30\n",
      "653/653 [==============================] - 21s 32ms/step - loss: 753.1201 - val_loss: 751.0034\n",
      "435/435 [==============================] - 5s 10ms/step\n",
      "In calc_results: 41760, 13920, 13920, sum = 69600\n",
      "N_clusters=5\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(10203, 11, 65), dataset_y.shape=(10203, 65)\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 3s 34ms/step - loss: 146.0454 - val_loss: 175.4404\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 3s 34ms/step - loss: 145.9270 - val_loss: 175.3049\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 3s 32ms/step - loss: 145.8133 - val_loss: 175.1697\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 3s 34ms/step - loss: 145.7057 - val_loss: 175.0346\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 3s 34ms/step - loss: 145.6006 - val_loss: 174.9083\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 3s 32ms/step - loss: 145.4899 - val_loss: 174.7859\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 3s 30ms/step - loss: 145.3786 - val_loss: 174.6658\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 145.2768 - val_loss: 174.5476\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 145.1790 - val_loss: 174.4312\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 145.0820 - val_loss: 174.3107\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.9832 - val_loss: 174.1932\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.8871 - val_loss: 174.0752\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.7916 - val_loss: 173.9602\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.6965 - val_loss: 173.8445\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.6020 - val_loss: 173.7285\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.5076 - val_loss: 173.6132\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.4140 - val_loss: 173.4992\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.3214 - val_loss: 173.3874\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.2291 - val_loss: 173.2777\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.1376 - val_loss: 173.1677\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 144.0463 - val_loss: 173.0578\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.9552 - val_loss: 172.9487\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.8642 - val_loss: 172.8408\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.7740 - val_loss: 172.7322\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.6846 - val_loss: 172.6242\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.5955 - val_loss: 172.5163\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.5065 - val_loss: 172.4070\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 3s 31ms/step - loss: 143.4175 - val_loss: 172.2988\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 3s 32ms/step - loss: 143.3289 - val_loss: 172.1908\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 3s 34ms/step - loss: 143.2417 - val_loss: 172.0820\n",
      "64/64 [==============================] - 1s 11ms/step\n",
      "In calc_results: 6122, 2040, 2041, sum = 10203\n",
      "In split_to_train_test: dataset_X.shape=(183887, 11, 65), dataset_y.shape=(183887, 65)\n",
      "Epoch 1/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 647.0798 - val_loss: 645.6560\n",
      "Epoch 2/30\n",
      "1724/1724 [==============================] - 55s 32ms/step - loss: 645.4848 - val_loss: 644.0435\n",
      "Epoch 3/30\n",
      "1724/1724 [==============================] - 58s 33ms/step - loss: 643.9155 - val_loss: 642.4686\n",
      "Epoch 4/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 642.3600 - val_loss: 640.9012\n",
      "Epoch 5/30\n",
      "1724/1724 [==============================] - 54s 31ms/step - loss: 640.8089 - val_loss: 639.3398\n",
      "Epoch 6/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 639.2632 - val_loss: 637.7964\n",
      "Epoch 7/30\n",
      "1724/1724 [==============================] - 55s 32ms/step - loss: 637.7338 - val_loss: 636.2715\n",
      "Epoch 8/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 636.2111 - val_loss: 634.7552\n",
      "Epoch 9/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 634.6912 - val_loss: 633.2510\n",
      "Epoch 10/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 633.1823 - val_loss: 631.7660\n",
      "Epoch 11/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 631.6811 - val_loss: 630.2893\n",
      "Epoch 12/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 630.1843 - val_loss: 628.8202\n",
      "Epoch 13/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 628.6934 - val_loss: 627.3591\n",
      "Epoch 14/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 627.2097 - val_loss: 625.9059\n",
      "Epoch 15/30\n",
      "1724/1724 [==============================] - 56s 33ms/step - loss: 625.7321 - val_loss: 624.4644\n",
      "Epoch 16/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 624.2457 - val_loss: 622.9402\n",
      "Epoch 17/30\n",
      "1724/1724 [==============================] - 58s 34ms/step - loss: 622.6337 - val_loss: 621.3896\n",
      "Epoch 18/30\n",
      "1724/1724 [==============================] - 54s 32ms/step - loss: 621.0606 - val_loss: 619.8652\n",
      "Epoch 19/30\n",
      "1724/1724 [==============================] - 56s 33ms/step - loss: 619.5012 - val_loss: 618.3472\n",
      "Epoch 20/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 617.9506 - val_loss: 616.8347\n",
      "Epoch 21/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 616.4081 - val_loss: 615.3265\n",
      "Epoch 22/30\n",
      "1724/1724 [==============================] - 56s 32ms/step - loss: 614.8740 - val_loss: 613.8251\n",
      "Epoch 23/30\n",
      "1724/1724 [==============================] - 55s 32ms/step - loss: 613.3481 - val_loss: 612.3322\n",
      "Epoch 24/30\n",
      "1724/1724 [==============================] - 59s 34ms/step - loss: 611.8323 - val_loss: 610.8566\n",
      "Epoch 25/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 610.3298 - val_loss: 609.4025\n",
      "Epoch 26/30\n",
      "1724/1724 [==============================] - 57s 33ms/step - loss: 608.8431 - val_loss: 607.9768\n",
      "Epoch 27/30\n",
      "1724/1724 [==============================] - 56s 33ms/step - loss: 607.3727 - val_loss: 606.5870\n",
      "Epoch 28/30\n",
      "1724/1724 [==============================] - 58s 33ms/step - loss: 605.9192 - val_loss: 605.2285\n",
      "Epoch 29/30\n",
      "1724/1724 [==============================] - 59s 34ms/step - loss: 604.4814 - val_loss: 603.9038\n",
      "Epoch 30/30\n",
      "1724/1724 [==============================] - 58s 34ms/step - loss: 603.0638 - val_loss: 602.6041\n",
      "1150/1150 [==============================] - 13s 11ms/step\n",
      "In calc_results: 110332, 36778, 36777, sum = 183887\n",
      "In split_to_train_test: dataset_X.shape=(20954, 11, 65), dataset_y.shape=(20954, 65)\n",
      "Epoch 1/30\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 1031.3289 - val_loss: 1047.1755\n",
      "Epoch 2/30\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 1031.0817 - val_loss: 1046.9310\n",
      "Epoch 3/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1030.8379 - val_loss: 1046.6869\n",
      "Epoch 4/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1030.5941 - val_loss: 1046.4429\n",
      "Epoch 5/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1030.3502 - val_loss: 1046.1989\n",
      "Epoch 6/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1030.1063 - val_loss: 1045.9552\n",
      "Epoch 7/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1029.8627 - val_loss: 1045.7113\n",
      "Epoch 8/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1029.6196 - val_loss: 1045.4675\n",
      "Epoch 9/30\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 1029.3760 - val_loss: 1045.2235\n",
      "Epoch 10/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1029.1322 - val_loss: 1044.9795\n",
      "Epoch 11/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1028.8885 - val_loss: 1044.7358\n",
      "Epoch 12/30\n",
      "197/197 [==============================] - 6s 33ms/step - loss: 1028.6450 - val_loss: 1044.4921\n",
      "Epoch 13/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1028.4015 - val_loss: 1044.2484\n",
      "Epoch 14/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1028.1581 - val_loss: 1044.0045\n",
      "Epoch 15/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1027.9146 - val_loss: 1043.7606\n",
      "Epoch 16/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1027.6714 - val_loss: 1043.5175\n",
      "Epoch 17/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1027.4274 - val_loss: 1043.2733\n",
      "Epoch 18/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1027.1843 - val_loss: 1043.0298\n",
      "Epoch 19/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1026.9406 - val_loss: 1042.7860\n",
      "Epoch 20/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1026.6974 - val_loss: 1042.5421\n",
      "Epoch 21/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1026.4531 - val_loss: 1042.2986\n",
      "Epoch 22/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1026.2097 - val_loss: 1042.0549\n",
      "Epoch 23/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1025.9667 - val_loss: 1041.8112\n",
      "Epoch 24/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1025.7234 - val_loss: 1041.5676\n",
      "Epoch 25/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1025.4799 - val_loss: 1041.3237\n",
      "Epoch 26/30\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 1025.2362 - val_loss: 1041.0803\n",
      "Epoch 27/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1024.9928 - val_loss: 1040.8364\n",
      "Epoch 28/30\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 1024.7498 - val_loss: 1040.5929\n",
      "Epoch 29/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1024.5063 - val_loss: 1040.3492\n",
      "Epoch 30/30\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 1024.2629 - val_loss: 1040.1057\n",
      "131/131 [==============================] - 1s 11ms/step\n",
      "In calc_results: 12572, 4191, 4191, sum = 20954\n",
      "In split_to_train_test: dataset_X.shape=(14489, 11, 65), dataset_y.shape=(14489, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 12515677508206592.0000 - val_loss: 12547475298582528.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515673213239296.0000 - val_loss: 12547475298582528.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515769850003456.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 12515677508206592.0000 - val_loss: 12547475298582528.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 12ms/step\n",
      "In calc_results: 8693, 2898, 2898, sum = 14489\n",
      "In split_to_train_test: dataset_X.shape=(96933, 11, 65), dataset_y.shape=(96933, 65)\n",
      "Epoch 1/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 791.9721 - val_loss: 793.9699\n",
      "Epoch 2/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 790.7288 - val_loss: 792.7227\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909/909 [==============================] - 28s 31ms/step - loss: 789.4865 - val_loss: 791.4764\n",
      "Epoch 4/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 788.2462 - val_loss: 790.2317\n",
      "Epoch 5/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 787.0066 - val_loss: 788.9886\n",
      "Epoch 6/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 785.7697 - val_loss: 787.7490\n",
      "Epoch 7/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 784.5405 - val_loss: 786.5198\n",
      "Epoch 8/30\n",
      "909/909 [==============================] - 28s 30ms/step - loss: 783.3257 - val_loss: 785.3090\n",
      "Epoch 9/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 782.0275 - val_loss: 783.8920\n",
      "Epoch 10/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 780.6612 - val_loss: 782.5894\n",
      "Epoch 11/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 779.3768 - val_loss: 781.3076\n",
      "Epoch 12/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 778.1061 - val_loss: 780.0358\n",
      "Epoch 13/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 776.8420 - val_loss: 778.7698\n",
      "Epoch 14/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 775.5828 - val_loss: 777.5110\n",
      "Epoch 15/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 774.3304 - val_loss: 776.2635\n",
      "Epoch 16/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 773.0915 - val_loss: 775.0303\n",
      "Epoch 17/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 771.8586 - val_loss: 773.8013\n",
      "Epoch 18/30\n",
      "909/909 [==============================] - 28s 30ms/step - loss: 770.6311 - val_loss: 772.5762\n",
      "Epoch 19/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 769.4074 - val_loss: 771.3537\n",
      "Epoch 20/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 768.1879 - val_loss: 770.1323\n",
      "Epoch 21/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 766.9719 - val_loss: 768.9139\n",
      "Epoch 22/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 765.7632 - val_loss: 767.6993\n",
      "Epoch 23/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 764.5616 - val_loss: 766.4875\n",
      "Epoch 24/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 763.3679 - val_loss: 765.2779\n",
      "Epoch 25/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 762.1796 - val_loss: 764.0730\n",
      "Epoch 26/30\n",
      "909/909 [==============================] - 28s 31ms/step - loss: 760.9984 - val_loss: 762.8782\n",
      "Epoch 27/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 759.8257 - val_loss: 761.6946\n",
      "Epoch 28/30\n",
      "909/909 [==============================] - 30s 33ms/step - loss: 758.6602 - val_loss: 760.5228\n",
      "Epoch 29/30\n",
      "909/909 [==============================] - 29s 32ms/step - loss: 757.5010 - val_loss: 759.3620\n",
      "Epoch 30/30\n",
      "909/909 [==============================] - 28s 30ms/step - loss: 756.3506 - val_loss: 758.2107\n",
      "606/606 [==============================] - 6s 11ms/step\n",
      "In calc_results: 58160, 19386, 19387, sum = 96933\n",
      "N_clusters=7\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(93232, 11, 65), dataset_y.shape=(93232, 65)\n",
      "Epoch 1/30\n",
      "875/875 [==============================] - 29s 34ms/step - loss: 788.7578 - val_loss: 787.5394\n",
      "Epoch 2/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 788.0221 - val_loss: 786.7990\n",
      "Epoch 3/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 787.2844 - val_loss: 786.0584\n",
      "Epoch 4/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 786.5486 - val_loss: 785.3181\n",
      "Epoch 5/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 785.8134 - val_loss: 784.5777\n",
      "Epoch 6/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 785.0778 - val_loss: 783.8375\n",
      "Epoch 7/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 784.3437 - val_loss: 783.0988\n",
      "Epoch 8/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 783.6103 - val_loss: 782.3599\n",
      "Epoch 9/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 782.8782 - val_loss: 781.6224\n",
      "Epoch 10/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 782.1474 - val_loss: 780.8871\n",
      "Epoch 11/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 781.4209 - val_loss: 780.1584\n",
      "Epoch 12/30\n",
      "875/875 [==============================] - 28s 33ms/step - loss: 780.6996 - val_loss: 779.4349\n",
      "Epoch 13/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 779.9849 - val_loss: 778.7150\n",
      "Epoch 14/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 779.2741 - val_loss: 777.9978\n",
      "Epoch 15/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 778.5679 - val_loss: 777.2875\n",
      "Epoch 16/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 777.8641 - val_loss: 776.5786\n",
      "Epoch 17/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 777.1605 - val_loss: 775.8716\n",
      "Epoch 18/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 776.4597 - val_loss: 775.1663\n",
      "Epoch 19/30\n",
      "875/875 [==============================] - 28s 31ms/step - loss: 775.7596 - val_loss: 774.4627\n",
      "Epoch 20/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 775.0613 - val_loss: 773.7611\n",
      "Epoch 21/30\n",
      "875/875 [==============================] - 27s 30ms/step - loss: 774.3644 - val_loss: 773.0616\n",
      "Epoch 22/30\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 773.6687 - val_loss: 772.3636\n",
      "Epoch 23/30\n",
      "875/875 [==============================] - 27s 30ms/step - loss: 772.9746 - val_loss: 771.6677\n",
      "Epoch 24/30\n",
      "875/875 [==============================] - 27s 30ms/step - loss: 772.2805 - val_loss: 770.9745\n",
      "Epoch 25/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 771.5895 - val_loss: 770.2832\n",
      "Epoch 26/30\n",
      "875/875 [==============================] - 30s 34ms/step - loss: 770.9014 - val_loss: 769.5969\n",
      "Epoch 27/30\n",
      "875/875 [==============================] - 28s 32ms/step - loss: 770.2194 - val_loss: 768.9177\n",
      "Epoch 28/30\n",
      "875/875 [==============================] - 30s 34ms/step - loss: 769.5402 - val_loss: 768.2397\n",
      "Epoch 29/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 768.8610 - val_loss: 767.5632\n",
      "Epoch 30/30\n",
      "875/875 [==============================] - 27s 31ms/step - loss: 768.1821 - val_loss: 766.8871\n",
      "583/583 [==============================] - 6s 10ms/step\n",
      "In calc_results: 55939, 18647, 18646, sum = 93232\n",
      "In split_to_train_test: dataset_X.shape=(9862, 11, 65), dataset_y.shape=(9862, 65)\n",
      "Epoch 1/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 154.4073 - val_loss: 185.7959\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 154.3220 - val_loss: 185.7106\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 154.2394 - val_loss: 185.6251\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 154.1599 - val_loss: 185.5422\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 154.0841 - val_loss: 185.4618\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 154.0099 - val_loss: 185.3855\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 3s 35ms/step - loss: 153.9407 - val_loss: 185.3139\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 153.8739 - val_loss: 185.2429\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 153.8079 - val_loss: 185.1720\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 153.7421 - val_loss: 185.1017\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 153.6764 - val_loss: 185.0308\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 153.6066 - val_loss: 184.9539\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 153.5315 - val_loss: 184.8779\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 3s 32ms/step - loss: 153.4563 - val_loss: 184.8029\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 153.3830 - val_loss: 184.7303\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 3s 33ms/step - loss: 153.3112 - val_loss: 184.6574\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 3s 30ms/step - loss: 153.2402 - val_loss: 184.5839\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 153.1698 - val_loss: 184.5120\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 153.0998 - val_loss: 184.4401\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 153.0302 - val_loss: 184.3684\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 152.9624 - val_loss: 184.2996\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 3s 30ms/step - loss: 152.8951 - val_loss: 184.2300\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 152.8281 - val_loss: 184.1599\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 3s 32ms/step - loss: 152.7613 - val_loss: 184.0908\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 3s 31ms/step - loss: 152.6949 - val_loss: 184.0214\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 152.6284 - val_loss: 183.9521\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 152.5623 - val_loss: 183.8828\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 3s 34ms/step - loss: 152.4963 - val_loss: 183.8130\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 152.4305 - val_loss: 183.7439\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 3s 33ms/step - loss: 152.3659 - val_loss: 183.6770\n",
      "62/62 [==============================] - 1s 11ms/step\n",
      "In calc_results: 5917, 1973, 1972, sum = 9862\n",
      "In split_to_train_test: dataset_X.shape=(141672, 11, 65), dataset_y.shape=(141672, 65)\n",
      "Epoch 1/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 795.3496 - val_loss: 793.7443\n",
      "Epoch 2/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 793.5396 - val_loss: 791.9257\n",
      "Epoch 3/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 791.7445 - val_loss: 790.1082\n",
      "Epoch 4/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 789.9542 - val_loss: 788.2933\n",
      "Epoch 5/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 788.1654 - val_loss: 786.4833\n",
      "Epoch 6/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 786.3822 - val_loss: 784.6859\n",
      "Epoch 7/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 784.6174 - val_loss: 782.9240\n",
      "Epoch 8/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 782.8661 - val_loss: 781.1782\n",
      "Epoch 9/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 781.1306 - val_loss: 779.4580\n",
      "Epoch 10/30\n",
      "1329/1329 [==============================] - 41s 30ms/step - loss: 779.4078 - val_loss: 777.7510\n",
      "Epoch 11/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 777.6918 - val_loss: 776.0572\n",
      "Epoch 12/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 775.9839 - val_loss: 774.3740\n",
      "Epoch 13/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 774.2867 - val_loss: 772.7042\n",
      "Epoch 14/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 772.5980 - val_loss: 771.0435\n",
      "Epoch 15/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 770.9200 - val_loss: 769.3892\n",
      "Epoch 16/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 769.2488 - val_loss: 767.7416\n",
      "Epoch 17/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 767.5875 - val_loss: 766.1097\n",
      "Epoch 18/30\n",
      "1329/1329 [==============================] - 41s 31ms/step - loss: 765.9446 - val_loss: 764.4919\n",
      "Epoch 19/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 764.3114 - val_loss: 762.8771\n",
      "Epoch 20/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 762.6816 - val_loss: 761.2665\n",
      "Epoch 21/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 761.0577 - val_loss: 759.6661\n",
      "Epoch 22/30\n",
      "1329/1329 [==============================] - 42s 32ms/step - loss: 759.4433 - val_loss: 758.0768\n",
      "Epoch 23/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 757.8403 - val_loss: 756.5014\n",
      "Epoch 24/30\n",
      "1329/1329 [==============================] - 43s 32ms/step - loss: 756.2512 - val_loss: 754.9426\n",
      "Epoch 25/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 754.6741 - val_loss: 753.4030\n",
      "Epoch 26/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 753.1126 - val_loss: 751.8835\n",
      "Epoch 27/30\n",
      "1329/1329 [==============================] - 40s 30ms/step - loss: 751.5648 - val_loss: 750.3861\n",
      "Epoch 28/30\n",
      "1329/1329 [==============================] - 42s 31ms/step - loss: 750.0339 - val_loss: 748.9165\n",
      "Epoch 29/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 748.5181 - val_loss: 747.4797\n",
      "Epoch 30/30\n",
      "1329/1329 [==============================] - 44s 33ms/step - loss: 747.0190 - val_loss: 746.0742\n",
      "886/886 [==============================] - 10s 11ms/step\n",
      "In calc_results: 85003, 28335, 28334, sum = 141672\n",
      "In split_to_train_test: dataset_X.shape=(10862, 11, 65), dataset_y.shape=(10862, 65)\n",
      "Epoch 1/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1132.3645 - val_loss: 1140.7610\n",
      "Epoch 2/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1132.1705 - val_loss: 1140.5665\n",
      "Epoch 3/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1131.9760 - val_loss: 1140.3716\n",
      "Epoch 4/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1131.7814 - val_loss: 1140.1770\n",
      "Epoch 5/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1131.5868 - val_loss: 1139.9822\n",
      "Epoch 6/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1131.3927 - val_loss: 1139.7876\n",
      "Epoch 7/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1131.1980 - val_loss: 1139.5928\n",
      "Epoch 8/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1131.0039 - val_loss: 1139.3981\n",
      "Epoch 9/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1130.8092 - val_loss: 1139.2034\n",
      "Epoch 10/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1130.6147 - val_loss: 1139.0087\n",
      "Epoch 11/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1130.4205 - val_loss: 1138.8137\n",
      "Epoch 12/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1130.2262 - val_loss: 1138.6191\n",
      "Epoch 13/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1130.0319 - val_loss: 1138.4246\n",
      "Epoch 14/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1129.8373 - val_loss: 1138.2295\n",
      "Epoch 15/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1129.6434 - val_loss: 1138.0350\n",
      "Epoch 16/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1129.4487 - val_loss: 1137.8402\n",
      "Epoch 17/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1129.2545 - val_loss: 1137.6455\n",
      "Epoch 18/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1129.0599 - val_loss: 1137.4507\n",
      "Epoch 19/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1128.8658 - val_loss: 1137.2562\n",
      "Epoch 20/30\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 1128.6714 - val_loss: 1137.0614\n",
      "Epoch 21/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1128.4772 - val_loss: 1136.8668\n",
      "Epoch 22/30\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 1128.2825 - val_loss: 1136.6721\n",
      "Epoch 23/30\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 1128.0885 - val_loss: 1136.4774\n",
      "Epoch 24/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1127.8939 - val_loss: 1136.2825\n",
      "Epoch 25/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1127.6996 - val_loss: 1136.0883\n",
      "Epoch 26/30\n",
      "102/102 [==============================] - 3s 30ms/step - loss: 1127.5055 - val_loss: 1135.8934\n",
      "Epoch 27/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1127.3109 - val_loss: 1135.6987\n",
      "Epoch 28/30\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 1127.1163 - val_loss: 1135.5040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.9221 - val_loss: 1135.3094\n",
      "Epoch 30/30\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 1126.7277 - val_loss: 1135.1147\n",
      "68/68 [==============================] - 1s 10ms/step\n",
      "In calc_results: 6517, 2173, 2172, sum = 10862\n",
      "In split_to_train_test: dataset_X.shape=(24538, 11, 65), dataset_y.shape=(24538, 65)\n",
      "Epoch 1/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 995.9687 - val_loss: 1025.4841\n",
      "Epoch 2/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 995.6832 - val_loss: 1025.2001\n",
      "Epoch 3/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 995.3986 - val_loss: 1024.9156\n",
      "Epoch 4/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 995.1138 - val_loss: 1024.6317\n",
      "Epoch 5/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 994.8291 - val_loss: 1024.3475\n",
      "Epoch 6/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 994.5447 - val_loss: 1024.0635\n",
      "Epoch 7/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 994.2603 - val_loss: 1023.7795\n",
      "Epoch 8/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 993.9755 - val_loss: 1023.4957\n",
      "Epoch 9/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 993.6905 - val_loss: 1023.2114\n",
      "Epoch 10/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 993.4062 - val_loss: 1022.9277\n",
      "Epoch 11/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 993.1216 - val_loss: 1022.6441\n",
      "Epoch 12/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 992.8369 - val_loss: 1022.3601\n",
      "Epoch 13/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 992.5528 - val_loss: 1022.0764\n",
      "Epoch 14/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 992.2684 - val_loss: 1021.7924\n",
      "Epoch 15/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 991.9838 - val_loss: 1021.5087\n",
      "Epoch 16/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 991.6991 - val_loss: 1021.2252\n",
      "Epoch 17/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 991.4150 - val_loss: 1020.9417\n",
      "Epoch 18/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 991.1303 - val_loss: 1020.6583\n",
      "Epoch 19/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 990.8459 - val_loss: 1020.3744\n",
      "Epoch 20/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 990.5612 - val_loss: 1020.0911\n",
      "Epoch 21/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 990.2775 - val_loss: 1019.8075\n",
      "Epoch 22/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 989.9929 - val_loss: 1019.5244\n",
      "Epoch 23/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 989.7087 - val_loss: 1019.2407\n",
      "Epoch 24/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 989.4240 - val_loss: 1018.9574\n",
      "Epoch 25/30\n",
      "231/231 [==============================] - 8s 33ms/step - loss: 989.1402 - val_loss: 1018.6740\n",
      "Epoch 26/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 988.8555 - val_loss: 1018.3904\n",
      "Epoch 27/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 988.5715 - val_loss: 1018.1071\n",
      "Epoch 28/30\n",
      "231/231 [==============================] - 7s 32ms/step - loss: 988.2877 - val_loss: 1017.8236\n",
      "Epoch 29/30\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 988.0034 - val_loss: 1017.5400\n",
      "Epoch 30/30\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 987.7194 - val_loss: 1017.2568\n",
      "154/154 [==============================] - 2s 11ms/step\n",
      "In calc_results: 14723, 4907, 4908, sum = 24538\n",
      "In split_to_train_test: dataset_X.shape=(14488, 11, 65), dataset_y.shape=(14488, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515618452406272.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515613083697152.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515641000984576.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515612009955328.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8693, 2897, 2898, sum = 14488\n",
      "In split_to_train_test: dataset_X.shape=(31812, 11, 65), dataset_y.shape=(31812, 65)\n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 403.5208 - val_loss: 400.8733\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 10s 33ms/step - loss: 403.1012 - val_loss: 400.4601\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 402.6888 - val_loss: 400.0490\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 402.2766 - val_loss: 399.6382\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 401.8646 - val_loss: 399.2276\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 401.4526 - val_loss: 398.8171\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 401.0408 - val_loss: 398.4066\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 9s 30ms/step - loss: 400.6291 - val_loss: 397.9965\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 400.2188 - val_loss: 397.5872\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 399.8087 - val_loss: 397.1782\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 10s 33ms/step - loss: 399.3991 - val_loss: 396.7693\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 398.9893 - val_loss: 396.3603\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 398.5801 - val_loss: 395.9514\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 398.1704 - val_loss: 395.5426\n",
      "Epoch 15/30\n",
      "299/299 [==============================] - 9s 30ms/step - loss: 397.7608 - val_loss: 395.1346\n",
      "Epoch 16/30\n",
      "299/299 [==============================] - 9s 30ms/step - loss: 397.3520 - val_loss: 394.7284\n",
      "Epoch 17/30\n",
      "299/299 [==============================] - 9s 30ms/step - loss: 396.9433 - val_loss: 394.3226\n",
      "Epoch 18/30\n",
      "299/299 [==============================] - 10s 33ms/step - loss: 396.5347 - val_loss: 393.9180\n",
      "Epoch 19/30\n",
      "299/299 [==============================] - 10s 33ms/step - loss: 396.1235 - val_loss: 393.4853\n",
      "Epoch 20/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 395.6469 - val_loss: 393.0079\n",
      "Epoch 21/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 395.1827 - val_loss: 392.5592\n",
      "Epoch 22/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 394.7324 - val_loss: 392.1184\n",
      "Epoch 23/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 394.2878 - val_loss: 391.6819\n",
      "Epoch 24/30\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 393.8462 - val_loss: 391.2490\n",
      "Epoch 25/30\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 393.4066 - val_loss: 390.8182\n",
      "Epoch 26/30\n",
      "299/299 [==============================] - 10s 32ms/step - loss: 392.9682 - val_loss: 390.3889\n",
      "Epoch 27/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 392.5305 - val_loss: 389.9606\n",
      "Epoch 28/30\n",
      "299/299 [==============================] - 9s 32ms/step - loss: 392.0940 - val_loss: 389.5332\n",
      "Epoch 29/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 391.6571 - val_loss: 389.1064\n",
      "Epoch 30/30\n",
      "299/299 [==============================] - 9s 31ms/step - loss: 391.2213 - val_loss: 388.6808\n",
      "199/199 [==============================] - 2s 11ms/step\n",
      "In calc_results: 19087, 6363, 6362, sum = 31812\n",
      "N_clusters=9\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(9797, 11, 65), dataset_y.shape=(9797, 65)\n",
      "Epoch 1/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 154.6214 - val_loss: 186.3387\n",
      "Epoch 2/30\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 154.5709 - val_loss: 186.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 154.5206 - val_loss: 186.2552\n",
      "Epoch 4/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 154.4667 - val_loss: 186.2034\n",
      "Epoch 5/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.4131 - val_loss: 186.1542\n",
      "Epoch 6/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.3533 - val_loss: 186.0905\n",
      "Epoch 7/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.2960 - val_loss: 186.0351\n",
      "Epoch 8/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.2441 - val_loss: 185.9819\n",
      "Epoch 9/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.1945 - val_loss: 185.9300\n",
      "Epoch 10/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.1462 - val_loss: 185.8798\n",
      "Epoch 11/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.0993 - val_loss: 185.8300\n",
      "Epoch 12/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 154.0529 - val_loss: 185.7805\n",
      "Epoch 13/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 154.0067 - val_loss: 185.7321\n",
      "Epoch 14/30\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 153.9608 - val_loss: 185.6833\n",
      "Epoch 15/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 153.9150 - val_loss: 185.6347\n",
      "Epoch 16/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.8694 - val_loss: 185.5864\n",
      "Epoch 17/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.8240 - val_loss: 185.5383\n",
      "Epoch 18/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.7786 - val_loss: 185.4904\n",
      "Epoch 19/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.7333 - val_loss: 185.4425\n",
      "Epoch 20/30\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 153.6880 - val_loss: 185.3951\n",
      "Epoch 21/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.6428 - val_loss: 185.3472\n",
      "Epoch 22/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.5978 - val_loss: 185.2993\n",
      "Epoch 23/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.5528 - val_loss: 185.2520\n",
      "Epoch 24/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.5078 - val_loss: 185.2040\n",
      "Epoch 25/30\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 153.4630 - val_loss: 185.1561\n",
      "Epoch 26/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.4181 - val_loss: 185.1088\n",
      "Epoch 27/30\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 153.3736 - val_loss: 185.0620\n",
      "Epoch 28/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.3298 - val_loss: 185.0160\n",
      "Epoch 29/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.2862 - val_loss: 184.9699\n",
      "Epoch 30/30\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 153.2427 - val_loss: 184.9236\n",
      "62/62 [==============================] - 1s 11ms/step\n",
      "In calc_results: 5878, 1960, 1959, sum = 9797\n",
      "In split_to_train_test: dataset_X.shape=(7329, 11, 65), dataset_y.shape=(7329, 65)\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.5403 - val_loss: 1160.2640\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.4525 - val_loss: 1160.1787\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1153.3676 - val_loss: 1160.0938\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.2828 - val_loss: 1160.0089\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.1979 - val_loss: 1159.9240\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.1132 - val_loss: 1159.8391\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1153.0284 - val_loss: 1159.7542\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.9438 - val_loss: 1159.6693\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.8594 - val_loss: 1159.5845\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.7743 - val_loss: 1159.4996\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.6902 - val_loss: 1159.4148\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.6050 - val_loss: 1159.3302\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.5203 - val_loss: 1159.2454\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.4358 - val_loss: 1159.1605\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.3514 - val_loss: 1159.0757\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.2666 - val_loss: 1158.9908\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1152.1821 - val_loss: 1158.9062\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 1152.0975 - val_loss: 1158.8214\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 1152.0131 - val_loss: 1158.7366\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.9281 - val_loss: 1158.6517\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.8439 - val_loss: 1158.5670\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.7590 - val_loss: 1158.4823\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.6744 - val_loss: 1158.3975\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.5897 - val_loss: 1158.3129\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.5049 - val_loss: 1158.2279\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.4207 - val_loss: 1158.1432\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 2s 31ms/step - loss: 1151.3359 - val_loss: 1158.0586\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 1151.2513 - val_loss: 1157.9736\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 2s 35ms/step - loss: 1151.1667 - val_loss: 1157.8890\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 2s 36ms/step - loss: 1151.0822 - val_loss: 1157.8042\n",
      "46/46 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4397, 1466, 1466, sum = 7329\n",
      "In split_to_train_test: dataset_X.shape=(65248, 11, 65), dataset_y.shape=(65248, 65)\n",
      "Epoch 1/30\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 670.9005 - val_loss: 661.5264\n",
      "Epoch 2/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 669.9180 - val_loss: 660.5518\n",
      "Epoch 3/30\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 668.8877 - val_loss: 659.4194\n",
      "Epoch 4/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 667.7712 - val_loss: 658.3419\n",
      "Epoch 5/30\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 666.7132 - val_loss: 657.2875\n",
      "Epoch 6/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 665.6655 - val_loss: 656.1904\n",
      "Epoch 7/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 664.4982 - val_loss: 655.0245\n",
      "Epoch 8/30\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 663.3729 - val_loss: 653.9205\n",
      "Epoch 9/30\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 662.2784 - val_loss: 652.8420\n",
      "Epoch 10/30\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 661.1963 - val_loss: 651.7728\n",
      "Epoch 11/30\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 660.1193 - val_loss: 650.7104\n",
      "Epoch 12/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 659.0453 - val_loss: 649.6550\n",
      "Epoch 13/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 657.9601 - val_loss: 648.5317\n",
      "Epoch 14/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 656.7814 - val_loss: 647.4013\n",
      "Epoch 15/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 655.6428 - val_loss: 646.2999\n",
      "Epoch 16/30\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 654.5195 - val_loss: 645.2092\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/612 [==============================] - 19s 31ms/step - loss: 653.4053 - val_loss: 644.1248\n",
      "Epoch 18/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 652.2986 - val_loss: 643.0461\n",
      "Epoch 19/30\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 651.1974 - val_loss: 641.9717\n",
      "Epoch 20/30\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 650.1003 - val_loss: 640.9011\n",
      "Epoch 21/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 649.0066 - val_loss: 639.8333\n",
      "Epoch 22/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 647.9167 - val_loss: 638.7686\n",
      "Epoch 23/30\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 646.8310 - val_loss: 637.7081\n",
      "Epoch 24/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 645.7509 - val_loss: 636.6569\n",
      "Epoch 25/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 644.6783 - val_loss: 635.6136\n",
      "Epoch 26/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 643.6085 - val_loss: 634.5725\n",
      "Epoch 27/30\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 642.5417 - val_loss: 633.5343\n",
      "Epoch 28/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 641.4767 - val_loss: 632.4996\n",
      "Epoch 29/30\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 640.4145 - val_loss: 631.4674\n",
      "Epoch 30/30\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 639.3567 - val_loss: 630.4371\n",
      "408/408 [==============================] - 4s 11ms/step\n",
      "In calc_results: 39149, 13049, 13050, sum = 65248\n",
      "In split_to_train_test: dataset_X.shape=(14488, 11, 65), dataset_y.shape=(14488, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515620599889920.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515617378664448.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515005345824768.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 12515617378664448.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8693, 2897, 2898, sum = 14488\n",
      "In split_to_train_test: dataset_X.shape=(72910, 11, 65), dataset_y.shape=(72910, 65)\n",
      "Epoch 1/30\n",
      "684/684 [==============================] - 23s 34ms/step - loss: 772.8296 - val_loss: 770.6936\n",
      "Epoch 2/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 771.9727 - val_loss: 769.8500\n",
      "Epoch 3/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 771.1335 - val_loss: 769.0102\n",
      "Epoch 4/30\n",
      "684/684 [==============================] - 22s 33ms/step - loss: 770.2989 - val_loss: 768.1724\n",
      "Epoch 5/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 769.4659 - val_loss: 767.3362\n",
      "Epoch 6/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 768.6339 - val_loss: 766.5004\n",
      "Epoch 7/30\n",
      "684/684 [==============================] - 21s 30ms/step - loss: 767.8045 - val_loss: 765.6663\n",
      "Epoch 8/30\n",
      "684/684 [==============================] - 22s 31ms/step - loss: 766.9788 - val_loss: 764.8358\n",
      "Epoch 9/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 766.1567 - val_loss: 764.0129\n",
      "Epoch 10/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 765.3390 - val_loss: 763.1938\n",
      "Epoch 11/30\n",
      "684/684 [==============================] - 22s 33ms/step - loss: 764.5311 - val_loss: 762.3824\n",
      "Epoch 12/30\n",
      "684/684 [==============================] - 22s 32ms/step - loss: 763.7318 - val_loss: 761.5793\n",
      "Epoch 13/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 762.9351 - val_loss: 760.7785\n",
      "Epoch 14/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 762.1408 - val_loss: 759.9793\n",
      "Epoch 15/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 761.3479 - val_loss: 759.1821\n",
      "Epoch 16/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 760.5572 - val_loss: 758.3878\n",
      "Epoch 17/30\n",
      "684/684 [==============================] - 22s 31ms/step - loss: 759.7684 - val_loss: 757.5949\n",
      "Epoch 18/30\n",
      "684/684 [==============================] - 22s 32ms/step - loss: 758.9803 - val_loss: 756.8054\n",
      "Epoch 19/30\n",
      "684/684 [==============================] - 22s 32ms/step - loss: 758.1943 - val_loss: 756.0182\n",
      "Epoch 20/30\n",
      "684/684 [==============================] - 23s 33ms/step - loss: 757.4094 - val_loss: 755.2336\n",
      "Epoch 21/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 756.6268 - val_loss: 754.4528\n",
      "Epoch 22/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 755.8492 - val_loss: 753.6782\n",
      "Epoch 23/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 755.0785 - val_loss: 752.9111\n",
      "Epoch 24/30\n",
      "684/684 [==============================] - 23s 33ms/step - loss: 754.3099 - val_loss: 752.1466\n",
      "Epoch 25/30\n",
      "684/684 [==============================] - 22s 33ms/step - loss: 753.5428 - val_loss: 751.3820\n",
      "Epoch 26/30\n",
      "684/684 [==============================] - 22s 32ms/step - loss: 752.7253 - val_loss: 750.4805\n",
      "Epoch 27/30\n",
      "684/684 [==============================] - 23s 33ms/step - loss: 751.8267 - val_loss: 749.6254\n",
      "Epoch 28/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 750.9857 - val_loss: 748.7908\n",
      "Epoch 29/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 750.1578 - val_loss: 747.9626\n",
      "Epoch 30/30\n",
      "684/684 [==============================] - 21s 31ms/step - loss: 749.3359 - val_loss: 747.1372\n",
      "456/456 [==============================] - 5s 11ms/step\n",
      "In calc_results: 43746, 14582, 14582, sum = 72910\n",
      "In split_to_train_test: dataset_X.shape=(22125, 11, 65), dataset_y.shape=(22125, 65)\n",
      "Epoch 1/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 1000.5034 - val_loss: 1022.4319\n",
      "Epoch 2/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 1000.2614 - val_loss: 1022.1914\n",
      "Epoch 3/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 1000.0202 - val_loss: 1021.9509\n",
      "Epoch 4/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 999.7783 - val_loss: 1021.7105\n",
      "Epoch 5/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 999.5368 - val_loss: 1021.4700\n",
      "Epoch 6/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 999.2955 - val_loss: 1021.2296\n",
      "Epoch 7/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 999.0543 - val_loss: 1020.9893\n",
      "Epoch 8/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 998.8129 - val_loss: 1020.7487\n",
      "Epoch 9/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 998.5712 - val_loss: 1020.5087\n",
      "Epoch 10/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 998.3297 - val_loss: 1020.2686\n",
      "Epoch 11/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 998.0888 - val_loss: 1020.0285\n",
      "Epoch 12/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 997.8475 - val_loss: 1019.7885\n",
      "Epoch 13/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 997.6060 - val_loss: 1019.5485\n",
      "Epoch 14/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 997.3644 - val_loss: 1019.3082\n",
      "Epoch 15/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 997.1234 - val_loss: 1019.0682\n",
      "Epoch 16/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 996.8823 - val_loss: 1018.8282\n",
      "Epoch 17/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 996.6407 - val_loss: 1018.5886\n",
      "Epoch 18/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 996.3992 - val_loss: 1018.3484\n",
      "Epoch 19/30\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 996.1581 - val_loss: 1018.1083\n",
      "Epoch 20/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 995.9168 - val_loss: 1017.8685\n",
      "Epoch 21/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 995.6752 - val_loss: 1017.6284\n",
      "Epoch 22/30\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 995.4343 - val_loss: 1017.3884\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 7s 31ms/step - loss: 995.1927 - val_loss: 1017.1488\n",
      "Epoch 24/30\n",
      "208/208 [==============================] - 7s 33ms/step - loss: 994.9520 - val_loss: 1016.9088\n",
      "Epoch 25/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 994.7108 - val_loss: 1016.6687\n",
      "Epoch 26/30\n",
      "208/208 [==============================] - 6s 31ms/step - loss: 994.4693 - val_loss: 1016.4290\n",
      "Epoch 27/30\n",
      "208/208 [==============================] - 7s 32ms/step - loss: 994.2285 - val_loss: 1016.1888\n",
      "Epoch 28/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 993.9868 - val_loss: 1015.9490\n",
      "Epoch 29/30\n",
      "208/208 [==============================] - 6s 30ms/step - loss: 993.7454 - val_loss: 1015.7093\n",
      "Epoch 30/30\n",
      "208/208 [==============================] - 7s 31ms/step - loss: 993.5041 - val_loss: 1015.4692\n",
      "139/139 [==============================] - 2s 12ms/step\n",
      "In calc_results: 13275, 4425, 4425, sum = 22125\n",
      "In split_to_train_test: dataset_X.shape=(11549, 11, 65), dataset_y.shape=(11549, 65)\n",
      "Epoch 1/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1070.3489 - val_loss: 1089.9995\n",
      "Epoch 2/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1070.1843 - val_loss: 1089.8353\n",
      "Epoch 3/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1070.0195 - val_loss: 1089.6711\n",
      "Epoch 4/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1069.8556 - val_loss: 1089.5070\n",
      "Epoch 5/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1069.6912 - val_loss: 1089.3430\n",
      "Epoch 6/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1069.5262 - val_loss: 1089.1788\n",
      "Epoch 7/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1069.3619 - val_loss: 1089.0145\n",
      "Epoch 8/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1069.1976 - val_loss: 1088.8500\n",
      "Epoch 9/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1069.0331 - val_loss: 1088.6858\n",
      "Epoch 10/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1068.8687 - val_loss: 1088.5219\n",
      "Epoch 11/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1068.7043 - val_loss: 1088.3578\n",
      "Epoch 12/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1068.5400 - val_loss: 1088.1936\n",
      "Epoch 13/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1068.3756 - val_loss: 1088.0293\n",
      "Epoch 14/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1068.2111 - val_loss: 1087.8654\n",
      "Epoch 15/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1068.0466 - val_loss: 1087.7014\n",
      "Epoch 16/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1067.8823 - val_loss: 1087.5370\n",
      "Epoch 17/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1067.7180 - val_loss: 1087.3727\n",
      "Epoch 18/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1067.5536 - val_loss: 1087.2084\n",
      "Epoch 19/30\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 1067.3893 - val_loss: 1087.0443\n",
      "Epoch 20/30\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1067.2247 - val_loss: 1086.8802\n",
      "Epoch 21/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1067.0605 - val_loss: 1086.7162\n",
      "Epoch 22/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1066.8960 - val_loss: 1086.5520\n",
      "Epoch 23/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1066.7313 - val_loss: 1086.3881\n",
      "Epoch 24/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1066.5675 - val_loss: 1086.2241\n",
      "Epoch 25/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1066.4027 - val_loss: 1086.0598\n",
      "Epoch 26/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1066.2385 - val_loss: 1085.8955\n",
      "Epoch 27/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1066.0741 - val_loss: 1085.7313\n",
      "Epoch 28/30\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 1065.9097 - val_loss: 1085.5673\n",
      "Epoch 29/30\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1065.7451 - val_loss: 1085.4032\n",
      "Epoch 30/30\n",
      "109/109 [==============================] - 3s 30ms/step - loss: 1065.5809 - val_loss: 1085.2393\n",
      "73/73 [==============================] - 1s 11ms/step\n",
      "In calc_results: 6929, 2310, 2310, sum = 11549\n",
      "In split_to_train_test: dataset_X.shape=(18864, 11, 65), dataset_y.shape=(18864, 65)\n",
      "Epoch 1/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 366.3127 - val_loss: 362.8287\n",
      "Epoch 2/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 366.0531 - val_loss: 362.5897\n",
      "Epoch 3/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 365.8123 - val_loss: 362.3554\n",
      "Epoch 4/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 365.5740 - val_loss: 362.1225\n",
      "Epoch 5/30\n",
      "177/177 [==============================] - 6s 33ms/step - loss: 365.3367 - val_loss: 361.8901\n",
      "Epoch 6/30\n",
      "177/177 [==============================] - 6s 34ms/step - loss: 365.0999 - val_loss: 361.6580\n",
      "Epoch 7/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 364.8633 - val_loss: 361.4269\n",
      "Epoch 8/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 364.6272 - val_loss: 361.1970\n",
      "Epoch 9/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 364.3925 - val_loss: 360.9678\n",
      "Epoch 10/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 364.1581 - val_loss: 360.7389\n",
      "Epoch 11/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 363.9239 - val_loss: 360.5097\n",
      "Epoch 12/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 363.6895 - val_loss: 360.2807\n",
      "Epoch 13/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 363.4552 - val_loss: 360.0517\n",
      "Epoch 14/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 363.2212 - val_loss: 359.8228\n",
      "Epoch 15/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 362.9868 - val_loss: 359.5935\n",
      "Epoch 16/30\n",
      "177/177 [==============================] - 6s 31ms/step - loss: 362.7526 - val_loss: 359.3646\n",
      "Epoch 17/30\n",
      "177/177 [==============================] - 5s 31ms/step - loss: 362.5183 - val_loss: 359.1357\n",
      "Epoch 18/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 362.2842 - val_loss: 358.9071\n",
      "Epoch 19/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 362.0503 - val_loss: 358.6787\n",
      "Epoch 20/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 361.8163 - val_loss: 358.4504\n",
      "Epoch 21/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 361.5825 - val_loss: 358.2222\n",
      "Epoch 22/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 361.3485 - val_loss: 357.9941\n",
      "Epoch 23/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 361.1147 - val_loss: 357.7660\n",
      "Epoch 24/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 360.8810 - val_loss: 357.5379\n",
      "Epoch 25/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 360.6470 - val_loss: 357.3098\n",
      "Epoch 26/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 360.4133 - val_loss: 357.0819\n",
      "Epoch 27/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 360.1794 - val_loss: 356.8541\n",
      "Epoch 28/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 359.9458 - val_loss: 356.6267\n",
      "Epoch 29/30\n",
      "177/177 [==============================] - 5s 30ms/step - loss: 359.7123 - val_loss: 356.3999\n",
      "Epoch 30/30\n",
      "177/177 [==============================] - 6s 32ms/step - loss: 359.4788 - val_loss: 356.1731\n",
      "118/118 [==============================] - 1s 11ms/step\n",
      "In calc_results: 11318, 3773, 3773, sum = 18864\n",
      "In split_to_train_test: dataset_X.shape=(104156, 11, 65), dataset_y.shape=(104156, 65)\n",
      "Epoch 1/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 851.0070 - val_loss: 850.4915\n",
      "Epoch 2/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 849.7338 - val_loss: 849.1989\n",
      "Epoch 3/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 848.4133 - val_loss: 847.7473\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977/977 [==============================] - 31s 32ms/step - loss: 846.9617 - val_loss: 846.3271\n",
      "Epoch 5/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 845.5778 - val_loss: 844.9324\n",
      "Epoch 6/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 844.2092 - val_loss: 843.5450\n",
      "Epoch 7/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 842.8450 - val_loss: 842.1634\n",
      "Epoch 8/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 841.4855 - val_loss: 840.7922\n",
      "Epoch 9/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 840.1416 - val_loss: 839.4443\n",
      "Epoch 10/30\n",
      "977/977 [==============================] - 32s 32ms/step - loss: 838.8082 - val_loss: 838.1027\n",
      "Epoch 11/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 837.4787 - val_loss: 836.7702\n",
      "Epoch 12/30\n",
      "977/977 [==============================] - 32s 33ms/step - loss: 836.1597 - val_loss: 835.4515\n",
      "Epoch 13/30\n",
      "977/977 [==============================] - 33s 34ms/step - loss: 834.8489 - val_loss: 834.1376\n",
      "Epoch 14/30\n",
      "977/977 [==============================] - 33s 33ms/step - loss: 833.5411 - val_loss: 832.8297\n",
      "Epoch 15/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 832.1469 - val_loss: 831.3837\n",
      "Epoch 16/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 830.7477 - val_loss: 830.0104\n",
      "Epoch 17/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 829.3767 - val_loss: 828.6524\n",
      "Epoch 18/30\n",
      "977/977 [==============================] - 32s 33ms/step - loss: 828.0153 - val_loss: 827.3044\n",
      "Epoch 19/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 826.6611 - val_loss: 825.9650\n",
      "Epoch 20/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 825.3113 - val_loss: 824.6327\n",
      "Epoch 21/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 823.9677 - val_loss: 823.3087\n",
      "Epoch 22/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 822.6348 - val_loss: 822.0012\n",
      "Epoch 23/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 821.3170 - val_loss: 820.7021\n",
      "Epoch 24/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 820.0061 - val_loss: 819.4057\n",
      "Epoch 25/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 818.6981 - val_loss: 818.1127\n",
      "Epoch 26/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 817.3947 - val_loss: 816.8238\n",
      "Epoch 27/30\n",
      "977/977 [==============================] - 31s 31ms/step - loss: 816.0959 - val_loss: 815.5423\n",
      "Epoch 28/30\n",
      "977/977 [==============================] - 30s 31ms/step - loss: 814.8034 - val_loss: 814.2706\n",
      "Epoch 29/30\n",
      "977/977 [==============================] - 31s 32ms/step - loss: 813.5159 - val_loss: 813.0088\n",
      "Epoch 30/30\n",
      "977/977 [==============================] - 32s 33ms/step - loss: 812.2366 - val_loss: 811.7574\n",
      "651/651 [==============================] - 7s 11ms/step\n",
      "In calc_results: 62494, 20831, 20831, sum = 104156\n",
      "N_clusters=11\n",
      "dataset_windows.shape=(326466, 1, 12, 65), labels.shape=(326466,)\n",
      "In split_to_train_test: dataset_X.shape=(9579, 11, 65), dataset_y.shape=(9579, 65)\n",
      "Epoch 1/30\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 155.5281 - val_loss: 188.1815\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 155.4579 - val_loss: 188.1015\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 155.3834 - val_loss: 188.0060\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 155.3086 - val_loss: 187.9284\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 3s 34ms/step - loss: 155.2417 - val_loss: 187.8551\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 155.1789 - val_loss: 187.7851\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 155.1145 - val_loss: 187.7173\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 155.0574 - val_loss: 187.6525\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 155.0018 - val_loss: 187.5888\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 154.9468 - val_loss: 187.5249\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 3s 35ms/step - loss: 154.8924 - val_loss: 187.4609\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 3s 35ms/step - loss: 154.8384 - val_loss: 187.3978\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.7847 - val_loss: 187.3348\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.7318 - val_loss: 187.2739\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.6794 - val_loss: 187.2118\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.6270 - val_loss: 187.1506\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.5748 - val_loss: 187.0894\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.5226 - val_loss: 187.0277\n",
      "Epoch 19/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.4706 - val_loss: 186.9670\n",
      "Epoch 20/30\n",
      "90/90 [==============================] - 3s 33ms/step - loss: 154.4187 - val_loss: 186.9057\n",
      "Epoch 21/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.3669 - val_loss: 186.8452\n",
      "Epoch 22/30\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 154.3152 - val_loss: 186.7845\n",
      "Epoch 23/30\n",
      "90/90 [==============================] - 3s 34ms/step - loss: 154.2635 - val_loss: 186.7236\n",
      "Epoch 24/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.2119 - val_loss: 186.6632\n",
      "Epoch 25/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.1602 - val_loss: 186.6032\n",
      "Epoch 26/30\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 154.1086 - val_loss: 186.5428\n",
      "Epoch 27/30\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 154.0571 - val_loss: 186.4823\n",
      "Epoch 28/30\n",
      "90/90 [==============================] - 3s 31ms/step - loss: 154.0055 - val_loss: 186.4231\n",
      "Epoch 29/30\n",
      "90/90 [==============================] - 3s 32ms/step - loss: 153.9540 - val_loss: 186.3627\n",
      "Epoch 30/30\n",
      "90/90 [==============================] - 3s 30ms/step - loss: 153.9026 - val_loss: 186.3025\n",
      "60/60 [==============================] - 1s 10ms/step\n",
      "In calc_results: 5747, 1916, 1916, sum = 9579\n",
      "In split_to_train_test: dataset_X.shape=(7830, 11, 65), dataset_y.shape=(7830, 65)\n",
      "Epoch 1/30\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 1141.6252 - val_loss: 1167.0645\n",
      "Epoch 2/30\n",
      "74/74 [==============================] - 3s 36ms/step - loss: 1141.5116 - val_loss: 1166.9508\n",
      "Epoch 3/30\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 1141.3981 - val_loss: 1166.8367\n",
      "Epoch 4/30\n",
      "74/74 [==============================] - 3s 34ms/step - loss: 1141.2841 - val_loss: 1166.7229\n",
      "Epoch 5/30\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 1141.1708 - val_loss: 1166.6094\n",
      "Epoch 6/30\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 1141.0570 - val_loss: 1166.4955\n",
      "Epoch 7/30\n",
      "74/74 [==============================] - 3s 34ms/step - loss: 1140.9437 - val_loss: 1166.3817\n",
      "Epoch 8/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.8302 - val_loss: 1166.2678\n",
      "Epoch 9/30\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 1140.7162 - val_loss: 1166.1543\n",
      "Epoch 10/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.6029 - val_loss: 1166.0404\n",
      "Epoch 11/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.4890 - val_loss: 1165.9265\n",
      "Epoch 12/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.3755 - val_loss: 1165.8129\n",
      "Epoch 13/30\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 1140.2621 - val_loss: 1165.6991\n",
      "Epoch 14/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.1483 - val_loss: 1165.5854\n",
      "Epoch 15/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1140.0348 - val_loss: 1165.4713\n",
      "Epoch 16/30\n",
      "74/74 [==============================] - 3s 34ms/step - loss: 1139.9210 - val_loss: 1165.3577\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 2s 33ms/step - loss: 1139.8074 - val_loss: 1165.2441\n",
      "Epoch 18/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1139.6937 - val_loss: 1165.1302\n",
      "Epoch 19/30\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 1139.5803 - val_loss: 1165.0162\n",
      "Epoch 20/30\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 1139.4668 - val_loss: 1164.9027\n",
      "Epoch 21/30\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 1139.3530 - val_loss: 1164.7892\n",
      "Epoch 22/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1139.2393 - val_loss: 1164.6752\n",
      "Epoch 23/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1139.1261 - val_loss: 1164.5613\n",
      "Epoch 24/30\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 1139.0122 - val_loss: 1164.4478\n",
      "Epoch 25/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1138.8990 - val_loss: 1164.3341\n",
      "Epoch 26/30\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 1138.7852 - val_loss: 1164.2201\n",
      "Epoch 27/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1138.6715 - val_loss: 1164.1064\n",
      "Epoch 28/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1138.5579 - val_loss: 1163.9927\n",
      "Epoch 29/30\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 1138.4442 - val_loss: 1163.8789\n",
      "Epoch 30/30\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 1138.3306 - val_loss: 1163.7653\n",
      "49/49 [==============================] - 1s 11ms/step\n",
      "In calc_results: 4698, 1566, 1566, sum = 7830\n",
      "In split_to_train_test: dataset_X.shape=(6021, 11, 65), dataset_y.shape=(6021, 65)\n",
      "Epoch 1/30\n",
      "57/57 [==============================] - 2s 35ms/step - loss: 224.3458 - val_loss: 227.4635\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 224.2533 - val_loss: 227.3846\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 224.1770 - val_loss: 227.3086\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 224.0945 - val_loss: 227.2164\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 224.0097 - val_loss: 227.1328\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.9290 - val_loss: 227.0511\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.8497 - val_loss: 226.9696\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.7652 - val_loss: 226.8825\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.6848 - val_loss: 226.8005\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 2s 35ms/step - loss: 223.6058 - val_loss: 226.7202\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.5276 - val_loss: 226.6402\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.4496 - val_loss: 226.5603\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.3718 - val_loss: 226.4806\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 223.2942 - val_loss: 226.4014\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 223.2130 - val_loss: 226.3041\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 223.1130 - val_loss: 226.2054\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 223.0204 - val_loss: 226.1130\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 2s 35ms/step - loss: 222.9315 - val_loss: 226.0228\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.8444 - val_loss: 225.9340\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 222.7583 - val_loss: 225.8459\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 222.6640 - val_loss: 225.7316\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.5616 - val_loss: 225.6421\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.4746 - val_loss: 225.5532\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.3882 - val_loss: 225.4646\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.3020 - val_loss: 225.3766\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.2163 - val_loss: 225.2886\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 222.1308 - val_loss: 225.2010\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 222.0456 - val_loss: 225.1136\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 221.9607 - val_loss: 225.0263\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 221.8759 - val_loss: 224.9394\n",
      "38/38 [==============================] - 0s 11ms/step\n",
      "In calc_results: 3613, 1204, 1204, sum = 6021\n",
      "In split_to_train_test: dataset_X.shape=(21775, 11, 65), dataset_y.shape=(21775, 65)\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 697.5362 - val_loss: 686.4810\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 697.2548 - val_loss: 686.2158\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 696.9741 - val_loss: 685.9507\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 696.6932 - val_loss: 685.6859\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 696.4127 - val_loss: 685.4213\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 696.1317 - val_loss: 685.1564\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 695.8509 - val_loss: 684.8922\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 695.5716 - val_loss: 684.6290\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 7s 32ms/step - loss: 695.2922 - val_loss: 684.3656\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 695.0135 - val_loss: 684.1021\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 694.7344 - val_loss: 683.8386\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 7s 32ms/step - loss: 694.4553 - val_loss: 683.5748\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 694.1760 - val_loss: 683.3109\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 693.8966 - val_loss: 683.0471\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 693.6176 - val_loss: 682.7829\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 693.3380 - val_loss: 682.5188\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 6s 32ms/step - loss: 693.0596 - val_loss: 682.2547\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 692.7800 - val_loss: 681.9906\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 692.5004 - val_loss: 681.7267\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 692.2212 - val_loss: 681.4623\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 691.9423 - val_loss: 681.1985\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 691.6628 - val_loss: 680.9349\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 691.3839 - val_loss: 680.6714\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 691.1052 - val_loss: 680.4078\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 690.8264 - val_loss: 680.1445\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 7s 33ms/step - loss: 690.5475 - val_loss: 679.8809\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 690.2686 - val_loss: 679.6174\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 689.9899 - val_loss: 679.3539\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 7s 33ms/step - loss: 689.7110 - val_loss: 679.0903\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 689.4324 - val_loss: 678.8272\n",
      "137/137 [==============================] - 1s 11ms/step\n",
      "In calc_results: 13065, 4355, 4355, sum = 21775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In split_to_train_test: dataset_X.shape=(14488, 11, 65), dataset_y.shape=(14488, 65)\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 12515615231180800.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 12515617378664448.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3/30\n",
      "135/136 [============================>.] - ETA: 0s - loss: 12515278076248064.0000Restoring model weights from the end of the best epoch: 1.\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 12515614157438976.0000 - val_loss: 12548022906912768.0000\n",
      "Epoch 3: early stopping\n",
      "91/91 [==============================] - 1s 11ms/step\n",
      "In calc_results: 8693, 2897, 2898, sum = 14488\n",
      "In split_to_train_test: dataset_X.shape=(5022, 11, 65), dataset_y.shape=(5022, 65)\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 1116.0916 - val_loss: 1119.7113\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 1116.0439 - val_loss: 1119.6644\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1115.9968 - val_loss: 1119.6172\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1115.9498 - val_loss: 1119.5702\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.9027 - val_loss: 1119.5229\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.8556 - val_loss: 1119.4760\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1115.8085 - val_loss: 1119.4290\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1115.7617 - val_loss: 1119.3818\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.7145 - val_loss: 1119.3346\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.6675 - val_loss: 1119.2877\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.6205 - val_loss: 1119.2406\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1115.5732 - val_loss: 1119.1936\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.5267 - val_loss: 1119.1464\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1115.4796 - val_loss: 1119.0992\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1115.4324 - val_loss: 1119.0521\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1115.3854 - val_loss: 1119.0052\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 1115.3383 - val_loss: 1118.9583\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1115.2913 - val_loss: 1118.9109\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1115.2443 - val_loss: 1118.8638\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1115.1973 - val_loss: 1118.8168\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1115.1504 - val_loss: 1118.7698\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 1115.1033 - val_loss: 1118.7230\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1115.0563 - val_loss: 1118.6758\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1115.0092 - val_loss: 1118.6287\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 1114.9622 - val_loss: 1118.5814\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1114.9150 - val_loss: 1118.5344\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1114.8680 - val_loss: 1118.4875\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1114.8208 - val_loss: 1118.4403\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1114.7739 - val_loss: 1118.3932\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 1114.7268 - val_loss: 1118.3462\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "In calc_results: 3013, 1005, 1004, sum = 5022\n",
      "In split_to_train_test: dataset_X.shape=(70121, 11, 65), dataset_y.shape=(70121, 65)\n",
      "Epoch 1/30\n",
      "658/658 [==============================] - 22s 33ms/step - loss: 771.5203 - val_loss: 769.7983\n",
      "Epoch 2/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 771.0293 - val_loss: 769.3052\n",
      "Epoch 3/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 770.5396 - val_loss: 768.8122\n",
      "Epoch 4/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 770.0496 - val_loss: 768.3188\n",
      "Epoch 5/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 769.5597 - val_loss: 767.8258\n",
      "Epoch 6/30\n",
      "658/658 [==============================] - 20s 30ms/step - loss: 769.0478 - val_loss: 767.2355\n",
      "Epoch 7/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 768.4307 - val_loss: 766.6406\n",
      "Epoch 8/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 767.8560 - val_loss: 766.0703\n",
      "Epoch 9/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 767.2943 - val_loss: 765.5071\n",
      "Epoch 10/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 766.7379 - val_loss: 764.9471\n",
      "Epoch 11/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 766.1843 - val_loss: 764.3887\n",
      "Epoch 12/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 765.6330 - val_loss: 763.8325\n",
      "Epoch 13/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 765.0836 - val_loss: 763.2780\n",
      "Epoch 14/30\n",
      "658/658 [==============================] - 20s 30ms/step - loss: 764.5376 - val_loss: 762.7281\n",
      "Epoch 15/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 763.9940 - val_loss: 762.1823\n",
      "Epoch 16/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 763.4566 - val_loss: 761.6392\n",
      "Epoch 17/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 762.9217 - val_loss: 761.0980\n",
      "Epoch 18/30\n",
      "658/658 [==============================] - 20s 31ms/step - loss: 762.3891 - val_loss: 760.5611\n",
      "Epoch 19/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 761.8596 - val_loss: 760.0266\n",
      "Epoch 20/30\n",
      "658/658 [==============================] - 20s 30ms/step - loss: 761.3298 - val_loss: 759.4929\n",
      "Epoch 21/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 760.8014 - val_loss: 758.9603\n",
      "Epoch 22/30\n",
      "658/658 [==============================] - 20s 30ms/step - loss: 760.2728 - val_loss: 758.4285\n",
      "Epoch 23/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 759.7457 - val_loss: 757.8982\n",
      "Epoch 24/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 759.2202 - val_loss: 757.3687\n",
      "Epoch 25/30\n",
      "658/658 [==============================] - 22s 33ms/step - loss: 758.6947 - val_loss: 756.8401\n",
      "Epoch 26/30\n",
      "658/658 [==============================] - 22s 33ms/step - loss: 758.1697 - val_loss: 756.3122\n",
      "Epoch 27/30\n",
      "658/658 [==============================] - 21s 31ms/step - loss: 757.6457 - val_loss: 755.7855\n",
      "Epoch 28/30\n",
      "658/658 [==============================] - 21s 32ms/step - loss: 757.1212 - val_loss: 755.2603\n",
      "Epoch 29/30\n",
      "658/658 [==============================] - 22s 33ms/step - loss: 756.5985 - val_loss: 754.7361\n",
      "Epoch 30/30\n",
      "658/658 [==============================] - 22s 34ms/step - loss: 756.0773 - val_loss: 754.2134\n",
      "439/439 [==============================] - 5s 11ms/step\n",
      "In calc_results: 42073, 14024, 14024, sum = 70121\n",
      "In split_to_train_test: dataset_X.shape=(13022, 11, 65), dataset_y.shape=(13022, 65)\n",
      "Epoch 1/30\n",
      "123/123 [==============================] - 4s 34ms/step - loss: 1051.0287 - val_loss: 1079.0861\n",
      "Epoch 2/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1050.8276 - val_loss: 1078.8851\n",
      "Epoch 3/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1050.6261 - val_loss: 1078.6844\n",
      "Epoch 4/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1050.4249 - val_loss: 1078.4835\n",
      "Epoch 5/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1050.2236 - val_loss: 1078.2826\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 4s 31ms/step - loss: 1050.0223 - val_loss: 1078.0818\n",
      "Epoch 7/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1049.8210 - val_loss: 1077.8811\n",
      "Epoch 8/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1049.6199 - val_loss: 1077.6802\n",
      "Epoch 9/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1049.4188 - val_loss: 1077.4796\n",
      "Epoch 10/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1049.2173 - val_loss: 1077.2789\n",
      "Epoch 11/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1049.0161 - val_loss: 1077.0784\n",
      "Epoch 12/30\n",
      "123/123 [==============================] - 4s 32ms/step - loss: 1048.8146 - val_loss: 1076.8777\n",
      "Epoch 13/30\n",
      "123/123 [==============================] - 4s 34ms/step - loss: 1048.6138 - val_loss: 1076.6770\n",
      "Epoch 14/30\n",
      "123/123 [==============================] - 4s 35ms/step - loss: 1048.4125 - val_loss: 1076.4764\n",
      "Epoch 15/30\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 1048.2113 - val_loss: 1076.2760\n",
      "Epoch 16/30\n",
      "123/123 [==============================] - 4s 35ms/step - loss: 1048.0099 - val_loss: 1076.0757\n",
      "Epoch 17/30\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 1047.8092 - val_loss: 1075.8750\n",
      "Epoch 18/30\n",
      "123/123 [==============================] - 4s 32ms/step - loss: 1047.6077 - val_loss: 1075.6746\n",
      "Epoch 19/30\n",
      "123/123 [==============================] - 4s 32ms/step - loss: 1047.4065 - val_loss: 1075.4744\n",
      "Epoch 20/30\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 1047.2052 - val_loss: 1075.2740\n",
      "Epoch 21/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1047.0040 - val_loss: 1075.0732\n",
      "Epoch 22/30\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 1046.8030 - val_loss: 1074.8730\n",
      "Epoch 23/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1046.6017 - val_loss: 1074.6727\n",
      "Epoch 24/30\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 1046.4005 - val_loss: 1074.4724\n",
      "Epoch 25/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1046.1993 - val_loss: 1074.2719\n",
      "Epoch 26/30\n",
      "123/123 [==============================] - 4s 32ms/step - loss: 1045.9983 - val_loss: 1074.0720\n",
      "Epoch 27/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1045.7972 - val_loss: 1073.8716\n",
      "Epoch 28/30\n",
      "123/123 [==============================] - 4s 32ms/step - loss: 1045.5959 - val_loss: 1073.6716\n",
      "Epoch 29/30\n",
      "123/123 [==============================] - 4s 31ms/step - loss: 1045.3948 - val_loss: 1073.4714\n",
      "Epoch 30/30\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 1045.1937 - val_loss: 1073.2712\n",
      "82/82 [==============================] - 1s 12ms/step\n",
      "In calc_results: 7813, 2605, 2604, sum = 13022\n",
      "In split_to_train_test: dataset_X.shape=(67326, 11, 65), dataset_y.shape=(67326, 65)\n",
      "Epoch 1/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 687.8627 - val_loss: 677.4518\n",
      "Epoch 2/30\n",
      "632/632 [==============================] - 22s 34ms/step - loss: 687.0890 - val_loss: 676.6804\n",
      "Epoch 3/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 686.3248 - val_loss: 675.9105\n",
      "Epoch 4/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 685.5626 - val_loss: 675.1422\n",
      "Epoch 5/30\n",
      "632/632 [==============================] - 20s 31ms/step - loss: 684.8007 - val_loss: 674.3740\n",
      "Epoch 6/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 684.0396 - val_loss: 673.6071\n",
      "Epoch 7/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 683.2786 - val_loss: 672.8400\n",
      "Epoch 8/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 682.5183 - val_loss: 672.0739\n",
      "Epoch 9/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 681.7579 - val_loss: 671.3083\n",
      "Epoch 10/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 680.9980 - val_loss: 670.5439\n",
      "Epoch 11/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 680.2399 - val_loss: 669.7805\n",
      "Epoch 12/30\n",
      "632/632 [==============================] - 21s 32ms/step - loss: 679.4828 - val_loss: 669.0190\n",
      "Epoch 13/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 678.7260 - val_loss: 668.2593\n",
      "Epoch 14/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 677.9720 - val_loss: 667.5035\n",
      "Epoch 15/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 677.2244 - val_loss: 666.7548\n",
      "Epoch 16/30\n",
      "632/632 [==============================] - 20s 31ms/step - loss: 676.4802 - val_loss: 666.0083\n",
      "Epoch 17/30\n",
      "632/632 [==============================] - 20s 31ms/step - loss: 675.7379 - val_loss: 665.2634\n",
      "Epoch 18/30\n",
      "632/632 [==============================] - 19s 31ms/step - loss: 674.9963 - val_loss: 664.5210\n",
      "Epoch 19/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 674.2558 - val_loss: 663.7807\n",
      "Epoch 20/30\n",
      "632/632 [==============================] - 19s 31ms/step - loss: 673.5185 - val_loss: 663.0444\n",
      "Epoch 21/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 672.7857 - val_loss: 662.3141\n",
      "Epoch 22/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 672.0546 - val_loss: 661.5854\n",
      "Epoch 23/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 671.3260 - val_loss: 660.8580\n",
      "Epoch 24/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 670.5983 - val_loss: 660.1329\n",
      "Epoch 25/30\n",
      "632/632 [==============================] - 19s 31ms/step - loss: 669.8722 - val_loss: 659.4089\n",
      "Epoch 26/30\n",
      "632/632 [==============================] - 20s 31ms/step - loss: 669.1480 - val_loss: 658.6874\n",
      "Epoch 27/30\n",
      "632/632 [==============================] - 19s 30ms/step - loss: 668.4251 - val_loss: 657.9674\n",
      "Epoch 28/30\n",
      "632/632 [==============================] - 20s 31ms/step - loss: 667.7051 - val_loss: 657.2495\n",
      "Epoch 29/30\n",
      "632/632 [==============================] - 21s 33ms/step - loss: 666.9865 - val_loss: 656.5333\n",
      "Epoch 30/30\n",
      "632/632 [==============================] - 20s 32ms/step - loss: 666.2693 - val_loss: 655.8181\n",
      "421/421 [==============================] - 5s 11ms/step\n",
      "In calc_results: 40396, 13465, 13465, sum = 67326\n",
      "In split_to_train_test: dataset_X.shape=(17133, 11, 65), dataset_y.shape=(17133, 65)\n",
      "Epoch 1/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1007.3709 - val_loss: 1023.3930\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 5s 30ms/step - loss: 1007.1921 - val_loss: 1023.2146\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1007.0137 - val_loss: 1023.0361\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1006.8359 - val_loss: 1022.8580\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1006.6574 - val_loss: 1022.6796\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1006.4791 - val_loss: 1022.5013\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1006.3011 - val_loss: 1022.3232\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1006.1227 - val_loss: 1022.1449\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1005.9445 - val_loss: 1021.9665\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1005.7662 - val_loss: 1021.7880\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 5s 33ms/step - loss: 1005.5880 - val_loss: 1021.6098\n",
      "Epoch 12/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1005.4099 - val_loss: 1021.4316\n",
      "Epoch 13/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1005.2319 - val_loss: 1021.2534\n",
      "Epoch 14/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1005.0540 - val_loss: 1021.0750\n",
      "Epoch 15/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1004.8757 - val_loss: 1020.8967\n",
      "Epoch 16/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1004.6972 - val_loss: 1020.7183\n",
      "Epoch 17/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1004.5195 - val_loss: 1020.5402\n",
      "Epoch 18/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1004.3411 - val_loss: 1020.3619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1004.1632 - val_loss: 1020.1837\n",
      "Epoch 20/30\n",
      "161/161 [==============================] - 5s 33ms/step - loss: 1003.9850 - val_loss: 1020.0052\n",
      "Epoch 21/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1003.8070 - val_loss: 1019.8271\n",
      "Epoch 22/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1003.6284 - val_loss: 1019.6488\n",
      "Epoch 23/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1003.4507 - val_loss: 1019.4702\n",
      "Epoch 24/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1003.2724 - val_loss: 1019.2922\n",
      "Epoch 25/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1003.0945 - val_loss: 1019.1139\n",
      "Epoch 26/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1002.9161 - val_loss: 1018.9354\n",
      "Epoch 27/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1002.7382 - val_loss: 1018.7570\n",
      "Epoch 28/30\n",
      "161/161 [==============================] - 5s 32ms/step - loss: 1002.5598 - val_loss: 1018.5790\n",
      "Epoch 29/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1002.3817 - val_loss: 1018.4007\n",
      "Epoch 30/30\n",
      "161/161 [==============================] - 5s 31ms/step - loss: 1002.2040 - val_loss: 1018.2224\n",
      "108/108 [==============================] - 1s 11ms/step\n",
      "In calc_results: 10280, 3426, 3427, sum = 17133\n",
      "In split_to_train_test: dataset_X.shape=(94149, 11, 65), dataset_y.shape=(94149, 65)\n",
      "Epoch 1/30\n",
      "883/883 [==============================] - 30s 34ms/step - loss: 911.8430 - val_loss: 912.0358\n",
      "Epoch 2/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 910.4939 - val_loss: 910.6675\n",
      "Epoch 3/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 909.1476 - val_loss: 909.2991\n",
      "Epoch 4/30\n",
      "883/883 [==============================] - 29s 33ms/step - loss: 907.8022 - val_loss: 907.9323\n",
      "Epoch 5/30\n",
      "883/883 [==============================] - 29s 33ms/step - loss: 906.4580 - val_loss: 906.5671\n",
      "Epoch 6/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 905.1154 - val_loss: 905.2042\n",
      "Epoch 7/30\n",
      "883/883 [==============================] - 29s 33ms/step - loss: 903.7753 - val_loss: 903.8456\n",
      "Epoch 8/30\n",
      "883/883 [==============================] - 29s 32ms/step - loss: 902.4394 - val_loss: 902.5013\n",
      "Epoch 9/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 901.1203 - val_loss: 901.1732\n",
      "Epoch 10/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 899.8082 - val_loss: 899.8489\n",
      "Epoch 11/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 898.4976 - val_loss: 898.5292\n",
      "Epoch 12/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 897.1896 - val_loss: 897.2173\n",
      "Epoch 13/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 895.8931 - val_loss: 895.9205\n",
      "Epoch 14/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 894.6033 - val_loss: 894.6312\n",
      "Epoch 15/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 893.3201 - val_loss: 893.3487\n",
      "Epoch 16/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 892.0393 - val_loss: 892.0732\n",
      "Epoch 17/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 890.7652 - val_loss: 890.8052\n",
      "Epoch 18/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 889.4952 - val_loss: 889.5430\n",
      "Epoch 19/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 888.2285 - val_loss: 888.2877\n",
      "Epoch 20/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 886.9658 - val_loss: 887.0375\n",
      "Epoch 21/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 885.7074 - val_loss: 885.7923\n",
      "Epoch 22/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 884.4531 - val_loss: 884.5532\n",
      "Epoch 23/30\n",
      "883/883 [==============================] - 30s 34ms/step - loss: 883.2051 - val_loss: 883.3236\n",
      "Epoch 24/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 881.9706 - val_loss: 882.1085\n",
      "Epoch 25/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 880.7443 - val_loss: 880.8961\n",
      "Epoch 26/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 879.5236 - val_loss: 879.6867\n",
      "Epoch 27/30\n",
      "883/883 [==============================] - 27s 31ms/step - loss: 878.3038 - val_loss: 878.4813\n",
      "Epoch 28/30\n",
      "883/883 [==============================] - 28s 31ms/step - loss: 877.0882 - val_loss: 877.2820\n",
      "Epoch 29/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 875.8776 - val_loss: 876.0901\n",
      "Epoch 30/30\n",
      "883/883 [==============================] - 28s 32ms/step - loss: 874.6719 - val_loss: 874.9079\n",
      "589/589 [==============================] - 7s 11ms/step\n",
      "In calc_results: 56489, 18830, 18830, sum = 94149\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"N_clusters\":Ns_clusters, \"window_size_for_clustering\":window_sizes_for_clustering, \"dif\":True}\n",
    "models, model_mase, results_training = Forecasting.try_parameters(parameters, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [<keras.engine.sequential.Sequential at 0x7f382bcc3850>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f382d6e3490>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f37cac32c10>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f37e4d4ec10>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f37cd2d2910>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f382737d4f0>,\n",
       "  <keras.engine.sequential.Sequential at 0x7f37cd8b2070>],\n",
       " 'scalers': [<Forecasting.MyStandardScaler at 0x7f38331d3040>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37ca986910>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37e2ba9670>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37cd0b3d60>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37e4a0c130>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37ca663a30>,\n",
       "  <Forecasting.MyStandardScaler at 0x7f37ca24f160>],\n",
       " 'clusters_model': KMeans(init='random', max_iter=100, n_clusters=7)}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62528581074936.414"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'Forecasting.MyStandardScaler'>: it's not the same object as Forecasting.MyStandardScaler",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [162], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalers\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscalers/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 6\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscalers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'Forecasting.MyStandardScaler'>: it's not the same object as Forecasting.MyStandardScaler"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i, file_name in enumerate(list(os.listdir(\"scalers\"))):\n",
    "    os.unlink(\"scalers/\"+file_name)\n",
    "for i in range(len(models['scalers'])):\n",
    "    with open(\"scalers/\"+str(i)+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(models['scalers'][i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.unlink(\"clusters_model.pkl\")\n",
    "with open(\"clusters_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models['clusters_model'], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/0/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/1/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/2/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/3/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/4/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_16_layer_call_fn, lstm_cell_16_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/5/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_17_layer_call_fn, lstm_cell_17_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/6/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_18_layer_call_fn, lstm_cell_18_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/7/assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "for i, file_name in enumerate(list(os.listdir(\"models\"))):\n",
    "    shutil.rmtree(\"models/\"+file_name)\n",
    "for i in range(len(models['models'])):\n",
    "    models['models'][i].save(\"models/\"+str(i))\n",
    "print(len(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "N_clusters = len(os.listdir('models'))\n",
    "assert(len(os.listdir('scalers')) == N_clusters)\n",
    "models = {'models':[], 'clusters_model':[], 'scalers':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "for file_name in os.listdir('models'):\n",
    "    models['models'].append(keras.models.load_model('models/'+file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir('scalers'):\n",
    "    with open('scalers/'+file_name, 'rb') as f:\n",
    "        models['scalers'].append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clusters_model.pkl', 'rb') as f:\n",
    "    models['clusters_model'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_model = models[\"clusters_model\"]\n",
    "forecasting_models = models['models']\n",
    "scalers = models['scalers']\n",
    "assert(len(forecasting_models) == len(scalers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/grinenko/anna/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 10, 15]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_sizes_for_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.shape=(81619, 65), dataset_windows.shape=(81605, 975), cluster_nums.shape=(81605,), 14\n",
      "After pad: dataset.shape=(81619, 65), cluster_nums.shape=(81619,)\n"
     ]
    }
   ],
   "source": [
    "window_size_for_clustering = clusters_model.cluster_centers_.shape[-1] // dataset_test.shape[-1]\n",
    "y_pred, results_testing = Forecasting.predict_through_clusters(dataset_test, clusters_model, forecasting_models, scalers, window_size_clustering=window_size_for_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 7)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#лучшие параметры\n",
    "window_size_for_clustering, len(forecasting_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81609, 65), 65)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, dataset.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape=(81609, 65), dataset_test.shape=(81619, 65)\n",
      "(81609, 65) (81609, 65)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{y_pred.shape=}, {dataset_test.shape=}\")\n",
    "y_true = dataset_test[-y_pred.shape[0]:]\n",
    "print(y_true.shape, results_testing[:, :dataset_train.shape[-1]].shape)\n",
    "results_testing[:, :dataset_train.shape[-1]] = y_true\n",
    "cur_mase = Forecasting.my_mase(y_true, y_pred, multioutput='raw_values')\n",
    "cur_mae = Forecasting.my_mae(y_true, y_pred, multioutput=\"raw_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = dataset_train.shape[-1]\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "full_results = np.concatenate((results_training, results_testing), axis=0)\n",
    "with open(\"output_table4.csv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow([\"real \"+str(i) for i in range(Q)] + [\"predicted \" + str(i) for i in range(Q)] + [\"cluster_num\", \"mode\"])\n",
    "    for i in range(full_results.shape[0]):\n",
    "        writer.writerow(full_results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_mase, cur_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = cur_mase <= np.percentile(cur_mase, 40)\n",
    "plt.plot(np.arange(cur_mase.shape[0])[mask], cur_mase[mask])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"MASE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = cur_mae <= np.percentile(cur_mae, 40)\n",
    "# plt.plot(np.arange(cur_mae.shape[0])[mask], cur_mae[mask])\n",
    "plt.plot(cur_mae[mask])\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"best MAE\")\n",
    "# plt.show()\n",
    "plt.savefig(\"best MAE.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAHFCAYAAAD7ZFORAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACV8UlEQVR4nO29eZgcZbn+f1fvs++ZJeskJCEhZCEJSyDsoEFQAQU5IiCiIkE2N1CPqIevgOfAD44EUFYXBPQIKDsokABhSSCBmLBknWwzmcy+9/r+/uh+36ruqeqq7q5eqvr5XNdckJmenpqe7uq77ud+nkdijDEQBEEQBEHYEEe+D4AgCIIgCCJbkNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCBvz2muvQZIk1Y+333573O3ff/99nHrqqSgvL0d1dTXOOecc7NixI+42Y2NjWLlyJRoaGjBp0iT88pe/ROKA9ba2NpSXl+Nf//pXSsf5f//3f+n/smnw3HPP4ec//7nh219yySWQJAkVFRUYGhoa9/W2tjY4HA5IkqR5v//4xz8gSRLq6urg9/tVbzM8PIxbb70VCxYsQGVlJSoqKjBjxgycd955WL16tbhdsr+vJEl4+OGHDf9uBGFXXPk+AIIgss+vfvUrnHTSSXGfmzdvXty/P/74Y5x44olYuHAh/vKXv2BsbAw/+9nPsHz5cmzcuBENDQ0AgF//+td44okncM8992BgYADf/e53MX36dFx44YXivr7zne/g3HPPxSmnnJL9Xy4DnnvuOaxatSolseN2uxEKhfD444/jG9/4RtzXHnroIVRUVGBgYEDz+x944AEAQE9PD5566imcf/75cV8Ph8M4/fTTsWnTJvzgBz/AkUceCQDYunUrnn76abz++us44YQT4r5H7e8LADNmzDD8exGEXSGhQxBFwMyZM3H00Ucnvc3PfvYzeL1ePPPMM6isrAQALF68GDNnzsT//M//4NZbbwUAPPvss7jqqqvwpS99CQDw9ttv45lnnhFC57HHHsO7776Ljz/+OIu/Uf7weDw466yz8OCDD8YJHcYYHn74YZx//vm47777VL+3o6MDzz33HE4++WSsXbsWDzzwwDihs2bNGqxduxYPPvggvv71r4vPf+Yzn8GVV16JSCQy7n6N/H0Jolih0hVBEAiFQnjmmWdw7rnnCpEDAFOnTsVJJ52EJ598UnxubGwMZWVl4t/l5eUYGxsDAPT19eGaa67B7bffjvr6+pSPY2xsDNdddx2amppQUlKCE044ARs2bBh3u/Xr1+Pzn/88amtr4fP5sGjRIvzlL3+Ju83IyAi+//3vo7W1FT6fD7W1tViyZAkeffRRANEy1KpVqwAgrtyza9cu3eO89NJLsXbtWnzyySfic//85z/R1tYWJ04S+f3vf49QKIRrr70W55xzDv71r3+hra0t7jbd3d0AgObmZtX7cDjotE0QqUCvGIIoAlauXAmXy4XKykp85jOfwRtvvBH39e3bt2N0dBTz588f973z58/Htm3bhJhZtmwZHnzwQbS1tWHz5s14/PHHsWzZMgDAD3/4Qxx22GG46KKL0jrOH//4x9ixYwfuv/9+3H///di/fz9OPPHEuJzQq6++imOPPRZ9fX2499578fe//x0LFy7E+eefH5dJue6663DPPffgqquuwgsvvIA//vGP+PKXvyyExH/+538KV+qtt94SH1oCQ8mpp56KqVOn4sEHHxSfe+CBB3D88cdj5syZmt/34IMPorm5GStWrMCll16KSCQyLkezZMkSuN1uXH311XjkkUfQ3t6uezyRSAShUGjcB0EQABhBELbl/fffZ1dffTV78skn2Zo1a9iDDz7I5syZw5xOJ3vhhRfE7d58800GgD366KPj7uNXv/oVA8D279/PGGOso6ODLV26lAFgANgZZ5zBRkZG2Jo1a1hJSQn79NNPUz7OV199lQFgRxxxBItEIuLzu3btYm63m1122WXic4ceeihbtGgRCwaDcfdx5plnsubmZhYOhxljjM2bN4998YtfTPpzV65cyVI5DV588cWsrKyMMcbYjTfeyJqamlgwGGTd3d3M6/Wyhx9+mB08eJABYDfeeGPc965Zs4YBYNdffz1jjLFIJMJaW1vZ1KlT435nxhh74IEHWHl5uXiMm5ub2UUXXcTWrFkTdzv+uGl97Nmzx/DvRhB2hRwdgrAxixYtwh133IEvfvGLWL58Ob7+9a9j7dq1aG5uxg9/+MNxt5ckSfO++NcaGxvxzjvvYOfOndi3bx+effZZOJ1OfPvb38ZPf/pTzJw5E3/7299w2GGHoba2FmeeeSb27Nlj6Hj/4z/+I+4Ypk6dimXLluHVV18FAGzbtg0ff/wxvvrVrwJAnHtxxhlnoL29XZSTjjzySDz//PO4/vrr8dprr2F0dNTYg2aQr3/96zhw4ACef/55PPLII/B4PPjyl7+seXseQr700ksBRB/PSy65BG1tbeO60y699FLs3bsXf/7zn3HVVVdh8uTJ+NOf/oQTTjgB//3f/z3uvm+99VasW7du3EdjY6OJvzFBWBMSOgRRZFRXV+PMM8/Ehx9+KN786+rqAMj5ECU9PT2QJAnV1dXic5IkYdq0aWhpaQEA3HLLLXA4HPjBD34ghMhtt92GvXv3or6+Pq4jKxlNTU2qn+PHdeDAAQDA97//fbjd7riPK664AgDQ1dUFAPjf//1f/OhHP8JTTz2Fk046CbW1tfjiF7+IrVu3GjoWPaZOnYpTTjkFDz74IB588EF85StfQWlpqeptBwcH8de//hVHHnkkGhoa0NfXh76+Ppx99tmQJEmIICVVVVW44IILcOedd+Kdd97Bhx9+iMbGRvzkJz9BX19f3G2nT5+OJUuWjPtwu92m/K4EYWWo64ogihAWm3vD3ZMZM2agpKQEmzZtGnfbTZs24ZBDDoHP51O9r08++QS33HIL/vnPf8LtduOf//wnDjvsMHz2s58FEM3KLFiwAENDQygvL096XB0dHaqf40KMB5xvuOEGnHPOOar3MXv2bABAWVkZfvGLX+AXv/iFcF6uv/56nHXWWaZ1hF166aW48MILEYlEcM8992je7tFHH8XIyAjeffdd1NTUjPv6k08+id7eXtWvcQ477DB85StfwR133IFPP/1UtJ0TBJEcEjoEUWT09vbimWeewcKFC4V4cblcOOuss/DEE0/g17/+NSoqKgAAu3fvxquvvoprr71W8/6+/e1v45JLLhGBZMYYhoeHxdf5YD2WMFRQjUcffRTXXXedEGBtbW1Yu3atCDfPnj0bM2fOxAcffIBf/epXhn/nxsZGXHLJJfjggw9wxx13YGRkBKWlpfB6vQCA0dFRlJSUGL4/ztlnn42zzz4bVVVVSdu7H3jgAVRUVOCpp54a1zW1fv16/OAHP8AjjzyCK6+8Et3d3aioqIDH4xl3P1ygcSeNIAh9SOgQhI35j//4D0yZMgVLlixBfX09tm7dittuuw0HDhwY1+3zi1/8AkuXLsWZZ56J66+/XgwMrK+vx/e+9z3V+3/wwQfx6aef4u9//7v43CmnnIJrr71WDBu88cYbceyxxwrxlIzOzk6cffbZ+OY3v4n+/n7ceOON8Pl8uOGGG8Rtfvvb32LFihX4zGc+g0suuQQTJ05ET08PPvroI7z//vv461//CgA46qijcOaZZ2L+/PmoqanBRx99hD/+8Y845phjRInp8MMPBxDNuKxYsQJOpxPz589XFRlq+Hw+3WnO//73v/Huu+/iO9/5Dk4++eRxXz/22GNx22234YEHHsCVV16JV199FVdffTW++tWvYtmyZairq0NnZyceffRRvPDCC7joooswadKkuPvYunWr6qTrSZMmjbstQRQdeQ5DEwSRRW6++Wa2cOFCVlVVxZxOJ2toaGBnn302e/fdd1Vvv379enbKKaew0tJSVllZyb74xS+ybdu2qd62s7OT1dbWsr/+9a/jvvbII4+wmTNnsvLycnbaaaexHTt2JD1O3j30xz/+kV111VWsoaGBeb1etnz5crZ+/fpxt//ggw/YeeedxyZMmMDcbjdrampiJ598Mrv33nvFba6//nq2ZMkSVlNTw7xeL5s+fTq79tprWVdXl7iN3+9nl112GWtoaGCSJDEAbOfOnZrHqey60iKx6+qaa65hANjGjRs1v+f6669nANh7773H9uzZw37605+yY489ljU1NTGXy8UqKirYUUcdxX7zm9+wUCg07nHT+vjJT36S9FgJohiQGDPgJxMEQRAEQVgQ6roiCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2kNAhCIIgCMK2FPXAwEgkgv3796OioiLpMkOCIAiCIAoHxhgGBwfR0tIybtp4IkUtdPbv34/Jkyfn+zAIgiAIgkiDPXv26E7/Lmqhw0fS79mzB5WVlXk+GoIgCIIgjDAwMIDJkycbWi1T1EKHl6sqKytJ6BAEQRCExTASO6EwMkEQBEEQtqUohc6qVaswd+5cLF26NN+HQhAEQRBEFinqpZ4DAwOoqqpCf38/la4IgiAIwiKk8v5dlI4OQRAEQRDFAQkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwkdgiAIgiBsCwmdLDE4FsT6XT35PgyCIAiCKGpI6GSBTzoGsezmV3DZH9ZjJBDK9+EQBEEQRNFCQicLHDKhHDVlHvSNBPG39/bm+3AIgiAIomghoZMFnA4Jlx47DQDwwBs7EY4U7d5UgiAIgsgrJHSyxJeXTEalz4Vd3SN4ecuBfB8OQRAEQRQlJHSyRJnXhQuPngoAuP/1HXk+GoIgCpk9PSPwh8L5PgyCsCUkdLLIxcumwe2UsL6tFxt29+b7cAiCKEA+ah/A8l+/iu//9cN8HwpB2BISOlmksdKHzy+YCAC4//WdeT4agiAKkV1dwwCA7Z1DeT4SgrAnJHSyzGXLWwEAz/+7HXt6RvJ8NARBFBqBcAQAMBak0hVBZIOiFDqrVq3C3LlzsXTp0qz/rDnNlVg+sx4RFu3AIgiCUBIMR7syRwIkdAgiGxSl0Fm5ciW2bNmCdevW5eTnfXP5dADAX9bvQf9IMCc/kyAIaxCMOTqj5OgQRFYoSqGTa5bPrMehTRUYCYTx53d35/twCIIoIAKhmNAhR4cgsgIJnRwgSRK+cVw0q/Pw2p3ixEYQBMEdnUA4glCYzg0EYTYkdHLE5xe2oKHCiwMDfjzz4f58Hw5BEAVCQCFuqHxFEOZDQidHeF1OXLJsGgDgvtd3gjFaC0EQBBAMyecCEjoEYT4kdHLIV4+aghK3Ex+1D+DNbd35PhyCIAqAoNLRoZwOQZgOCZ0cUl3qwXlLJgEA7qO1EARBIEHokKNDEKZDQifHXHpcKyQJWP3pQXQOjuX7cAiCyDPKjA7N0iEI8yGhk2Om1pVhRkM5AGDL/oE8Hw1BEPlG6eiMkdAhCNMhoZMHDm2qAAB81D6Y5yMhCCLfKMPI5OgQhPmQ0MkDc5orAQAfd5CjY1Woa44wC2ovJ4jsQkInD8xpjjo6H5OjY0l6hgM47tZXcesLH+f7UAgbEKCuK4LIKiR08sChTVFHZ/vBIfhDdGKzGh/u7cO+vlG8uLkj34dC2IBgSBlGDuXxSAjCnpDQyQPNVT5UlbgRijBs6xzK9+EQKcKvuv1BGtdPZE58ezk9pwjCbEjo5AFJkiiQbGF4jmKM8hSECQTDisnI5OgQhOmQ0MkTIpDcToFkq8E7Y0joEGZAYWSCyC4kdPIEDyR/RJ1XloOXrsZoCz1hAkEaGEgQWYWETp7gjs5H7YPUqmwx+FV3OMLi3qQIIh1oBQRBZBcSOnliVmMFHFK0VfngoD/fh0OkgPKqm8pXRKbEbS8nR4cgTIeETp7wuZ1orS8DAHzUQYFkK6EMjI5RlwyRIZTRIYjsQkInjxxKgWRLonwzIkeHyJRAiDI6BJFNSOjkkbkip0NCx0pQ6Yowk7ilnvR8IgjTIaGTR/gsnY+pdGUpRuOEDpWuiMygriuCyC4kdPII77za1kmrIKxEXOmK/m5EhsQPDKTnE0GYDQmdPNJc5UOlz4VQhGF753C+D4cwCJWuCDOhMDJBZBcSOnlEkiQRSKacjnUYC1LpijAHxuJnMZGjQxDmQ0Inz/BA8sc0IdkykKNDmEU4wqCcFzoaDCMSoQGiBGEmJHTyDC33tB5KoUOlBiITAiqTtf20WoQgTKUohc6qVaswd+5cLF26NN+HIi/3JEfHMihdHD8JHSIDlFOROSO0wZwgTKUohc7KlSuxZcsWrFu3Lt+HIlZBdA3RKggrwBiLeyOijA6RCUpHx+uKno7JJSQIcylKoVNIlHicmMZXQVAgueDxhyJQRigoo0NkAg8iu50SSj1OABRIJgizIaFTAMxpovKVVUgUNjRHh8gELnQ8TgdKPS4ANDSQIMyGhE4BMKeZAslWIfFNiEpXRCYIR8flQAl3dMglJAhTIaFTABzaRLN0rELimxCVrohMCMTCyG6nAyVuKl0RRDYgoVMAzGmJCp3tB4fiNhkThUfimxBdfROZoCxdkaNDENmBhE4B0BJbBREMM2w/OJTvwyGSkFi68lPpisgAZRiZOzqU0SEIcyGhUwDQKgjrQKUrwky4g+t2OuSuK3pOEYSpkNApEObEJiR/3EGB5EJmNGGYG3VdEZkQCMtCR87o0MBAgjATEjoFwhxydCwBdV0RZhIMx8LIyq6rAD2nCMJMSOgUCHLpihydQoaXFfgUWypdEZnAMzpehaMzEiRHhyDMhIROgTCrsRySBHQN+WkVRAHDu65qyzwASOgQmSHP0ZEnI49RGJkgTIWEToFQ6nGhtS66CoImJBcuvHRVU8qFDpUZiPRRhpF9Huq6IohsQEKngDg0NiH5YypfFSy8dEWODmEGIqPjdKDUTV1XBJENSOgUEHNoQnLBw0tX1aVuACR0iMxQHRhIjg5BmAoJnQJCBJKpxbxgGZfRoUnWRAbIpSsJJbGlnuToEIS5uPJ9AIQMX+65rXMQNz2zBVUlblSVuqP/jX1Mry9HVcxNIHLPSDA+oxOOMATDEbiddM1ApI7aHB3K6BCEuZDQKSAmVpegvtyLriE/7n9jp+ptyjxOvHn9yaiOvdESuYUPc6tRiM2xYJiEDpEWyu3louuKHB2CMBUSOgWEJEl4+OtLsfrTgxgYDaJ/NIi+keh/+0eD+PTAIIYDYezuGSGhkyd4WUH5+I8FI6jw5euICCujzOj4yNEhiKxAQqfAmDexCvMmVql+7YT/fhVt3SO04TyP8DehUo8TPrcDY8EIXYETacO7rjwu2nVFENmC/HYL4YmVR3hdn8g9o0LouMQVOAkdIl3iwshu6roiiGxAQsdCeGJrB8jRyR/8arvE44DPxYUO/T2I9AiG1beXM8byeVgEYStI6FgIHnjldjeRe3jpqsTtgs8d23dFG8yJNFEKHT4ZORxh5NoShImQ0LEQ5OjkH76HqMTjpNIVkTEio6NoLweofEUQZkJCx0J4hKNDQicfMMbEHJ1SjxNeN5WuiMxQZnTcTgfcTgkABZIJwkxI6FgIcnTySyAcQTgSvQIv8Tjhi/09yNEh0iWgmKMDgIYGEkQWIKFjIfjVHtXv88NYQH7cS9xOsZuIhA6RLsqMDgDad0UQWYCEjoXwxLp8yNHJDyPB6FRkXmaQu67oTYlIDy50vDFHp5T2XRGE6ZDQsRDc0aGMTn6QO66iAkd0XVFGh0iTYChaCuWOjo9m6RCE6ZDQsRBeyujklVFFxxUA6roiMiaQULris3Qoo0MQ5kFCx0K4qesqr4wG5anIgELo0BwdIk3kjE7UrS0h8UwQpkNCx0Lw9nI/CZ28kFi68lLpisgQ5VJPQHYLydEhCPMgoWMheAsqr+sTuWVc6YrCyESGiDk6rvjSFYWRCcI8SOhYCHmpJ50E88ForOuKvxnJ7eXk6BDpwScji/ZyEUYO5e2YCMJukNCxEDQwML+Mxubo8GwODQwkMiWQmNEhR4cgTIeEjoXw0FLPvDISiHd0qOuKyJTEOTo0GZkgzIeEjoUgRye/8IzOOKFDXVdEmgRD6u3lJJ4JwjxI6FgIt8jokNDJB7yc4KOBgYRJJGZ0fOToEITpkNCxEOTo5JeRBEfHS6UrIgMYYyoDA2MrIEjoEIRpkNCxELQCIr/IpavYwEBqLycyIBSRs3byHJ3ofymMTBDmQULHQtAKiPySWLqi9nIiE5QXLG4Xn4xMjg5BmA0JHQtBKyDyS2LpSs7o0JsSkTrKCxY3TUYmiKxBQsdC8IyOnxydvJA4MJBKV0QmKJsKXI6oo0OTkQnCfCwvdAYHB7F06VIsXLgQhx9+OO677758H1LWIEcnv/Bygtx1xdvL6e9BpA7vuPK4HJCk+KWeVLoiCPNw5fsAMqW0tBSrV69GaWkpRkZGMG/ePJxzzjmoq6vL96GZjui6IqGTF7RKV+EIQzAcEUKUIIzAZ+h4FM8buXRFKyAIwiwsf2Z2Op0oLS0FAIyNjSEcDoMxe04OFpORaalnXuAlqpIER0f5NYIwSjBh/QMgP7co4E4Q5pF3obNmzRqcddZZaGlpgSRJeOqpp8bd5u6770Zrayt8Ph8WL16M119/Pe7rfX19WLBgASZNmoQf/vCHqK+vz9HR5xZydPLLSML2ct4FB9AbEwF0Dfnx3Uc34K3t3YZunzhDB5DdwkA4ghC9zgnCFPIudIaHh7FgwQLcddddql9//PHHcc011+AnP/kJNmzYgOXLl2PFihXYvXu3uE11dTU++OAD7Ny5E3/+859x4MCBXB1+ThGTkSkTkhcS5+hIkkSdV4Tgxc0dePqD/XjozZ2Gbp84FRmIdwkpkEwQ5pB3obNixQrcdNNNOOecc1S/fvvtt+Mb3/gGLrvsMsyZMwd33HEHJk+ejHvuuWfcbRsbGzF//nysWbNG9b78fj8GBgbiPqwEOTr5ZTShdAXQYk9CZmA0mqsxKlB46cqjcAa9LgdiDVgUSCYIk8i70ElGIBDAe++9h9NPPz3u86effjrWrl0LADhw4IAQLAMDA1izZg1mz56ten8333wzqqqqxMfkyZOz+wuYjEfh6Ng1h1SoBEIRMcmWl64AZYs5ic9iZ9gfFTpGRa+80FPO6EiSJHdekXgmCFMoaKHT1dWFcDiMxsbGuM83Njaio6MDALB3714cf/zxWLBgAY477jhceeWVmD9/vur93XDDDejv7xcfe/bsyfrvYCbK7gzl+Hgi+yivrkuVQoeXrmiDedEzFBM6Rudc+VUyOgBQEiuN0tBAgjAHS7SX8xkTHMaY+NzixYuxceNGQ/fj9Xrh9XrNPrycobS4AyFqZ84l/Ora5ZBUMxVUuiK4o+M36O6J9nJXotChfVcEYSYF/U5ZX18Pp9Mp3BtOZ2fnOJenGFBa3DQ0MLfwuSbKshWg3GBOf49iZzj2HDHq7qmFkQGglPZdEYSpFLTQ8Xg8WLx4MV5++eW4z7/88stYtmxZno4qf7icclCROq9yC7+6Lk0QOj4XdV0RUYb80eeAYUcnPH5gIAD4PDQdmSDMJO+lq6GhIWzbtk38e+fOndi4cSNqa2sxZcoUXHfddfja176GJUuW4JhjjsHvfvc77N69G5dffnkejzp/uJ0O+EMR6rzKMfxNR9lxBSg3mNObUrEzNBYEAPgNOjoBlYGBAFAae46N0HOKIEwh70Jn/fr1OOmkk8S/r7vuOgDAxRdfjIcffhjnn38+uru78ctf/hLt7e2YN28ennvuOUydOjVfh5xXPK6Y0CFHJ6fIwwLjXzK02JPgDMccHaNlzKBmGDn2nCJHhyBMIe9C58QTT9Rtlb7iiitwxRVXmPYzV61ahVWrViEctt6JRKyBCFPXVS6RZ+gklBnEwEASnsWO3HUVjmuY0EK0l48LI9O+K4Iwk4LO6GSLlStXYsuWLVi3bl2+DyVlxNBAcnRySuJUZA51XREcHkaOMGMXImJ7eaKjI+bo0GucIMygKIWOlRFrICijk1MS91xxhNChOTpFD28vB4zldDQzOiKMTI4OQZgBCR2LQY5OflBb/wAAXipdEYgKG6WLY2RoYCCkkdGhycgEYSokdCyGW2R06I01l/Cr6/Ht5VS6IoChsXj3xcjzQW3XFaDM6NBziiDMgISOxSBHJz9ola7k9nL6exQzvOOKY8TR0ZqjQ44OQZgLCR2L4YnV8ymjk1u0Slc0MJAA5I4rjpGhgZqTkWlgIEGYCgkdi8EdHSpd5Ra560ojjExCp6gZTggOGwmnB2ipJ0HkhKIUOqtWrcLcuXOxdOnSfB9KynCb2+iGZMIchKOj1V5OXVdFTVqOjpijE991RaUrgjCXohQ6Vp6jQ2Hk/DCisQKCBgYSQHxrOWBM+GpldKh0RRDmUpRCx8pQGDk/aJWuvFS6IjC+6yqTjI6PHB2CMBUSOhbDQ45OXuBvOr5xYWQSOoRK6cqAo+PXmKNDjg5BmAsJHYtBjk5+GNFwdKi9nABU2ssNOTrJ5+iQo0MQ5kBCx2LIKyBoqWcu0RwY6ObhcHpTKmYSu66MPB+CGisgeA6MlnoShDmQ0LEYZjs6n3QM4oE3dlIpTAe90hWVGYqbxNKVEYdPc2CgwiWMROiChiAyxaV/E6KQMLvr6qZnt+D1rV1orS/FyYc2mnKfdkSrdCW3l5NQLGYSu66MLfVMPjAQiHZvlXroNE0QmUCOjsUw29E5OOiP+y+hDg8bJ77p8NJVOMLIFStiuNDxuozPuZLn6CR0XblkoUNOIUFkDgkdi8FXQJj1psqzBYNjlAfQIhiOiFbg8XN0FFffFB4tWvjrp77cCyC1pZ6JGR2HQxICmqYjE0TmFKXQsfRkZJMdHd4tQkJHG+WbTeJST6/iapw6r4oXfsFQW+YBkNlST0AW1CSeCSJzilLo2GEyst8kR4eHKEnoaMPfbFwOaVwrsCRJiunI9KZUrPALBiF0DIjegMYcHUAukZKjQxCZU5RCx8qIpZ4mODrBcEScbAfHghnfn13RWv/A4eUrajEvXvgFQ115VOgYW+oZLYcmimdAzn7RLB2CyBwSOhZDnqOTudBRdooktsdmSv+IfYQTn2eSWLbiyC3mVLoqVvhriWd0UhkYmMzRoTAyQWQOCR2LwTMhZoSRleLGzNLVb1dvx4JfvoR/fXTAtPvMJ2Nic7mWoxMrXZGjU5SEI0y4fnVlxh0dIxkdcnQIInNI6FgMflI0I4ysHFtvZulq3a4eAMCHe/tNu898YrR0RRmd4kQ5FTmVjI5wdFzSuK9xUU0ZHYLIHBI6FsPMFRDZcnQODPhNv898ojUskCNvMKfSVTHCy1ZOh4TKEjcA/bwWY0xzezlAjg5BmAkJHYthZnu5MqMzaGJGp2NgDAAwYJOAs27pykVdV8UMfx2VeZwKdy/56zOouFBRz+jw3Jc9LhYIIp+Q0LEYZq6AiBM6JomSUDiCriHu6KR2n73DATz4xk50DxXWlGa5dKU+ip9KV8XNUKwEXO51CdGr5+goX79qGR1fHktXjDEwRju2CPtAQsdimOnoKEtXY8GIKeKpaygAfo5MtXT1h7fa8MtntuD3a3dlfBxmMqpTuiqhfVdFDb9gKPe5RBlTb2Cg8rWWOBkZAErzVLoKRxi+sOpNXPjAOyR2CNtA2+IshidLjg4QFSY8TJkuvGzF7y8VDgxGv7d7OJDRMZgNf7PRDiPHSlcUHC1K+AVDmdcluiL1Slf8QkWSotmeREpE6Sq3z6n9faOiicAfisStOCEIq1KUjg6tgIgynHASHTIhPHwgTuikVroaGI3evtBCvbpzdKh0VdTw102512V4eGRA0VouSYUjdA4qysb89UgQVqcohY61V0BET4pmDAxMHBJoRnhYKXQGUhRO/PaFJhj4IECt0pUQOjRHpyjh7eVlHtnR0Wsv52FktXwOILuHIzl+LRwcVAgdm3RNEkRRCh0rk62uK8CcdvBERyeVOr/s6BSWYBgNxhwdDRvf6zZWriDsibJ0xUVvIBxBJKL93Jdn6KifgrmoznU5NF7okKND2AMSOhbDzIxOoqNjxhqIjn75RBkMM0NbnDn8xFpos0NE15XOCohCE2hEbhBhZK8zbpt9MtdVXug5vmwFyC5hrruulELHLnOwCIKEjsXgjk6ERVu5M2G8o5P5FVzn4Fjcv1O5Khws2NKV3goIGhhYzPAJ4+U+V5zQSfY8TrbnClDsusp16YoyOoQNIaFjMZQnxkxzOsoVEID5patU75OfWEcLTDDwNxvt9nLadVXMKEtXLqcDrlgXVTI302hGJ+dhZHJ0CBtCQsdieBRXjMFQZnMu+Amat5Sb4eh09Cc4OgavCseCYfHGULCOjt7AQGovL0qUXVcAFC3m6Ts6ousqr2FkcnQIe0BCx2K4HBJ4N6o/nNlJkJeumip9ADJfAzEaCItOjYnVJdH7NHhVqLxdoQkd3YwOdV0VNcquKwCGhgYGkiz0BGT3MNcZna4hpaNDQoewByR0LIYkSYo1EJk5OlzoNFfFhE6GVjUvW5W4nSkLHeXVY6GFkfVKVz7quipqlKUrQN59lqzFnIeR9UpXuRT9jLF4R2eUSleEPSChY0G8TnNazPkJutFkodNY6UWFzxW7T2NXhcoSV6E5OnLpSm97eWEdN5Eb+AUDf857DTh8+mFk7uiEcraKYdAfinOhyNEh7AIJHQvCZ29k0mLOGBOTkZt56SrDE9uB2NVgY6VPnPSN1vkH4kpXyWeQ5BrdycjUXl7U8FB/WUJGJ5mjw1+7Ho05OnypZ4SZMxzUCEo3B6CBgYR9IKFjQTwmODr+UAThmJhoijk6ma6AONDPHR0fKnxuAKlkdOIFUSrzd7INla6IZAwp5ugAyoxOEkcn1kigGUZWuIe56rxKFDrk6BB2gYSOBeEBxkyu9JTDARsrzS9dVZbw0pXBjE5CHqBQ3JFgOCKyUKUaXVfc6dHbb0TYD8bYuIyOkcWeIoysMTDQ7XSIr+Uqs8aFDt8xShkdwi4UpdCx8lJPwBxHh+cKSj1OVJVw9yWzK7iOgfGOjvHSVfztCiWQrDwOn0ejzODKz8wTIv8onVERRjbi6OhkdADFvqscOzqTakoBkKND2IeiFDpWXuoJQNF1lbmjU+Z1odyXmvuiRefA+IyOcUcn/qRaKI4OFy9Oh6TZISO3l1PpqthQOqOivdyAoxMMJ++6AnK/wZy3ls9oKANAGR3CPhSl0LE6XhMWe4qx9V6XECVDgVBGIWDu6DRVKRwdgwMDC9bRib3JlLqdkCStvUTRv0c4wkzZQUZYB6Uz6ozVfEQYOamjkzyjE73P3K6B4I7O9IZyAFERFy6gpgCCSBcSOhbEDEdnWDg6TlTGRAlj8vCzVGGMyRmdCh8qU3Z0EjM6hSEYeNnApxFEBmRHBygcJ4rIDYn5HEBZutJ+DvOvaXVdKe8nZ2HkmKPTWl8mPpdpgwJBFAIkdCyIR1wxmlC68kQXEfL9POmWrwZG5RkcEyq9cteV35ijk5gHKBTBoNdxBSBhkWNhCDQiNyidUY4ZKyCA3E9H5o7OxOoS4VLSGgjCDpDQsSBmTEYeFi2xLkiSJJev0lwDwctW1aVu+NzO1B2dhNsVSrBXb1ggEJ1WbeTNjbAfSmeUY8TRCYaSr4AAFIs9g7lxVbjQaajwCpeXhA5hB0joWBCPCRmdRMtdnnuT3olNWbaKvz9jk115lofHYAplb5TesEAOtZgXJ4MKZ5ST0sBAQ2Hk7LuEkQhD93AAQFTopNpMQBCFDAkdCyK3l6f/ppo4zVWeZJyZo8PXSfD7C0eYIeudXznWl3sBFJCjY6B0BShbzKl0VUwonVGO16W/AiJgIIwst5dnX2z0jgQQjjBIElBb5kFlSWrNBARRyJDQsSAelwmlq0D8NFd+ok73Cq5TODpRoaLsQjFynzyMPCH2/YXSqm2kdAUopiOTo1NUCKHjU4aRjTs6RjI6uSiH8iBybakHbqcj5cnmBFHIkNCxIMLRMWmODiCXmtLtsjigmKEDIC73o1cOC4YjwjkRQqdAHB3uRpV41Kcic3y02LMoUeu6EnktQwMDtTM6vhwODOT5HO6oVqa4q44gChkSOhZErIAwYTIyd3IqU9w2nkhi6QowXg5TXjU2xIROwczRCcpzdJIhbzAvDCeKyA1qpSsRRs5gqScgOzq5eC0og8gAyNEhbAUJHQvicUZPgJk4OsPjHB1zS1cAUOE1FnDmOYByr0scT6E4I6J0pZvRoa6rYmSIZ92UYWReukqW0dFZ6gkouq5y6OhwocN31VFGh7ADJHQsCHd0giZ2XZWb5Og0peHocHu80udStNQWhmDgx6ErdKh0VZQMqbSX8zByMkcnkErXVR4cnUpydAgbQULHgnhNyOjIg86iJ1N5wF/qJ7ZwhIkTJc/oABCdG/qOTkjc3ldgJaCRgLHSVQntuypK1EtXBlZAiDk6+kInJxmdWBi5gTI6hA0hoWNBTF0B4cm8dNU15EeEAQ4JqCvziM8bvU/Z0XHLgqFAnJFRg3N05E6bwjhuIjcMqXRdifZyQ3N0tMPIuey64gs9KaND2BESOhbE1BUQJgwM5MMCGyq8cCms+EqD98lzABU+l9ymnYOTe+fAGM679y088f5ezdukWroqlPk/RG5IzLoBRpd66reXl+Sh62pcRoccHcIGFKXQWbVqFebOnYulS5fm+1DSwuwVEABQkcEcncTWco5RR4d/XVm6ykUu4dVPOvHurh7c9/pOzduMGJ6joz8kjrAfSbuuklyIGBoYyLeX56G9nBwdwk6YKnQYY+js7DTzLrPCypUrsWXLFqxbty7fh5IW8gqI9E6AkQjDcEB9MnI6u65Ea3mC0BH7cnQ6N5Rh5Fw6I/zkvq1zULNVnx+H3mRkr3CiKKNTTKh2XZm01DNXwfxAKILekehrMDGMTF1XhB1ISeiUlpbi4MGD4t+f/exn0d7eLv7d2dmJ5uZm846OUMWToaMzojhxlo8rXaUudERreaU37vOGMzqxk2lliTunod6uoehun2CYYfvBIdXbyKUrnYGBrsLKFhG5Ycgvj0bgGFrqKebo6Gd0si36u4ejgt/lkFAdayBQvnaN7KojiEImJaEzNjYW96R/8803MTo6GncbelFkn0yXenK73SHJIVrlFONU/4Yd/bHW8nGlK2PiibefV/rcIguTi8nI3NEBgI/aB1RvY3wFRGF1ixFRsrknKhSOiL93fHu5foZOdF0lcXR8Odp1pSxbOWJrW3jHZCAcySgLSBCFgOkZHUnSvkIhzMHtzEzoKIPI/O/Fu0aCYZbyie1A7EQ5QSOjoxdolB0dV053RhkROiMGS1cltOuq4Hh83W7Mu/FFPL+pXf/GaTCsEONqXVfhCNPsjAykMBk52+I5MYgMAGUeJ2KahwLJhOUpyjCy1RGOTprt5WoBynKPC1yjplq+6tTI6KTaXl7hc+c2ozOkFDqDqrdJteuK2ssLh3d29CDCgPVtvVm5f/46cjslIW4AOa8FaLs6AQOODncRA+EIQhmMktAjsbUc4LvqeE6HAsmEtUlJ6EiSFOfYJP6byA18EaAZjg7H4ZBQ7klvOrKYipwYRo7Z33pXhIOK0lUuu64SHR21kl2qpatCmehMAN3D0QxWb+y/ZqPWWg7IpStAO7PF83VGJiMD2X1eyaUrT9znjS7lJYhCJ3nCMgHGGGbNmiXEzdDQEBYtWgSHwyG+TmQf7uikOzCQT0VOPEFX+FwY9IdScnTGgmH0xTo2tMLIQ/4QIhEm6v+JKEtXJQYWIprBSCAU12HWPRzAwUF/XPktFI4I10yvdOWjrquCoycmcHpGsiN0hhKGbnIkSYLH5UAgpJ1vMdJ15XU5IEkAY1HBzR0Ws1ErXQG882pUd4ULQRQ6KQmdhx56KFvHQaSA17TSVfybd7nPBfSn1mLeGZuh43U5UFUSfyLmLaqMAcOBkOaJOi6MrLDrwxEGp4Y4ypSuweibX4nbieZqH3YcHMZHHYNxQkd5Fa1XuvIW2ERnAuiOlWR6suToDKmUgDk+LnRUng+RCEMowufoaD+/JUlCqduJ4UA4q0MDE9c/cMjRIexCSkLn4osvztZxECkgBgZmWrryJDo6qU9HPjAo53MSy5helwNup4RgmGFwTF3ohMIRcTzKgYFAVDQkuk5mcXAoetz1FR7Maa6MCp32AZwwq0HchpetHFLyEgNA7eWFBmNMlK6yJXSGVRZ6crxuJzAWUnX4ghH5c8l2XQHRsQbDgXBOSlcNFRqlZ8roEBYn4zDy2NgYfv/73+Puu+/G1q1bzTgmQodshJEB49vGlfDW8sSyFRAfaNQqhyndowqfKy7fkN2Te/TNr6Hci7nNlQDGd17xn1/qcelm0ah0VViMBMKibJStjA4fFliuIuCTLfZUzr/SE9AlnujXs+roaJSuyNEh7EJKQucHP/gBrr76avHvQCCAY445Bt/85jfx4x//GIsWLcJbb71l+kES8WTaXq4VoixPYw3EAY2OK47eFmR+tVjidsLtdMDhkAxNls2Ug4pOk0ObKgCMFzr8zcWnE0QG5NJWsv1GRHpEIgx7e0dS+h6lizMcCGfluaRVAgbkFnO1jI7SiU2W0QGAUnf09ZPV10LSjA61lxPWJyWh8/zzz+OUU04R/37kkUfQ1taGrVu3ore3F1/+8pdx0003mX6QRDz8KjBdR2dIM4wcPbENpSB0OgfV91wl3qfWVaFY/1AiH0tJDrY2K0/uc2KOzvaDw3E/0+gMHUBZuiJHx2xuffFjHHfrq3j1E+PrZboUowMAoDcLgWStEjCQfA0EDyI7HZJuBs3nye5iz5FASMwDGi900t9/RxCFREpCZ/fu3Zg7d67490svvYQvfelLmDp1KiRJwtVXX40NGzaYfpBEPHLXVXpdblpXopVpWNVaU5E5erN05D1Xsv3PRcNoIHuiQQidch+aq3yoKnEjHGHY1imvghgLpiB0qL08a2zeNxD7b7/h70nM5WQjp6PljALJ10D4xQwd/aB9aZafVzyU73M7UJbwPJczOuToENYmJaHjcDjiWsjffvttHH300eLf1dXV6O3NznAuQoY7OuEIQziSutgZCqifoI0O+FPCS1cTVDI6yvvUyv3w0lWlomNLODpZLAMpHR1JkjCnOVq+2qIoX6VSuuKZjGTTcIn04KFivpssle/h9A6b/2adrOvKiKOjV7YC5NfCaJbWQPBQPn8dKEnnfEAQhUhKQufQQw/F008/DQDYvHkzdu/ejZNOOkl8va2tDY2NjeYeITEOZadGOm+quhkdfwpdVzoZHcOlK8UI/VxMRz6YMA12jkogme8YSsXRAajzymx4m7hywKMe4xydbJaukggd1YyOgWGBnJIsL/aUnc3xFyqU0SHsQkq9uz/4wQ9wwQUX4Nlnn8XmzZtxxhlnoLW1VXz9ueeew5FHHmn6QRLxKE+Q/lDEkOOgRLvrKrUN5owxHIjN0dEqXVXqjJFXbi7nyB1M2RMMXYP6QieV0lX8NNwIKtQfDiJFGGMiX3NwKH2hk43OK/E68qVWukrJ0eGLPbP0WtAKIgOpnw8IolBJydE599xz8dxzz2H+/Pm49tpr8fjjj8d9vbS0FCtXrjT1AInxKGv76Tg62mHk1KzqQX9IZAe0HZ3kuR9e0qpQvFmUZDmXwBgbN/ZebjEfFOXZVEpXkpSbbrFiY2AsJByQrhQcncQwcmIpywz4hHH1rquYo6PyXOBNBG6XgYwOL+Nm29FRETq8QYAyOoTVSXka26mnnopTTz1V9Ws33ngjNm7cmOkxETpIkgSP04FAOJJWi7lWGDnVgYEHYkHkSp9Lc3Kwbhh5VCWMnOUpwwOjIfFmUx+z7A+ZUA6nQ0L/aBDt/WNoqS5RzNEx5piVeJzwhyLUYm4i3QrBko6j01jpxYEBf1YcnWRdV0kdHQMLPTnC0cmW0BmSQ/mJkKND2AVTtpf39/fj7rvvxuLFi7F48WIz7pLQIZPFnloZnVQdHV620nJzAFnAaIknsdBTGUZ2Z7dVm5/cK30u8Ybkczsxo6EMgFy+GhXt5cauB6jF3HyUJajBsZBh8cu/b+aEaMg8GxkdrRIwoBdGTiOjk7XSVWxwppqjw88H/lBaTQ/5ZPvBITy+bjciFjtuIjtkJHReeeUVXHjhhWhubsZvfvMbrFixAuvXrzfr2LLGqlWrMHfuXCxdujTfh5I2mSz21F4B4Yr7uh4dOkFkQLa/U2ovz3LpSsuuT8zppFK6it4u+jehFnPzSCw5JZakNL8v1qF1yIRyANnN6KiGkU3O6GQtjDykvrkcQNzKllT23xUCP//HZvzob5vwt/f35vtQiAIgZaGzd+9e3HTTTZg+fTouuOAC1NTUIBgM4m9/+xtuuukmLFq0KBvHaSorV67Eli1bsG7dunwfStp4knR1JCMUljcqa4WRRwJhhAwIKL2OK+V9ak9GHj8wMNth5MSOK84cRU4HQMqlq2yX3IqRxFCx0RZz/n1c6GRjjs5gsjk6BjI6Hp09V4D83MveHB3tjI7H5RDOlNVyOju7hgEAf9+4P89HQhQCKQmdM844A3PnzsWWLVvwm9/8Bvv378dvfvObbB0bkQSx2DNFR4cHKAHt9nLA2BWcLHTUZ+gARgYGypvLOdkOI2stMUx0dPhVdIlBR8eb5ZJbMdKd4OAYaTEfCcgh+WwJHcaYcHQqVLqukj0XAikMDPRlMaOjDOWrCR1ALilbKacTiTBxblq7vSulsQSEPUlJ6Lz00ku47LLL8Itf/AKf+9zn4HSm1tZMmIdY7Jmio8OHBXqcjnFXlMorOCMnNn4yaarSd3T0wsgVKnN0/NnK6GjMDuFDA3d2D2MkEBJzdLSC1on4qOvKdNIpXfGylcflwOTaUgDRFRDKYaeZMhaMgMc/ks/RyWxgIM+HZUP0q4XyE6nQ2VVXiHQPB0QOKsKA5za15/mIiHyTktB5/fXXMTg4iCVLluCoo47CXXfdhYMHD2br2IgkeISjk9rJW84VaHVJGb+C42HkCUmGxihzP2qBRnnX1fjJyFkfkpZwFTuhwof6cg8YAz49MITRmNCi0lX+SHRijFyd8++pK/OgtjSaPQmGmak5E+V9lao4fkYyOsbCyLHcVxZeC3wqsjKUn0ilBTuv+Foazj8+oPJVsZOS0DnmmGNw3333ob29Hd/+9rfx2GOPYeLEiYhEInj55ZcxODiYreMkEhCOTji1E2Cyaa5AavuuUildKX82JxKR33yUpSt+NZy10lWSAKayfMXH7hstXYlsUZpb5YnxcNEysboEgDFHh39PbZkHJR6n+PuZuQZCXDB4nHCoLOZM1nUViF2cGHF0mquiv/cnHYPi9WYWnTplK0Dh6Fgoo9PePwog+pyRJOC9tl7s7R3J81ER+SStrqvS0lJceumleOONN7Bp0yZ873vfwy233IIJEybg85//vNnHSKjAT5KBUHqOjlpLLGC8xTwSYeJEmax05XU5NQONQ4EQeDUhbmBglreXJ8slxAmd2M83WroqESU3cnTUYIyl/Dfl4eNZjeWxf+sLHX6b2jJP3H+7h41lNfb0jOC6xzdi837tJaJ6FwyG5ugYCCPPaa7E0mk1CIQjeOCNnbq3TwX+2GqVrQBlRsc6Qod3gx4+sQpHtdYCAJ75kMpXxUzGc3Rmz56NX//619i7dy8ee+yxcYvhiOzAbe9AymHk5CfocoMt5l3DfoQjDJKU/EQJaJfDuPDxuhxx1nm2w8hdGl1XgJzT+ah9QARADc/RycGOLivzX898hPm/eAnbOo07vz0xcTK7KSpAUy1dAbLQ6TU4S+ev6/fgiQ37cN+aHZq3SbbQEzC61NPYufI7J84AADzydhv6R8wTHHpBZEB2eLWW8hYi7f1ydvCsBS0AgH9Q91VRk9Jk5EsvvVT3NnV1dWkfDGEcfjUYTDWMrLH+gVPhNXYF1znAyz9eXQu+0udC15B/3H2qbS4HshtGDkeY6ORJ5uh83D4oHiPjpavsb123Mmu3dyEQiuC9tl4cEhvklwzGmBAthzZFb2+kvVwInZgAr4kJnR6Dpav9sTfKrZ1DmrdJtucK0FvqaTyjAwAnzZ6AQ5sq8HHHIH7/1i5cdcpMQ9+nhzGhY0FHJ/b3a67y4Yx5zbjx75uxpX0A2zqHRBceUVyk5Og8/PDDePXVV9HX14fe3l7Vj76+viwdKqEkU0dHbT8PoOyySH4FZySfk3if4xydsfEdV0B2HZ2e4QAiDHBIQF3Z+GOf0VAOj9OBQX9IWOBGS1deMf+HMjpq8FInD7HrMeiX91zNaowKHSOOTrciowMAtaXRN2ujQwP5c3tb55DmROBk6x+A5KUrntExMkcHiK584a7OQ2/uFN2AmWJE6MgZHSs5OtGMTlOVDzVlHiyfWQ8AeJpCyUVLSkLn8ssvR39/P3bs2IGTTjoJDzzwAJ588sm4jyeeeCJbx0oo8MQWAqY6R0fvBG2064qLAK2t5Uq4Y5PYoqq25wqQBUM2hA4/udeWeeFUCZG6nY5xV32Gu65c1HWlRTAcEU6L0VAtbxMv9TgxqTYayh3y66+BGF+6ir6RG10DwR0BfyiCfb2jqrcZ1nFGk4aRU9h1xfnc4c2YUluK3pEgHl+3x/D3JUPec2Ugo+O3oqMTfc7w8tXTH+43dcQAYR1SEjp333032tvb8aMf/QhPP/00Jk+ejPPOOw8vvvgiPYFyjHB0UixdGc/oJD+xidZyA0JHy9FR23MFKHddZaOlVv8qlpevEo9HDx8NDNSkW1FyMuro8HxObZkHFV6XEA96rk73uDBy9PnVY3CqcodCiG3VyBPpOaNelzkrIDgupwPfPmE6AOC+NTvS2nGXiB0dHcaYnNGJnZtOm9sIr8uBHQeHsXn/QD4Pj8gTKYeRvV4vLrjgArz88svYsmULDjvsMFxxxRWYOnUqhoa0a9qEubgzLl3ptZfrlK5iJ5PGJDN0OFq5H3nPVULpKotdV0ZO7jyQnHg8esjt5eToJNI5KIuHg4OpOTp15V5IkiRC73pbzLu1MjoGHJ2RQCjuua+V09HvutJeASFndFJr3Dj3iEloqPBif/8Y/r5xX0rfq4YdMzp9I0EhLifEyuoVPjdOPnQCACpfFSsZdV1JkgRJksAYQyRCV7G5JO3JyHphZINCZ29fdC5Fc3X6jo5mGDmLW8C1piIrUTo6kiSXIfSg9nJtlC6McUcnvgTF35C7dBydcaWr2NBAIxmdxGFzWw8kFzqaXVcimG6OowNEHcPLjmsFANy7entGm7nDESYcs2SvBXlXnTUcHe7m1JV54jo5Px8rXz3zYTttNC9CUhY6fr8fjz76KE477TTMnj0bmzZtwl133YXdu3ejvJwS7bnCnWHpSstyLzfYdbXjYHRp3oyGMt2fKWd0DIaRs7jIkL/h1leMHxbIUQqdUrfT8MiEbG9dtzKdCnFycMivGfJVkhgqNuLojAXDYixAbWwgZCqOTkdCfkirFV7PGfUpLkQSy/p89pWROTqJfPXoqaj0ubD94DBe2tKR8vdzeChfkuTHVw2+bNcqjk7HgBxEVnLSoRNQ7nVhX98o3t/dm49DI/JISq+0K664As3Nzbj11ltx5plnYu/evfjrX/+KM844Aw5HxiN5iBTgLkPKSz0DyS13I47OSCAkrpxa6/XFrda+HK0wMnd0whGW8u+nh5EAZm2ZR3STlRicoQMot66Tu5mI0tEJR5ih4X1y6Yo7OtH/dg1qCxYujtxOCRWx53idaC/XFzo8KD0h5h5t7RxSzR/qla68CjchMaeTrqMDRIXVxcumAQDufm172tlI/veoK/PAleQ4hKMzGspLDvOuV7bikofeNVzG7uiP/l7NCULH53bi9LmNAGglRDGS0hyde++9F1OmTEFraytWr16N1atXq96OOq+yT/qlq8yFzs6uqJtTXepOejUo36fGwECVPVcA4PPIJ97RYDitNwQtugzkEoCoq3Ng4KDYNWQEL+260qQzIZfTOeBPuiMNkMPIonQlHB3tjA8PIteVeYUTxx2d/tEgQuFI0jd2/kZ5ZGstXvh3B0YCYezvHxMrKDh6jo6y3OkPRuLKKOlmdDiXLJuG+17fgQ/39uPNbd04LtY+nQryGpTkrwOenwuEI/CHIpo7sbLB3zfuw/+89CkA4K3t3TgplrNJRke/uqMDAGctbMETG/bhuU3t+NmZc5M+Dwh7kdJf+qKLLsJJJ52E6upqVFVVaX4Q2UcOI5u9AkK/dMWFzvR6/bJV9D7V7W/RdZVQuvI4HeDVojGTpwwb6boC5PJVqTsFR4fayzVJ7JQy0mIul66if6t6kdHRd3SUArw6JqQZi4qdZPDjmlRTitbY83vrgfHlK732crfTIcYXJG4wF0InjdIVEA1Zf2XpFADA3a9tS+s+jASRgegYCv5azOUG851dw/jxE5vi/m2E9oTWciXHHVKPmlI3uoYCeGtHtzkHqmDYxKWxhLmk5Og8/PDDWToMIlXSdXT0TtDKbeOMMdV8Cs/nGClbKe/TqKMjSRJK3E6MBMKml4H4CX6Czgl+XktV7NiodGUGPKPjdEgIR5ihQLIoXSU4Osn2XfUklLuAaGt2VYkb/aNB9I4ERDeWGh2iNdmLmY3l2No5hG2dQzhxdrybIDuj2g6H1+VQfQ6nstRTi28ePx1/ersNa7d3Y+OePiycXJ3S9xsVOg5HtAQ4MBbCwGgIBgZaZ8xYMIyVj7yP4UAYDgmIMGBXtzGhk2y+l9vpwIrDm/Hnd3bj6Q/2Y/nMBtOO+ZF32vDTp/6Nuy44Ap+b32za/RLmQN6dReEnyXQHBupNRo4wiFBnIjsORjtRphsIIgPaLaqi6yohowNkZzqyPxQWV/QN5cnLJqfNbcTlJ8zADz5zqOH7l6fhkqOTCH9jnRkbxphYylJDXuUQCyNX6IeRe1QcHeW/9dZAiDfKKp9YU7FNpcVcr+sKUK6BiH8+BGL/zkToTKwuwRcWTgQA3Pva9pS/36jQAYy5vGby/579CFvaB1BX5sE1p84CkI6jo/765t1Xz/+7w9TX6ctbDoAx4K0dXabdJ2EeJHQsSjqODmNMd2BgidspLHetnA4/6RjpuAJkIZM4dExrjg6gHL5n3smI70nyOB26To3H5cD1Kw7FkbHtx0YooYGBqjAmb7o/LOaU6Tk6yj1XiV1XydrLu4bljI4SWegk/7nyahOfEGVqs3T0dl0B2gMkgyY4OgDEAMF/fnTA8HoLTpeBUD5Hq2syGzy3qR1/fLsNAHD7+QvF9nHDjo5ioacaS6fVorHSi8GxEP72XuaziDj/3hcdRLi/z9iMqEzZ2TWMk297Dd/503vYtLc/Jz/TypDQsSg8yJiKo+MPRRCKtfVqCR1JkjQzNUD0DSjd0tVoMCyOlzEmd12VjHd0fFlYA6G8ijXaMp4K2RBndmBgLCQE+byJ0exTp05GZ8gfEsMwuWjh7sNwIKy570mtdAUANaX6jk44IguypiofZjbGhM6BwXEdR3qrVABtR0fO6GT2HJzVWIG5zZUIRVjKreapOTq5aTFv6x7Gj/7vQwDRje0nzGoQOal9vaO6F3WDY0Hxd9ESOk6HhK8fG51F9IunN+Pf+zIXCZ0DY0I4aq0MMZsn39+LHQeH8fy/O3DWXW/gogffxbs7e3Lys60ICR2LIhydFISOMiyX7ATN7fhBlXBd11AAg/4QJAmYWldq6Ocqr3qHYleFw4Ew+CgVtdJVNmbSiBk65fqdYunAxVkoC23xVoZPQq7wucRz5oBO6Yrnc0rcTjFXqczjFI+xViBZu3QVW+yZZJZOd2y+j0OKOh2t9WVwSFGhpgxTh2IdSEDy0pXWYs90dl1pwfMgz3zYntL3GRmzwNFyZM3EHwrjyj9vwKA/hCVTa/C906Ilq4YKL0o9TkQYsKd3JOl9cDenqsSN0iTnt28tn46TZjfAH4rgO4+8hz6DO9C0UK6V2Nc3mpM2/I0xF+fQpgo4JGDNpwdx3m/fwnn3voXVnx6klUwJkNCxKOkMDORBZGV5So1kiz15PmdidYnhVlO30yHKOvw++dWh2ymJNy8l2ZgynMpVbDooHw9ydWQ6FQFw3lKuV7rqHh7vzEiSJP52Wjkdta4rQDE0MEmJh+dz6su9cDkd8LqcmFYX67xSlK/46wjQdkYB7cWemczRSeRzh0eFztrt3aK13gipvBYqc+Do3PL8x9i0rx/VpW787wWLROu3JEmYGvsb7NLJ6STuuNLC4ZBwx/mLMKW2FHt6RnHN4xszmpa8eb/sCg35Q1kv8THG8MGePgDA/3x5AV79/om44Mgp8DgdeHdXDy5+8F18/q43sTF2G4KEjmXxpLHrSm+GDieZVb2Dt5Y3pDYFm2dieC5HGURWKyNlYzpyl8HW8nRRzk6xek6nZziANoO5CD2Ub6qNsTehriE/Qkmeu4lrHDhiOrJGTkfr+4wMDVTLd/BN9soW86FY2czjdCRtEdda7GlWRgcAptWXYd7ESoQjDC9uPmDoe+JC+UaEjsjoZEfovLi5Aw+9uQsAcNuXF6AlYWZRa33UBdQLJOvlc5RUlbpxz4VHwOty4LVPDuLOf21N48ij8HwOJ9vlq7buEfSPBuFxOTC7qQJT68pw8zmHY80PT8Klx7bC53Zg075+/M+Ln2T1OKwECR2L4k5jMjKfiqzVccVJttgz1Rk6HHlnTjDuv2r5HEB+kzBTMBjZc5UJkiRpXsVbjct+vw6n/39rsLs7ebnACHJLvw91ZR44HRIYk8PhaiRuIOfU67SYi4GBCX9jOaOj/TOVQWSOyOkoHB1efk3WWg4AXnfyjI7RHWp6nDk/2kn07CZjE395F5kn1navh9H9d+nAGMPP/7EZAPDN5a04ZU7juNtwV00vkKzXcZXIYS1V+NXZhwMA7vzXVrzysTGhmMjm9qijw13yfX3ZFTrcqZnXUhknlpuqfPjZWXPx4MVLAeiX+ooJEjoWxZtG6cqoo8NzB0NJSldGW8s5iSdLHkRO3HPFEY6OiQMDs126AuzRYh6OMHy4tx/+UASvftKZ8f11Kh53h0MSQjNZi3nisECOKF2pODpjwTCG+Z4rjfbyZBkdtRksh6h0Xg0Z6LgCtMW6maUrQC5fvbW9W9PpUsLdk1PmTDAUypczOuY7Opv3D6C9fwylHie+d/ps1dtMq+elK52Mjsaeq2Scu3gSvnb0VADANY9tTNnF7B8JYk9P9OcunVYDANifI6GzQGN20uTaqAPW3jdGC0xjkNCxKO402sv1Wss5yeZmiNKVwY6r8fcZEzqitVz9ipIvRRwzUTAYnYqcCXZoMe8a8ovuvLXbM58Lkjikke8RS5bT4c5LYnA8maPTo9hzlTiywFBGp1/uuOLMVJmlM2yg4wpQODoJ7p5fhJHN6fybXFuKBZOqEGHAC5uTd1919I/h7xujbdXfOn66ofvPpqPzr4+iQnr5zHrNzB/vvNIrXaXq6HD+88y5WDSlGgNjIVz+p/dTurjibs7E6hLMbY6OTsi2o/PB3j4A0BwS2VTlgyRFYw3dKY4dsCskdCyKRwwMNK7Y9dY/cOQlnPEntmA4IkoZrWk7OgkZHY15NtzRMXMFRG4cHeuXrpQn6rd39BjaNJ6MxMd9QiUPJGs7OlrdU/w+kgmdmlLPOKeiNla6SjZvRq10NaOhHJIUvW9eFjP6OhIrQUxc6qkF77569sPk5auH1u5EMMxw5LRaLJpSY+i+s5nR4eWik5PsseKlq/39o0mdUjmjM379QzI8Lgfu/uoRqC/34KP2AfzkyU2Gu5a2xDqu5k2sxMSa6M/NptAJhCKiy2vBpGrV27idDnFRkW13ySoUpdBZtWoV5s6di6VLl+b7UNKGnyQTg47JGNJZ/8DR6rra0zOCUITB53agWaezIZHKhPsc1HN0TG4vZ4wpMjqpHXsqZKMtPte0K4ae9Y8Gxck8XXiJindccUcn2SydLo2MTkPM4VEr0cidWuOFbG3s+4YDYU0Rqla6KvE4MSn2BsZdHaMlYNnRUQ8jp7vrSo0zYuWrd3b2aD6ug2NB/Pnt3QCMuzlA9hydg4N+fBBrkz5ptrbQqS/3oNzrAmPRc5AW6To60e8pwW8uOAJOh4QnNuzDS1uM5XW46DispUosfs1mGPmTjkEEQhFUlbiTjvfgge72fhI6QJEKnZUrV2LLli1Yt25dvg8lbTzphJF11j9wysW+q/grOG4dt9aXw5GkPV0NXkrgdX7uFmmFkbWmyqbLcCAsxEd9RXbm6ADKDebWLV0lnhwzLV8lOjqNBlrME9c/cOTS1XhnRt5cPv7vW+F1wRV7zmrldA4IRyBeKPHyFc/ppOroKF2IcIQJh8xMR2dSTSkWTakGY9H1Bmo8+u5uDPpDmNFQltRBSSRbGR2e/zp8YpVw+dSItpjzzit1oTMakDvJUsnoKDlmRh0uOiaa13lRpwTI4a3lh7VUykIniy7KxljZasHk6qT5qhZxLLmZ1KxFKBzBUxv2Ye32royd4UwoSqFjB7yZZHR0sgVaXVd8InKqHVeAdhhZbf0DYP6uK/5mW+ZxJh0mlik+G3Rd8TH2FbE38rXb09/0HAhF0DsS/VvLGZ2Y0EkSRpbbxI2HkbXKXUD0zTJZTmfYHxIDMhsT3nT5Kgju6PDAs9GuK6XoVV6YmJXR4fBQ8rMqwwMDoQgefGMXgKibk8qFSrYcnVdi+RwjoksOJKvndLgbV+ZxiudtOpwW6/p6fWuXbvlqNBAWz4l5E6tE6ergoD9rzQh8fs7CSVVJb9cSE3v5Ll11DQVwzeMb8bUH3oX5s+iNQ0LHoqSz1DP1OToJQkfM0ElH6MRKV/749vIKzdKVuYIhF/kcwB5rILijw3Mf7+7sSUlQK+ElKLdTEq3ME3TCyIwxzcF/3NEZDYbjJn0D2sMCOXJOZ7wzEfdGmfCclDuvorN0+Oui3Ju8NVttBUS80DH39MvLV+vaekRehfP0B/vRMTCGhgovvrhoYkr3y13XQX/ItKvyQCiC17ceBBDt/tKjNZbT2anRFcWfs9EgbvpvqYun1aDE7cTBQT8+7hhMetuPOwYQYdHS2oQKL2pK3eK81W7QSfn3vn6cevtqw63tH+h0XHEKpXTFc28TYh2X+YKEjkXhpatQhBluITQeRlbvukq3tRyQQ8eyo6MTRjZZMGR7WCBHCLQ0hUEhsD/2Jnni7AbUlXkwGgyLTo9U6RRrN+QTHc/qHNRwdIb88m6sxNJVmdeF0lhQPdHV4XuutFZ81MTWQHSrLPbkZatGlbLHzMZY6epAYukquaOjtgJC2TzgMVnotFSXYMnUGjAWXY7JYYzhvtd3AAC+fuw00fZuFOUIiCGVtTCcgbFg0q42Je/u7MFwIIz6ci/mtSR3JwADjo7I56QWRE7E63Li6OnRRaJciGnB8zlzW6ogSRIkSUq5fPW39/diW+cQ7vzXNt3bDo4FsS12Dp6vEUTm8Mch36UrfgGRrDSZC0joWBSl7W10OrLRMLKYo+NXd3SMLvNUUuGN34Cs216epdJVtoVONlZX5Jr22Em6pboER8+oAwCs3ZZe+SqxtRyQw8hdQwFVR5K/WfrcDtUyo1aLORcwibN3OLwMptZ5pRZE5nBHp3PQj/6RoOExDWrDI/nv63JIWbnCFd1XCqGz+tOD+LhjEGUeJ7561NSU79PrcorfRSunE4kwfOGuN3HKba8lnY/EeeVjXrZqMPQ48OnIWkKnPYWpyHosn9kAIFq+SgYXOvNaKsXnJtZEj9Oo0OGlrw/29GGvzoC/Tfv6wVi0lV3vPMYFV3ueS1c8GN+Y5fOuHiR0LIqyY8Oo0JFP0Mmv6NTaywfHguJNqzWTjA4PIyfZXA6YH0bO9lRkjtVLV4FQRMwbaq4qwbEz6gEAb6YZSOZvesoTc02pRwj1pN1TGoKFOzbjhU7y0hV3dHpGtEtXakKn3OsSmYdtBweNd12prIAwc6GnGivmNUOSgPfaekU+43drom7OV46cYmgSshrJ9t8BwLaDQ9jZNYzekSAeeH1n0vtijOFfoq18/CRkNeQW8zHV11aHwT1XRjh+VvQ5/87OnqQzdeQgsuxITayO/nyjnVc89wgAL2iEyDkf7In+PK35OUqaY8dxcMifdtnZDHh5OjH3lmtI6FgUt0P+0wUNPpHlFRDGSleBUETkC3jHVX25J62TpbwCgreXy7uu1DB7MnKuMzpWbS8/MDAGxqJllboyD5bFHJ0Nu3vT+lvIj7t8onM4JMVyz/FX/7wElVi24mgFkrU6tTjJZukkK10BwCGK8pXR15FaziwQNndYYCJNVT4snRotvTy3qR3/3tePtdu74XRIuPS41rTvN3FXXSLvt/WK///T221JN4Lv6BpGW/cI3E4Jx82sN/Tza8s84mJpt0qLuZmOzoyGcrRU+RAIRfDurh7V2wTDEZHhOUzp6KRQuhr2h+Jup3Th1BBBZANCp67MA4/LAcaSz6vKNvJsKnJ0iDRwOCRxsjReukptBQQgr4HYmeZEZI5yYCBjTLHrSv1YUpmM7A+F8Zf1e5KeXHMxFRlQ77SxEso3DIcj2tbbUuVDMMywvk39pJ+MTg2BmSyQLJeg1AWLWOyZ0GLOBZK2oxPrulJ5niRzdAC582pr55DhErCao8NLV2bO0EnkzAXR8tUzH7bjtzE35/MLWsSbcDroOTrv75aFznAgLNZMqPFqrGx19PQ6XbHIkSQp6YRkvv4hnRk6aj9LlK8+Vc/pbD84hEAognKvC1Nq5Xk2vPPKSLcT/z1KPU5IErBhd1/S79Nb/ZD4O3AXMtuTmpNxgJeuydEh0kV0XoXMDSM7HRLKYo4KP7FtP5h+xxUgl6j8oQgGxkIilKnVdZXKZOS/b9iPH/7fh/jeXz7QvE3OHB2XtUtXvEuDv2FIkoRlh8TKV2nkdNQyOoA8S0ctz6FXulJzdPyhsGgPr9f4Pi6AelRm8HToWOxxQicm0o06OnFCJ2T+DJ1EPjuvCZIUfWPkk5K/udz4gEA1EudgJfL+7j4AwDmxjq6H3typukIGkNc+pDLLBwCm1mkHklPZXG6E5bHy1RqNQDLfWD63pTIuY9RSZdzR2R4LFs9rqcKSqdEp1VozkDr6x9AxMAaHFJ3CbIRC6Lzq1LmAyBUkdCwMvyoMhI29qQ4bvBIFxl/B8Y6rdPI5QPybAq9fOyQIQZWIyLoYKMvxLb2vfNKpOTn1oKL7J5uYnS3KNXyGTovi6p+Xr95KI6ej5ejI+65SL12phZF52crlkDRdwmSLPQ/ovFHyLebbDgyK15Ge0BGOjmrpKnun3gkVPhzVGi1fRVh0j9TcFmNvjlpUJtl/1z8SFKHaG86Yg+kNZRgYC+FPsSnMSgbGglgXKwelKnRaY0MDE7eY+0NhMUAy064rzrEz6iFJwKcHhsa16gPxgwKVcEfHyEJN/pjNmFAuRgM8r1G+4l2PsxorDM8B46/h/XnsvFJbq5IPSOhYGLfYYK7v6DDGRLZAL4wMjN9NJUpXDemVrpwOSbwx8O6CyhK35swLMTDQgKPTFwuXMgb8+d3xJ9dIhOWhvdwejg4ALIsFkjft6xfTZ43SpeHocCu7U6V0lWzwH6AudLpjb3Q1ZeP3XHFqStUHBoYjTJQ2ta48D2mIZnT294+J0pfRgYFqpatsZXQ4n5vfIv7/28fPyPj+5IzO+NLVhj3RstW0ulI0VHhxxYmHAAAeeGPHOGfz9U+7EIowzGgoEw6NUaZplK74c8jjcqCmNL2wdSI1ZR7Rwq3WZq5c/aCkqdIHR2yhpto+NiXc0ZnRUIbPzmsCAKxv61UVVqnkczj5Ll35Q2ExLJQyOkTa8DkcRjI6I4Ew+KBPI3VxvgZi0B8CY0whdNJzdABZPPE6tFYQGZDfJEaDYd0JpX2KN9/H1+0ZN5W0fzQotnFrlUPMwurt5fzqr1nh6DRV+TC9oQwRBryzw3j5Km6/WKLQif37gErXVZeO0FErXcmTlLXXeygdHeVzqmvIj3CEwSFpz+CpKnWLn8u7WAwv9VRpL8+mowMAZx7ejOYqH46f1YBjD6nL+P60ZmsBctnqiNiS0C8sbMGkmhJ0DQXwWMKFB++2OmWOsW4rJVzotHXHu7bKHVeZDAtM5PiZvHwV72RGIgwfKZZ5KnE5HUIs79URGNzROWRCOZqrSrA4Vr564d/jXZ0PFKsfjNKS5xZzpQBNt9vPLEjoWBhPCmsgeD7HIclvxslQlq4ODPgxEgjD6ZAwuUZ7kZz+fUbfGPgVhlaJAYg/Rr3FpcoQcs9wYFybJr9aryl1ZzUECli/dMUdnZaEEg4vX6WyDqJ/NChE+PjSFXd0VEpXw7zMqCF0FI4OFyx6HVeALHSCYRY3I4pfQTdUeOFKIkB4TodjeKmniqPjzfLzsKbMg7XXn4zff32pKW/+ckZHxdGJBZEXxd6o3U4Hvn1C1EX67Zod4vwUjjCs/iTqjiRb4qkFn47c3j8W5/SKqcgml0d4IPmNrQfjylC7e0Yw6A/B43JghorDLbaYJ2kxD4Uj2BXb28XvY0XM1XluU/z5KxJh+DDWWq61sVyN5jyXrpQdV2YK0HQgoWNhPCmsgRhS7Lky8qRTlq54PmdKbWlGQoE7OPsMODo+hdDRC/by0hW/2vnT221xX89VEBmQS1dWbS9v15gwy8tXb6UgdHg+p6rEPW4ar9h3lSSjozX4jy9lHQtGxPNanqGj/Tf2uZ1iqrKyfKXXccVRCh1JgrgvLcQKCGVGJwdhZA6f1msGiStcOOEIwwbh6FSLz3958SRMqPCivX8MT27YCyDqSnQPB1Dhc2HJtJqUj6GmTB5t0dYjl686MthanoxFU6pR7nWhdyQoSlWAXLY6tKlC9e84UQgMbaGzt3cUgXAEPrdD3H6FYoWH8gJgR9cwBv0h+NwOzGo0Hh3gM3325ymMLGboVOQ3nwOQ0LE0blesvdyQo2M8iAzEL/bcLiYip1+2AhSOTuxKp0JjoScQfSPg26b13JG+0eib1uXHT4fTIWHdrl583CGfmHIpdLwWHhg4FgwLAdBSHX9yOmZ61NH55MCg6pA/NbQ6rgC5Zt87EowrNSr3XGmVoUo9LhFi5yHUZJvLlajldDoNBib5LB0AKDdwwaAWqM9FGDkbiIxOgqOztTM6QLHM48RsxePjcztFp9c9r21HKBwRSzyPn9WQ9u+vtgpCHolgThCZ43Y6cEzMyVR2X6kNClTSYmCWDi9bTa8vF11bE6tLsHBydAP9C4rt6Tyfc/jEqqSOYyL8YmVwLKTZAZdNCiWIDJDQsTSpZHSGDE5F5ijXQOzMYGu5En5VuLdX39EBjG8w547OrKYKnD43WvtXujq5mooMWLu9nL9hlLid42rqNWUezG2O5hHeMpjTSSYwq0rkMqIykDwcCItSj1ZGBwDqE3I6egFmjlrnlXB0dBwBpaNj5IKBOzrhCEMo9hrlwz3dWS5dmQ1f4ZL4hvl+Wx+AqJua+Cb8H0dNQXWpG7u6R/Dspnax9uGUFLutlEyLdV7t7JJzOvwN1WxHB1DkdBTzdP4tgsjqnWxGSlciiJxQDuUb6JW7ykQ+J4WyFRB9jvLXcbtKwDnbHBgkoUOYQDoZHaMDupThwx1dfJlneh1X8n1Gfza/Ytda/8DxGui8CoUjogW+usSNC4+O7vJ58v19QtzlalggIP+OBwf9pm16zhU8tNhcrR7qTLXNnM/IUXN0JEkSro5ylg4vW0X3XGmL8oaEzqtuAxkdQDE0ULHBvKPf2Jj6eKGjf8HgU8mZiYGBWe66Mhv+Wk3suuKDAnkQWUmZ14VLj41OY/7vFz/BlvYBSBJwYhr5HM40lVk6Zk5FTuT4WdGczvu7ezEUa8zYotFazjEyHVm0lic0d6w4PJrTeXdnjxDxRjeWq9Gcx86rTjGbKr8dVwAJHUvjTiGjI7eWGxU6cjsp38eSaekqUdjoOjoe/VZtZbtzVYkby2bUYXpDGYYDYTy1YR+A3M3QAYDZTRWo8LowMBbCh2lu/M4XfGt5i0YJ4NgUBwfqlQzF0ECFo8OnIteVJQ8wJraYG+m6AoDaWPtxj2KD+QGDGZ26cq9whIxcMCi3k3OHL1ddV2aTOG6CI4TO1GrV77v4mGko97qEi7tocrWu65YMMR25e3xGJxtD6abWlWFKbSmCYYa3t3ejc9CPrqEAnA4Jc5rTFzrc0TkkwdGZVFOKBZOqEGHAi5s74A+FsaU96iCl0lqeeCzteQgkU+mKMAVvCo6O0fUPHO7o9AwFxNybxKuPVEnM5CTrugLk0lWy6ci8tbzC64LL6YAkSWJD85/ebkva4pwN3E6H2N/z2ifqU1ULFeHoaFwZL22thdMhYXfPiOZgRiWdIqOjfn8TVIYGGi1B8UAy/9vyjE6yMLLy63GOjsHSFSC/MRl5HTkckhA73NEJhHMXRjYT4egoMjp9IwFxEbRosnq4uKrUja8dI29MT6etXIncYh79uaFwRDiC2ShdAdGBi0B0ng7P58xoKItz7JTwjM7gWEh1NxhjTEyaV+vaOkNRvtqyfwDBMENtmQeTalLPILUYCEZnC/66mkCODpEJKTk6KZau+O027+9HhEUnGGcqFBLXPeg5OnKYM4nQieVzqsvk+/rSEZPgczvwcccg3t/dm1OhAwAnzo7a3a9p7MkpVLij06yxE6nc68KCSdEAppGcjt7jLhZ7KsLN3Tr7qjgN5dHvTSxd6Wd0os8T5WJPsdDTwJUnL18ZfR15ExZ7Wt3RCYQj4nfh3VbT68tESVCNbxzXKroRT5mTftkKkFvMoyMvQjg45EeERSdi12XJseXlq9e3donVD1pBZCAqgqtjzqGawOgaCqB/NAhJUnfJudB5e0e32Au2YFJVWh10fIt5PoROZ4FsLgdI6FgantHRmzMDQLGI0FgYuVJRugKi+ZxMW1UrExydZF1XgGITeED79+uPdVxVl8gn2qpSN86KTYb941ttOZuKzDlhVvRk/uHevnFTeAsZrRk6Snj5au02/ZxOZ5KuK0C9xdxo1kbp6ARCck5Lt+sqYbHnsD8kdmQZcXR4+WCSwXlSiYs9eRg52/OczCbaZRb9f+5S8LLVIpV8jpL6ci/+9I2jcO+Fi3FoU2arKKpK3WL68a6uEZHPaaz0wenITu7pmBl1cDok7Ogaxktbot1QWvkcjihfqQSSedlqck2pqis0ubYUh0+Mlq8eeGMngPTyOcrjyHWL+ZA/JKoIJHSIjJAdHf3Q63CapStOJhOR5ftMLF0ZdHSSdDAJRydh9DsPJT+3qUO8qeWi6wqIvmEe2lQBxtTHxxcq7SpTkRM5RjE4UG9itW5Gh4eRFRmdHpHR0XN05A3mvIPK6ZB0J7DWxtrLuaPD7fVyr8uQS3P2oon44zeOxPdOn6V7W0AxS8fiYWSHYoULF5V6+RwlS6bVijUHmSJazLuHTV/mqUalz41FMaFhxNEBks/SUa5+0IKHkodjZft0hQ5vMc/10EA+sqHM4zTsfmYTEjoWJq2uK4ML4RJFSaZBZGB8qUq/vVx/+B4XOolvcAsmV2P+pCoEwhEwFn0T5DNUcsEJvHxloZzOfgOOzhFTauB1OdA56BcnbDXGgmERFNfK6CRzdPSyNry9vGvQL++5KvXEbZJWQ+66in6PXLYyJoJdTgeWz2wwfMHgSyhdWTWjA8iv14HRIMIRho0Jqx9yBe+82tk1nNWOKyW8fMXRW5LKszFqayCUqx+04G3mnFRby+XjiD4uHf36S0bNRAwLzPLfxSjWe7URgrQmIxvN6CQInUxbywGVjI5OGNmYoxMrXaks87vwKDkEWV+u/yZoJifGyldrPj2Y0xNMugz5Q+JKPZmj43M7xRvb2zt6NG/Hy4Uep0Pz76y2wZyLFuOOjl/RqaUvZOsSSlepBJHTIbF0FbDoHB1A2XkVwqcHBjEcCKPc68IsxaDAXKBsMe/gS2izXB7hgWQgOiFezzmclGSWTrIgMmdqXZkoj02pLU27U62x0geJLxlVdBpmGx4QL4SpyAAJHUsjHJ0shJETHZ1MhwWq3ade6arEiNCJuQZqbs1ZC1pELihX+RzOkmk1KPe60D0cwKZ9/Tn92enAO64qfPolnKOm1wIA3tmpLXQ6FWUrrWwX32A+MBYSf2MjO6sAub08EIpgV2zJo5E3A+7o9I8GEQpHhNDJVo6AOzp+i4eRAeUsnaAoWy2cXJ21bIwW0+qj+ahd3blzdOZPqhbiRi+fA+iUrgw4OgDw+QXRnOGRrbUpHasSt9MhxEYuW8yVe64KAeu92giB25m9FRBelzMuMGlG6UopdCRJv4zmMzAZWat0BQAlHifOXTwJQO6vLNxOh9gavdoC3Vd6M3SUHNUa/b3e2aGd0zHS6VbhdQkhwHM6RtvLSxS1/087BgHoiyMgOlQSABiLip0DWZzBAsiOzpjFMzpA/FoYPhFZud8qV7SKjM6IYs+VuesfEnE6JFG+MjLPRmsNxEggJD6XzNEBot1q/9/5C3DDikPTOGKZfHReGR3CmStI6FgYjzN6Es3GCghAPrE1VfoMC6RklCk6N8q9Lt1SkpFN4NzRqdbI33z35Jn4ytLJuOKkQ9I44szgE2Bf+6Qz5z87VZRTkfVYNKUaHmc0p9PWrT5Pp9OA0IlOR+Yt5mNgjImSV51ORkd5358ciAkdA46Oy+kQorhnOJD90lWCo2PVXVdAfEbn/YSN5bmEh5EPKnJi2XZ0AOBnZ87Ff545Fxcvm6Z7W74GojPWFcjhc4dqyzxJW/KB6HP17EWTMm6bF7N0crgGgq9/mEBCh8iUlJZ6BlIrXSlva4abA0Q7Nypi96kXRAaM7brq5xkdjTJYbZkHt5w7H4vzcEI+IXYFuHFPn8gSFSr7U7gy9rmdWDA52nXyzk71eTrJFnoq4U7bgYExjCj2XBlxZ+pjt/kk5ujoBZg5dYpAckeWZ334xjk61g0jc0e2rWcEO2MrGI7QGBSYTSp9bvE37I05utkaFqikocIbmwmkf7FYV+aB1+UAY/LkZkAxEdmEzKNRjGxTN5tOKl0RZpFKGDnV9nJADg+b0VqeeJ96+RxA0bGSZDJyr0Z7eSHQUl2CWY3liDBgzVZj+6HyBXd0knVcKZHLV+o5nYOxKzq9bJQ8HdkvylZeV/I9Vxye0+HdXbUGxBEg53R6RwLZL10lZnQsHEbmr1m+4HJGQxmq8vS6m6a4+HJIuc/g6SFJkhAYe/tk15Pnc2ZMMO+cqgcXgZkKnf6RIPxJhrcqOVBAwwIBEjqWJp0VEKk4OvwKzoyOq8T7TBweqEaJx8hkZN51lbvW8VSwSvmqXWcqciJ6geSDOusfOPxE2DkwJg8LLPMYGk6Z+OZWb7AzhQfXDw4FxMLXbJU+fIkDA2MXJV4LOzp8b1Wu28qV8M4rICp4C9EhU9tivk3M0Mmdo2NG6Wp39wiOu/UVXP7H93RvyxgzvD8uVxTes4MwjNEVEKFwRORcUnF0zjliEg5tqsDpczPbT6OkMiVHJ/n28nCEicnNhejoAMCJsfLVmk+7CrrN3MgMHSWLp9bA5ZCwr29Ude+VkYwOEN9iLvZVGXRmEpe0Gm3B5Wsgth4YRDjC4HRIWVv4qpnRcVkxjBz/GjsiD+VgzrQ6eTJ1LspW6SCXjBSlq85Ya7lOx5WZtFRlXrp6YsNeDPpDeH1rl+6F9cBoSAj7QnHaSOhYGKMrIIYVQiGVMPKXFk/CC9ccj8m1xsbdG4HPVDGS0dELIw8kbC4vRJZMq0Wpx4muIb/YQlxoMMYMTUVWUupx4fDY3qt3VVwdwxkd7ugM+hWOjrGTY+JJ1EiuB5BLVx/F/h4N5d6stUhrTUYuRAdCj8Q5WHl1dBSlq1wEkdNB7ryKXgiEI0xkm3KZ0eFDAw8O+g2XnpQwxvD0B/sBAKEIwy7F5ng1eBC5utRtKM+UC6z3aiMERh0dns9xOyXR7pov+MlSb88VoB9G5h1X5V5Xwb5xeFwOLJvBt5kXZvmqfzQoHuNUro75fI/EQHIkwgzvF+NfPzAwJs/QMejMjHd0Ugsjf9QeDTFnc3orP9GPGxhYoM/XZCgHP1Z4XWLBaT5QNkhku7U8XSYmtJjv6RlBIByB1+UQX8sFtbFgNAAc6E99aODHHYNiyCEgh/+1EDN0CmRYIEBCx9IYXQGRThA5W/AreCNXYYnj8xPh+ZxCdXM4Jxb4OghurdeWeVK6AjuaB5ITHJ2+0aDoLtIrCckZHb/hGTqceoWD45C0O+8S4RkdnltrymJnCH+DkbeXRx8XjwWFjtLRWTilOqeTxhOxgqPDMzr89cU7rqY3lOf0sZMkSZHTSb18xd0czqcHkgsd3mU2oUA6rgASOpbGY3Cpp5ihY3DPVTb51vHTcfM5h+OrR03Rva3eZGQ+LLCmzBpC5/3dvegfCercOvfwreWpZh2WTKuBQwLaFIPbALlsVVPq1t3SzYXOoD8ksj5GMzpKt6i2zPiKj0Qhlc3A5Ljt5RYuXSkbCPQ2lmebcq9LiOhCz+js6xtFJMIM7bjKFul2XjHG8MyH7QCAJbFMlp6jw/N5hdJxBZDQsTTGHZ2oUCiELbK1ZR5ccOSUcfV+NfQmI/eN8hk6hdlxxZlUU4pDJkTbzF/fVniuTiozdJRU+Nxii7OyfMX33Oh1XAHR52RZrLuOZ2bqDZaglG5RKruAEge1Zbd0lbjUkwsd64WRla/ZfMylSuS4Q+rgdkppL7zMNk1VPjik6Pm5a9hvaGt5tuCOTnuKnVcf7u3H7p4RlLid+M6JMwDoOzqFtv4BIKFjaYxmdNKZilwI6IWRxfqHAu24UsK7r1YXYPlKzNAxMBU5kaNETkcuXxlZ/6CEX/m1cUfHoGjxuZ0i65WK0KktLQBHx4JzdKpK3Cj1OOF1OQytQcg2t5+3EOt/elpcGauQcDsd4rm9v2/M0DLPbKG1kkKPZz6Mlq1OmTNB/M3bekYwEhtAq0ahtZYDJHQsjeGuqwLK6KQCn6Oj6ejwYYEFntEBgBNi5avVnx7U3A+VL9oz2BckAsk7lI6OsY4rDq/l84fFaOkKkLeYG+3UUrv/rAod3l4e4gMDrZvR8bgc+OM3jsSfv3lUQeTiHA6pII4jGUJg9I7mtXTVkkbpKhKRy1ZnLWhBXbkX9eUeMAbxu6jBhwUWyvoHgISOpeH2t27XVRrrHwoBn6I0pzaDpn+0cKciJ3Jkay1K3E50DhZem/n+DBydI1trIUnA9oPDwslJ19HhGO26AuTyldHWciDaMeRS5HmyWboSSz2DCUs9LejoAMDiqbVYPDX9bdrFBs/pfLi3D/2jQUiSeSt1UkGUrlLYYP7e7l6094+hwusS62xmNVYASJ7Tkdc/kNAhTEBMRjZcurKW0ClRrAFQm47cO2KNjA4QfcNbNiPapVRo3VeZODrVpR7Mjp38+Dwdo8MCOeOETgrD+/jPSKV0JUlSXE4nl46OlZd6EqnDBcbq2NqMSTUleZkt05LGBvNnYt1Wpx3WKI6ZCx2tnE4kwhRhZMroECbAT5ZG28ut5+jIJwS16ch9BbznSg25zbxw5ulEIkx0TKXbvXL0dN5mHi1fGd1zxVGWuDwuhwgnG+HzC1swo6EMp85JbXo3z+lUeF1ZvQDwJTg68hwd64WRidThLeYfxxyQXA4KVMIvYgb9IQyM6Xd+hiMMz27qAACcNb9FfH52U8zROaBeuuoeDiAUYZAk/dESuYSEjoXh9rf+wMCoSLBaGNnhkMTvOKYi5vpE6arwHR0AOOnQCZAkYN2uXmzrTN65kCu6hwMIhCOQpPTnkfBAcqKjY6TrCoiv5Rvdc8X5zGFN+Nf3TsS8iVWGvweQHaBslq0AlYwOL12Ro1MUTEoYDJiPIDIQdfN5nslI+eqdHd3oGvKjutSNYw+pF58Xjo5G6YoHkevKCmv/WOEcCZEybsUcnWR7lKxaugIU05FVHJ1+sdDTGo7OpJpSnBZzHu5bszPPRxOFz9BpyGAx4tKY0Pm4YxC9w4HUMzoV6bWJZwL/OdnuDBErIIIRhCMM/GVaSG8CRPZoSRA6+Qgic1IZGvh0rNtqxbymuDzZrMbo8XcMjKnOBOOjJQqpbAXYQOjs2bMHJ554IubOnYv58+fjr3/9a74PKWcon4DBiLarY9XSFZB8OrJwdAq880LJt46fDgB4csM+EdrLJ/tT3HGlRn25V5zA12w9iMHYolWjk1GVGZ1U8jmZwIdMZjsw6VMMvVQ6r1ZsLydSh5euOLlc5pmI0c6rYDiC5/8dLVudqShbAdFZSjxg/amKK807rgqptRywgdBxuVy44447sGXLFvzzn//Etddei+Hh5EvH7ILS/k6W0ymkycipojUdORJhouvKCnN0OEum1eKIKdUIhCN4eO2ufB+OcHSMbi3XgpeveDuq1+VAhUFhrRREqXRcZcLCydGBd0umZXfwnXKpp7JpgDI6xUG5omQE5K90BRjvvHpjWxf6RoKoL/eK/J0S7uqodV7x0lUhtZYDNhA6zc3NWLhwIQBgwoQJqK2tRU/P+G3KdkQpdJKtgbDqHB1AezrywFhQzF2xQteVkm8dH50w+qe328TfJl9k0nGl5KjYCZEPRGyo8BrO2pR6XGkN/suELy2ehPd+eiouOFJ/FUkmKJd6Ki9GKKNTPHCBUVvmydnzO9lx6Dk6fLfVGYc3wamyVmV2UyUA9c4r7uhQ6SqBNWvW4KyzzkJLSwskScJTTz017jZ33303Wltb4fP5sHjxYrz++uuq97V+/XpEIhFMnjw5y0ddGDgckpgHkszRGYiVEpTbh62C1nRk3nFV5nFabibJaXMb0VpfhoGxEB5ftyevx5LJDB0l3NHhroXRYYEcXkLK5RtBLspkXsVzk4tat1NKKXBNWBte6snH6gclosU8SUZnLBjGy5sPAIgOCVRjdlPU0flYxdEpxBk6QAEIneHhYSxYsAB33XWX6tcff/xxXHPNNfjJT36CDRs2YPny5VixYgV2794dd7vu7m5cdNFF+N3vfpeLwy4YjKyB6B6KquxUpscWCjyjk+joWK3jSonTIeGy5a0AgAfe2ImQTtdcNjHL0Wms9GFaXan4t9EgMoe3tqf6fYWOVzEigWeXKIhcXEyK5XTyGUQGlI6Odulq9acHMegPobnKh8Uai1uVs3QSp7x3FOCeK6AAhM6KFStw00034ZxzzlH9+u23345vfOMbuOyyyzBnzhzccccdmDx5Mu655x5xG7/fj7PPPhs33HADli1bpvmz/H4/BgYG4j6sjt4aiFA4IkRBKtNjCwWtjE5frOOq0EfAa3HuEZNQV+bBvr5RPLupPW/HwfdcNWfo6ADAUa1yPd9oaznnuyfPxAVHTsFnDmvK+DgKCbdTAnf/SegUJ+cvnYyTD52Arx41Na/HwS8m2vtHNbt0ecbuc4c3w6FStgKiOSOHFHXVeYclR6x/SPH1n20K+hUXCATw3nvv4fTTT4/7/Omnn461a9cCiK6Rv+SSS3DyySfja1/7WtL7u/nmm1FVVSU+7FDi0nN0ekeiWRZJAmos6H7w6ciJQsdK6x/U8LmduHjZNADA79bsyMv+q3CE4UDsRNWSoaMDAEdNl1cDpOrMHNlai5vPOdyywlULSZKEqzMYG9RGQqe4mNNciQcvWZryrCezaayMblMPhhm6hv3jvt4/EsQ/tyQvWwHRcxdfpPqJIqcTDEfQPcwzOiR0DNPV1YVwOIzGxvipp42NjejoiLa/vfnmm3j88cfx1FNPYeHChVi4cCE2bdqken833HAD+vv7xceePfnNR5iBV7EPSo2e4ajzUVPqUQ2WFTryZNl4odOr+L2syoVHT4XP7cDm/QNYu71b/xtMpnNwDOEIg8shmVIyOmq60tEpLOs6n/Chgbz70UMdV0QecDsdwmlJLF+919aLM+96HaPBMKbVlWL+pOSibLbKzquuIT8YA1wOKWfdk0axRDo1MbjHGBOfO+644xBJMkNGidfrhddrrxOw3mJPns/JZ9o/E3x8g3kgIYxswdbyRGrLPDhvyWT84a02/HbNjrgJpLmAn+waK32miOCJ1SWYXFuCPT2jWZ84bCWiYj0ol64sFp4n7ENLtQ8dA2No7xvFwsnVCEcYVr26DXf+ayvCEYaJ1SX43wsW6YblZzVW4Pl/d8R1XsllK69m2StfFPQrrr6+Hk6nU7g3nM7OznEuT7Hi0XF0umPOR6EpbKNwR2dcGHnEesMC1bjsuOlwSMCaTw/ioxxvNeczdNLdcaXGf31hHi49thXHzsitaCtkEh0dKl0R+YIPBt3XN4q9vSP4yu/ewu0vf4pwhOELC1vw/DXLMX9Ste79qO28KtQZOkCBCx2Px4PFixfj5Zdfjvv8yy+/nDR0XEyIxZ46jo4Vg8gAUOJRn4xs9YwOZ0pdKVbMawYA3Pf6jpz+7HYTpiIncuLsCfjZWXMt1/KfTXwio0NCh8gvvNX9mQ/bseLO17FuVy/KvS78f+cvwJ1fWYRKn7HzKe+82npgUASbOwu04wooAKEzNDSEjRs3YuPGjQCAnTt3YuPGjaJ9/LrrrsP999+PBx98EB999BGuvfZa7N69G5dffnkej7pw0HN0eoSjU3hPPiPodV1ZbVigGnwtxD827hcuSy7Yb9JUZCI53NHhYWQSgUS+4O7txj19GBwLYdGUajx31XKcvWhSSvczra4UHpcDI4Ew9sU6N+VhgYV3Psl7Rmf9+vU46aSTxL+vu+46AMDFF1+Mhx9+GOeffz66u7vxy1/+Eu3t7Zg3bx6ee+45TJ2a31a9QkHP0emKCR3LZnS0hI5NHB0AWDC5Gke11uKdnT146M1d+PEZc3Lyc4WjQ0Inq/CGAQojE/mGd0s5JODKkw7BVafMhCsNh9HldOCQhnJsaR/AJx2DmFxbqpihU3jnk7wLnRNPPFG3tfaKK67AFVdcYdrPXLVqFVatWoVwePyiSKvBT6JaYeSeoajQqbdo6UprBYTI6Fi460rJt0+Yjnd29uCv6/fkTujwjI6JpStiPPw5TKUrIt8cP7MB//WFw3D4pGosnFyd0X3NbqqICp0Dgzh1bqOc0SnAjsuifMWtXLkSW7Zswbp16/J9KBkjHB3NMDLvuiq8J58RZKGTuAIiVrqygaMDAEfGhu31jgQxEsjN/qv9sanIZszQIbThFyM0R4fIN06HhK8dMy1jkQPIOR3eYt7JN5cXoENMrziL4xGlK3VXTHRdWdTRUcvoKDeXW73rilPmcYp1F12Dgaz/vGA4gq6hwj0x2QkvhZEJG8J3XvEW8wODhVu6olecxdFtLx+yeHu5e3zX1aA/BD7B3MpzdJRIkoT62JLJg0Pjp5aaTedgdLiX21l4w73shhxGjmV0XJTRIawPd3S2HxzCkD8k4gSNBbb+ASChY3mSrYAIhiPC+cjFpuZsoObo9MdeUKUeZ9zSRKvDhU5XDoROR7989VVow73sBn+O0hwdwk5MrC5BmceJYJjh3Z3Rye5elwOVJXmP/o6DXnEWJ5mjw9ckOCTrlnjEZGSF0OkVreXW/J20yIfQaSpAm9lu+GhgIGFDJEnCrNjgwDWfdgGIXjjpTVXOB/SKszieJCsguobk1nKrXrXLu67k309e/2CvkktDRfT3yUVGR7SCUj4n63BHJxyrt9IcHcIu8J1Xa7YeBFCYwwIBEjqWJ5mjY/VhgYBie3lAdnT6yNHJmA7eWk6OTtbxJggbDzk6hE3gOZ0dB4cBFOb6B4CEjuVJNjBQbi23rvPBbX9l6cou6x8SyanQKeBWULvBRyRw3DQwkLAJfOcVpxCDyECRCp1Vq1Zh7ty5WLp0ab4PJWOSOTqi48qireWAHEYORZgoz8nDAknopAt3dEjoZJ9ER4cyOoRdSBQ6TVWFWT0oylecHQcGqmV0uKNj5fZh5dUw77yy21RkDp9ezbNV2YRndCiMnH14ezmHhA5hF+rLvXHvL4U4QwcoUqFjJ7xGMjoWbS0Hor8fD/HzQLJtMzqx0eldg9l1dBhjONBPpatc4UsYgUBhZMJO8JwOAEyg0hWRDZJldJRdV1ZFkiRF51XM0bF5RmfQHxq3xNRMeoYD4vlSqCcmOzHe0aGMDmEflOUr6roisoKc0Rm/AoI7OlZd6MlJDCRzR6eqxNq/VyKVPpfoyEklpxOOMNG6bARetqov95C7kAMSHR0qXRF2Is7RodIVkQ2Sdl0NWXuhJydxOrJdHZ3oGojUcjqRCMNZv3kDK+5cg5DGBvtExLBAKlvlhERHh8QlYSf4zqsKrwvl3sKbigwAhXlUhGH4STOo1nVl8YWeHLHBPDZLh6+AqLFZGBmI5nT2948Zzul0DfmxpX0AALC/bwxT6kp1v4eCyLklcU0JOTqEnVg4uQbnL5mMQ5sr9G+cJ0joWByPhqPjD4XFEkErd10BstAZC0XAGLOtowMg5cWe7TF3BgB294wYEzrk6OQUX6KjQ0KHsBFOh4RbvzQ/34eRFHrFWRy+CTmxvbx3OCoGXA4JlT5rCwI+HXk0EMagPyTyKFU267oCFC3mBh2d9tg8HCAqdIxAe65yCzk6BJFfivIVZ6uBgc7oSTSxvZyHWWssvOeKw6+I/aGwKFv53I5xE2ftQKpDA5WOTlvPsKHvEaWrqpIUj45Ih/EDA639eiQIq1GUQsdeAwOjJ83E0pW858raZStADiOPBsLysECbdVxxZKFjLIzcoRA6e8jRKUjGrYCgMDJB5BR6xVkcrRUQYiqyxYPIAODlQicYRt9obFigDfM5gDw0MN2MjhHkjI61u/GsAi31JIj8Qq84i6O1AkLsubJ4azmgbC+P2HbPFUduLzcmdJSOzu5ufaEz7A9h0B8NqVPpKjeMX+pJp12CyCX0irM4WisgeGu5lacic0qUjo5Y/2D930uNCSmugdivCCMPjIXE46MFz+eUF/DMC7uRODeH5ugQRG6hV5zFkR2d+Mm4PUP2mIoMKMLIwXARODpRoTMwFoI/lHwNRCTCcCAmXFyxwLle+Ypay3OP0yHFBZApjEwQuYWEjsXRz+jYp3QVzehEhU6VTYVOVYlbvBF26wSSu4cDCIYZHBJw2MQqACkIHQoi5xTlGgjK6BBEbqFXnMVRroBgTHZ17FS68qp0XdlxKjIQXQPBc1V6OR0uWhoqvJhRXwbAgNAZIEcnHyjXQFBGhyByC73iLI6y3q8sX3XbqHRVopiM3M+7rmw4LJBTX2EskMyHBTZVlWBybXQisl6LOTk6+UE5NJDaywkit9ArzuIobXDlLJ0e4ehYv3TlU5ujY9PSFaCYpTOYvHTFW8ubK32YEhM6bTqdV+2U0ckL8Y4OZXQIIpeQ0LE4cY5OLKczFgxjKNZCbIc5OiUeeTJyb6yrqMqmXVeA8X1XQuhU+8SOK73S1QFa6JkXvJTRIYi8UZSvODutgHA6JPAND9zR4fkct1NChQ1aiJWTkfttvNCTY3QNREesdNVc5cPUmKOzv2903EwlJeTo5AcfZXQIIm8U5SvOTisggPGdVz2KYYGSZH2bPG4yclGUrnhGx1jpqqmqBA0VXnhdDkRYVOyoEQxHRDceCZ3copyOTEKHIHILveJsgEfReQUAXbE3Mzt0XAGyo9M9FEAotrncrl1XQLSLCtAfGsg7qJqrfJAkSeR0tMpXnYN+MBZ1+mpt/PgVInFhZMroEEROIaFjA7ijw0sWwtGxQT4HkMPInYPRN3avy56byzlGSleMMdnRieVt9ALJvNTVWOmz/EZ7q8FLVx6nwxYuK0FYCRI6NkA4OiGe0YkNC7SZoxMzc2xdtgKMCZ2e4QACoQgkKSpcAOi2mHf0x8pWFETOOdzRITeHIHIPCR0b4E5wdHgY2Q5TkQFZ6HDsuueKwzM6vSNBzWAxd3Pqy73C0Zuq03klz90hoZNreEaHZugQRO6hV50N4I6Onzs6Q/aZigzEd6wA9l3/wKkp9cAZKy3xeUiJ8MF/zQrRopfRodby/MFLrRREJojcQ686G5C42JO/OdphKjIA+Dzxjk6NzYWOwyEJkXpQI5DcriJahNDpHolbByK+h1rL8wZ3dGiGDkHkHnrV2YDE9vLuId51ZY/SlXIhImD/0hWgn9Np75Nn6HB4RmfQHxJt+EoO0J6rvCE7OpTRIYhcQ0LHBnicWhkdewgCt1MSpRzA/mFkQHbjtBwdUbqqLhGf87mdaKyMCiS18lVilxaRO0RGhxwdgsg59KqzAeMdHT4w0B5CR5Ik+BQhTrtndACgQTg66hmddpWMDqCd02GMoXOAhgXmC77rykNhZILIOfSqswHcDg+EIxgJhDAaDAOwT9cVAJQocjpFUbqqSF666tAIFk/WEDo9wwExUHJCBQmdXENhZILIH/SqswFKR4e7OR6XA2Ue+wzVUw4ItHsYGVCugRgvdKLDAnlGpyTua8pAshK1dnQid1AYmSDyR1G+6uy01BOQrxIDoYjccVXmsdUEVqXQKYbSVbIwct9IEGPBqDvTWBXv2mnN0pGDyPZx+axEVUn0OVvus/6SXYKwGkUpdOy61FO5tLHWJkFkjnJoYFGUrrjQGRyf0ZHdGU/cDiVAO6NDQeT8cuLsCbjutFn43umz8n0oBFF00OWFDVCugOhWbC63E8qhgcXQddWQJKPTMaA94ZhndNr7RxEIRYQIptby/OJzO3HVKTPzfRgEUZQUpaNjN+IdHXu1lnOUpatiEDrc0ekZCSCUsAZCdmdKxn1fQ7kXPrcDEQbsj83aif8eEjoEQRQXJHRsgFgBEZYzOnZpLefw0pXH5Ri3+8qO1JZ54JAAxqJiR4na+geOJEnyFnNF+Up2dMaLI4IgCDtDQscGiKWeISZKHXZqLQdkR6e6xG2rkLUWTsUaiMSczv6+5GWoKbVlAOJzOuToEARRrJDQsQEioxMOC0fHLgs9OdzFKYayFUer84pndNQcHUAOJO9ROjq054ogiCKFhI4N8CgcHR5GtstCTw4PIxdDxxVHS+jIU5HVy1BTaqOf57N0hvwhDPpDAEjoEARRfJDQsQGyoxNRODo2K13Fhh8WwwwdjtrQQMZY0owOAEypi8/o8NuXe10o91KjJUEQxQUJHRsgVkCEInJGx2alqzJP9A26ttRev1cy6lX2XQ2MhTASiK740Mvo7OkZAWOMWssJgihq6PLOBnhiQ+P6RgPwxxZ72q29/Mz5zfhwbz++cuTkfB9KzhD7rhQbzPnqh5pSd1zLvZJJNdHS1ZA/hN6RIAWRCYIoakjo2ADu6PA3tBK3E6Uee/1ppzeU4/6Ll+T7MHIKd3QODimFjn6buM/tRFOlDx0DY9jdM0KODkEQRQ2VrmwADyPzLIbdOq6KFTmjI5eu+N+4RUe0KFdBcBeIHB2CIIoREjo2gIeReXbDbh1XxYpa11W7wTbxyWKL+TA6+v2GvocgCMKOkNCxAdzR4ZCjYw/4vque4QAiEQYA6OhPPkOHo9xiLnZjkaNDEEQRQkLHBrid8X9Gu01FLla4YA1HGHpjayCMZHSA+NIVOToEQRQzRSl0Vq1ahblz52Lp0qX5PhRTSHR07NZaXqy4nQ7UxOYG8ZxOu84MHQ4vXW0/OIzuYRI6BEEUL0UpdFauXIktW7Zg3bp1+T4UUxgndCijYxsSczp6wwI53NE5OOgHY9HOvGKaQUQQBMEpSqFjNzzOxIwOla7sgmgxH/RjcCyIIYOrHOrLPSj1yHN2Git9cDjsvwyVIAgiERI6NoAcHfsihgYO+YWbU1Xi1p2TJEmScHUACiITBFG8kNCxAePCyJTRsQ18VMDBIT/2GyxbcSYrhQ7lcwiCKFJI6NiA8Y4Ola7sgsjoDAZEa7lR0UKODkEQBAkdW8BXQHDI0bEPDYowstxxlby1nDOFHB2CIAgSOnbA65RDp2Uep+ayR8J6NKhkdIyWrqbUkdAhCIIgoWMD3C7Z0aGylb2oV3F0qHRFEARhHBI6NkDZXk7rH+xFfUX079k9FMD+PmPrHziTakrgjLWUt1QbK3cRBEHYjeQ9qoQlcDokSBLAGC30tBt1sZlIoQjDzq5hAMaFjtflxM8/fxh6hwMkdAiCKFpI6NgASZLgcTrgD0XI0bEZHpcDVSVu9I8GEYot9tTbc6Xka0dPzdahEQRBWAIqXdkEXr6ijI79ULp0FT4Xyr10fUIQBGEUEjo2gc/SodZy+1GvEK9Gy1YEQRBEFBI6NsEtHB0SOnaDr4EAUitbEQRBECR0bAN3dGihp/1oUDo61CZOEASREiR0bMKpcxoxqaYECyZV5ftQCJNRZnSaq0noEARBpAKlGm3Cz86ai/88cw4kSdK/MWEpKKNDEASRPuTo2AgSOfZEKXQoo0MQBJEaJHQIosBRhpHJ0SEIgkgNEjoEUeAoMzq0nJMgCCI1ijKjs2rVKqxatQrhcDjfh0IQurRUleC4Q+pRWeJCpc+d78MhCIKwFBJjjOX7IPLFwMAAqqqq0N/fj8rKynwfDkEQBEEQBkjl/ZtKVwRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BYSOgRBEARB2BZXvg8gnzDGAETXvRMEQRAEYQ34+zZ/H09GUQudwcFBAMDkyZPzfCQEQRAEQaTK4OAgqqqqkt5GYkbkkE2JRCLYv38/KioqIEmSqfc9MDCAyZMnY8+ePaisrDT1vq0MPS7a0GOjDj0u2tBjow49LtrY5bFhjGFwcBAtLS1wOJKncIra0XE4HJg0aVJWf0ZlZaWln0zZgh4XbeixUYceF23osVGHHhdt7PDY6Dk5HAojEwRBEARhW0joEARBEARhW0joZAmv14sbb7wRXq8334dSUNDjog09NurQ46INPTbq0OOiTTE+NkUdRiYIgiAIwt6Qo0MQBEEQhG0hoUMQBEEQhG0hoUMQBEEQhG0hoUMQBEEQhG0hoZMF7r77brS2tsLn82Hx4sV4/fXX831IOWfNmjU466yz0NLSAkmS8NRTT8V9nTGGn//852hpaUFJSQlOPPFEbN68OT8Hm0NuvvlmLF26FBUVFZgwYQK++MUv4pNPPom7TbE+Nvfccw/mz58vBpkdc8wxeP7558XXi/VxSeTmm2+GJEm45pprxOeK8bH5+c9/DkmS4j6amprE14vxMVGyb98+XHjhhairq0NpaSkWLlyI9957T3y9mB4fEjom8/jjj+Oaa67BT37yE2zYsAHLly/HihUrsHv37nwfWk4ZHh7GggULcNddd6l+/de//jVuv/123HXXXVi3bh2amppw2mmnif1jdmX16tVYuXIl3n77bbz88ssIhUI4/fTTMTw8LG5TrI/NpEmTcMstt2D9+vVYv349Tj75ZHzhC18QJ99ifVyUrFu3Dr/73e8wf/78uM8X62Nz2GGHob29XXxs2rRJfK1YHxMA6O3txbHHHgu3243nn38eW7ZswW233Ybq6mpxm6J6fBhhKkceeSS7/PLL4z536KGHsuuvvz5PR5R/ALAnn3xS/DsSibCmpiZ2yy23iM+NjY2xqqoqdu+99+bhCPNHZ2cnA8BWr17NGKPHJpGamhp2//330+PCGBscHGQzZ85kL7/8MjvhhBPY1VdfzRgr3ufMjTfeyBYsWKD6tWJ9TDg/+tGP2HHHHaf59WJ7fMjRMZFAIID33nsPp59+etznTz/9dKxduzZPR1V47Ny5Ex0dHXGPk9frxQknnFB0j1N/fz8AoLa2FgA9NpxwOIzHHnsMw8PDOOaYY+hxAbBy5Up87nOfw6mnnhr3+WJ+bLZu3YqWlha0trbiK1/5Cnbs2AGguB8TAPjHP/6BJUuW4Mtf/jImTJiARYsW4b777hNfL7bHh4SOiXR1dSEcDqOxsTHu842Njejo6MjTURUe/LEo9seJMYbrrrsOxx13HObNmweAHptNmzahvLwcXq8Xl19+OZ588knMnTu36B+Xxx57DO+//z5uvvnmcV8r1sfmqKOOwh/+8Ae8+OKLuO+++9DR0YFly5ahu7u7aB8Tzo4dO3DPPfdg5syZePHFF3H55Zfjqquuwh/+8AcAxfecKert5dlCkqS4fzPGxn2OoMfpyiuvxIcffog33nhj3NeK9bGZPXs2Nm7ciL6+Pvztb3/DxRdfjNWrV4uvF+PjsmfPHlx99dV46aWX4PP5NG9XbI/NihUrxP8ffvjhOOaYYzBjxgz8/ve/x9FHHw2g+B4TTiQSwZIlS/CrX/0KALBo0SJs3rwZ99xzDy666CJxu2J5fMjRMZH6+no4nc5xirizs3Occi5meGdEMT9O3/3ud/GPf/wDr776KiZNmiQ+X+yPjcfjwSGHHIIlS5bg5ptvxoIFC3DnnXcW9ePy3nvvobOzE4sXL4bL5YLL5cLq1avxv//7v3C5XOL3L8bHRklZWRkOP/xwbN26taifLwDQ3NyMuXPnxn1uzpw5oimm2B4fEjom4vF4sHjxYrz88stxn3/55ZexbNmyPB1V4dHa2oqmpqa4xykQCGD16tW2f5wYY7jyyivxxBNP4JVXXkFra2vc14v5sVGDMQa/31/Uj8spp5yCTZs2YePGjeJjyZIl+OpXv4qNGzdi+vTpRfvYKPH7/fjoo4/Q3Nxc1M8XADj22GPHja349NNPMXXqVABFeJ7JVwrarjz22GPM7XazBx54gG3ZsoVdc801rKysjO3atSvfh5ZTBgcH2YYNG9iGDRsYAHb77bezDRs2sLa2NsYYY7fccgurqqpiTzzxBNu0aRO74IILWHNzMxsYGMjzkWeX73znO6yqqoq99tprrL29XXyMjIyI2xTrY3PDDTewNWvWsJ07d7IPP/yQ/fjHP2YOh4O99NJLjLHifVzUUHZdMVacj833vvc99tprr7EdO3awt99+m5155pmsoqJCnGuL8THhvPvuu8zlcrH/9//+H9u6dSt75JFHWGlpKfvTn/4kblNMjw8JnSywatUqNnXqVObxeNgRRxwhWoeLiVdffZUBGPdx8cUXM8ai7Y033ngja2pqYl6vlx1//PFs06ZN+T3oHKD2mABgDz30kLhNsT42l156qXjdNDQ0sFNOOUWIHMaK93FRI1HoFONjc/7557Pm5mbmdrtZS0sLO+ecc9jmzZvF14vxMVHy9NNPs3nz5jGv18sOPfRQ9rvf/S7u68X0+EiMMZYfL4kgCIIgCCK7UEaHIAiCIAjbQkKHIAiCIAjbQkKHIAiCIAjbQkKHIAiCIAjbQkKHIAiCIAjbQkKHIAiCIAjbQkKHIAiCIAjbQkKHIIicwhjDt771LdTW1kKSJGzcuDHfh0QQhI2hgYEEQeSU559/Hl/4whfw2muvYfr06aivr4fL5croPi+55BL09fXhqaeeMucgCYKwDZmdXQiCIFJk+/btaG5uLsjlgeFwGJIkweEgs5sg7AK9mgmCyBmXXHIJvvvd72L37t2QJAnTpk0DYwy//vWvMX36dJSUlGDBggX4v//7P/E94XAY3/jGN9Da2oqSkhLMnj0bd955p/j6z3/+c/z+97/H3//+d0iSBEmS8Nprr+G1116DJEno6+sTt924cSMkScKuXbsAAA8//DCqq6vxzDPPYO7cufB6vWhra0MgEMAPf/hDTJw4EWVlZTjqqKPw2muviftpa2vDWWedhZqaGpSVleGwww7Dc889l+2HjyCINCBHhyCInHHnnXdixowZ+N3vfod169bB6XTipz/9KZ544gncc889mDlzJtasWYMLL7wQDQ0NOOGEExCJRDBp0iT85S9/QX19PdauXYtvfetbaG5uxnnnnYfvf//7+OijjzAwMICHHnoIAFBbW4u1a9caOqaRkRHcfPPNuP/++1FXV4cJEybg61//Onbt2oXHHnsMLS0tePLJJ/HZz34WmzZtwsyZM7Fy5UoEAgGsWbMGZWVl2LJlC8rLy7P50BEEkSYkdAiCyBlVVVWoqKiA0+lEU1MThoeHcfvtt+OVV17BMcccAwCYPn063njjDfz2t7/FCSecALfbjV/84hfiPlpbW7F27Vr85S9/wXnnnYfy8nKUlJTA7/ejqakp5WMKBoO4++67sWDBAgDR0tqjjz6KvXv3oqWlBQDw/e9/Hy+88AIeeugh/OpXv8Lu3btx7rnn4vDDDxfHTBBEYUJChyCIvLFlyxaMjY3htNNOi/t8IBDAokWLxL/vvfde3H///Whra8Po6CgCgQAWLlxoyjF4PB7Mnz9f/Pv9998HYwyzZs2Ku53f70ddXR0A4KqrrsJ3vvMdvPTSSzj11FNx7rnnxt0HQRCFAwkdgiDyRiQSAQA8++yzmDhxYtzXvF4vAOAvf/kLrr32Wtx222045phjUFFRgf/+7//GO++8k/S+eaBY2VgaDAbH3a6kpASSJMUdk9PpxHvvvQen0xl3W16euuyyy/CZz3wGzz77LF566SXcfPPNuO222/Dd737X6K9OEESOIKFDEETe4AHg3bt344QTTlC9zeuvv45ly5bhiiuuEJ/bvn173G08Hg/C4XDc5xoaGgAA7e3tqKmpAQBDM3sWLVqEcDiMzs5OLF++XPN2kydPxuWXX47LL78cN9xwA+677z4SOgRRgJDQIQgib1RUVOD73/8+rr32WkQiERx33HEYGBjA2rVrUV5ejosvvhiHHHII/vCHP+DFF19Ea2sr/vjHP2LdunVobW0V9zNt2jS8+OKL+OSTT1BXV4eqqioccsghmDx5Mn7+85/jpptuwtatW3HbbbfpHtOsWbPw1a9+FRdddBFuu+02LFq0CF1dXXjllVdw+OGH44wzzsA111yDFStWYNasWejt7cUrr7yCOXPmZPOhIggiTai9nCCIvPJf//Vf+NnPfoabb74Zc+bMwWc+8xk8/fTTQshcfvnlOOecc3D++efjqKOOQnd3d5y7AwDf/OY3MXv2bCxZsgQNDQ1488034Xa78eijj+Ljjz/GggULcOutt+Kmm24ydEwPPfQQLrroInzve9/D7Nmz8fnPfx7vvPMOJk+eDCDa8r5y5UrMmTMHn/3sZzF79mzcfffd5j4wBEGYAk1GJgiCIAjCtpCjQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbSGhQxAEQRCEbfn/ATrjtSPyOjyrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cur_mase[cur_mase <= np.percentile(cur_mase, 100)])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"50% best MASE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cur_mape = mape(y_true, y_pred)\n",
    "plt.plot(cur_mape[cur_mape < np.percentile(cur_mape, 50)])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.title(\"50% best MAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cur_mape[cur_mape <= np.percentile(cur_mape,100)])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.title(\"MAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 67), dtype=float32, numpy=\n",
       "array([[ 8.25200696e-04, -2.52360152e-03,  1.24429585e-03,\n",
       "         5.61252143e-03,  2.41480279e-03,  6.91283401e-03,\n",
       "        -1.55929942e-04,  5.05245430e-03, -3.45375156e-03,\n",
       "         2.31756223e-03, -5.02947718e-03,  5.21133933e-03,\n",
       "         2.63487943e-03, -1.15680450e-03, -8.61713605e-04,\n",
       "        -4.27721906e-03,  1.65054473e-04,  1.98029215e-03,\n",
       "         1.16372411e-03, -6.18001819e-03,  1.38662616e-03,\n",
       "        -3.87507980e-03, -5.78831602e-03, -4.70064487e-03,\n",
       "        -4.91938740e-03, -9.45061352e-03, -6.37044199e-03,\n",
       "        -1.24200690e-03, -2.50279880e-03,  6.26629405e-03,\n",
       "         7.07075465e-03,  4.37504100e-03,  8.11321661e-06,\n",
       "         6.27311785e-03, -2.45433138e-03,  6.55709300e-04,\n",
       "         1.01655931e-03,  3.92863574e-03, -7.48858880e-03,\n",
       "        -3.09383660e-03, -3.60715576e-03,  3.58439516e-04,\n",
       "         5.01208124e-04,  4.94992966e-03,  4.09077760e-03,\n",
       "        -1.00721011e-03, -6.26226794e-03, -6.27841754e-03,\n",
       "         5.10353828e-03,  4.80458234e-03, -2.42970185e-03,\n",
       "         3.23574082e-03,  4.15166654e-03,  1.97961787e-03,\n",
       "         2.42814887e-03, -2.23055086e-03, -2.55294633e-03,\n",
       "        -3.07056517e-03, -7.46782962e-03,  4.13405150e-03,\n",
       "         4.82963771e-03,  6.89104025e-04, -1.09297456e-04,\n",
       "         3.91600234e-03, -8.97214864e-04, -1.08612701e-03,\n",
       "         4.65779193e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(np.zeros(67 * 10).reshape(1, 10, 67))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters_labels.shape=(408095,)\n",
      "N_clusters=2, 2, 22, (30608, 67)\n",
      "Before prediction: train_X.shape=(18358, 10, 67), train_y.shape=(18358, 67), test_X.shape=(6119, 10, 67), test_y.shape=(6119, 67)\n",
      "Epoch 1/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.3112 - val_loss: 0.3270\n",
      "Epoch 2/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2915 - val_loss: 0.3111\n",
      "Epoch 3/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2761 - val_loss: 0.2987\n",
      "Epoch 4/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2646 - val_loss: 0.2898\n",
      "Epoch 5/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2564 - val_loss: 0.2830\n",
      "Epoch 6/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2504 - val_loss: 0.2777\n",
      "Epoch 7/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2456 - val_loss: 0.2732\n",
      "Epoch 8/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2415 - val_loss: 0.2694\n",
      "Epoch 9/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2378 - val_loss: 0.2660\n",
      "Epoch 10/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2345 - val_loss: 0.2630\n",
      "Epoch 11/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2314 - val_loss: 0.2603\n",
      "Epoch 12/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2286 - val_loss: 0.2579\n",
      "Epoch 13/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2262 - val_loss: 0.2559\n",
      "Epoch 14/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2241 - val_loss: 0.2541\n",
      "Epoch 15/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2222 - val_loss: 0.2526\n",
      "Epoch 16/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2206 - val_loss: 0.2513\n",
      "Epoch 17/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2191 - val_loss: 0.2500\n",
      "Epoch 18/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2179 - val_loss: 0.2489\n",
      "Epoch 19/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2167 - val_loss: 0.2478\n",
      "Epoch 20/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2156 - val_loss: 0.2470\n",
      "Epoch 21/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2147 - val_loss: 0.2461\n",
      "Epoch 22/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2138 - val_loss: 0.2453\n",
      "Epoch 23/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2129 - val_loss: 0.2444\n",
      "Epoch 24/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2122 - val_loss: 0.2439\n",
      "Epoch 25/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2115 - val_loss: 0.2432\n",
      "Epoch 26/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2109 - val_loss: 0.2427\n",
      "Epoch 27/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.2103 - val_loss: 0.2421\n",
      "Epoch 28/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2097 - val_loss: 0.2416\n",
      "Epoch 29/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.2092 - val_loss: 0.2410\n",
      "Epoch 30/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.2087 - val_loss: 0.2407\n",
      "Epoch 31/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2082 - val_loss: 0.2401\n",
      "Epoch 32/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2078 - val_loss: 0.2397\n",
      "Epoch 33/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2073 - val_loss: 0.2394\n",
      "Epoch 34/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2069 - val_loss: 0.2390\n",
      "Epoch 35/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2065 - val_loss: 0.2386\n",
      "Epoch 36/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2062 - val_loss: 0.2382\n",
      "Epoch 37/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2058 - val_loss: 0.2380\n",
      "Epoch 38/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2055 - val_loss: 0.2376\n",
      "Epoch 39/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.2052 - val_loss: 0.2373\n",
      "Epoch 40/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2049 - val_loss: 0.2370\n",
      "192/192 [==============================] - 2s 10ms/step\n",
      "predicted_original.shape=(6119, 67), test_y.shape=(6119, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1578.0531622950014, my average MASE = 2807.2183134542065\n",
      "Cluster 0, 1578.0531622950014\n",
      "Before prediction: train_X.shape=(8, 10, 67), train_y.shape=(8, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4317 - val_loss: 1.2280\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4299 - val_loss: 1.2276\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4282 - val_loss: 1.2273\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4264 - val_loss: 1.2269\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4247 - val_loss: 1.2266\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4231 - val_loss: 1.2263\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4214 - val_loss: 1.2259\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4198 - val_loss: 1.2256\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4182 - val_loss: 1.2253\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4166 - val_loss: 1.2251\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4151 - val_loss: 1.2248\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4135 - val_loss: 1.2245\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4120 - val_loss: 1.2242\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4105 - val_loss: 1.2239\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4089 - val_loss: 1.2236\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4074 - val_loss: 1.2233\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4060 - val_loss: 1.2229\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4045 - val_loss: 1.2226\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4031 - val_loss: 1.2223\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4017 - val_loss: 1.2220\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4004 - val_loss: 1.2216\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3990 - val_loss: 1.2213\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3977 - val_loss: 1.2210\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3964 - val_loss: 1.2207\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3951 - val_loss: 1.2204\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3938 - val_loss: 1.2201\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3926 - val_loss: 1.2198\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3914 - val_loss: 1.2195\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3901 - val_loss: 1.2192\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3889 - val_loss: 1.2189\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3877 - val_loss: 1.2186\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3866 - val_loss: 1.2184\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3854 - val_loss: 1.2181\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3842 - val_loss: 1.2178\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3831 - val_loss: 1.2175\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3820 - val_loss: 1.2173\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3809 - val_loss: 1.2170\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3798 - val_loss: 1.2167\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3787 - val_loss: 1.2165\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3776 - val_loss: 1.2162\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 708619678.5307182, my average MASE = 1911068024.4320867\n",
      "Cluster 1, 708619678.5307182\n",
      "clusters_labels.shape=(408095,)\n",
      "N_clusters=5, 5, 1026, (269, 67)\n",
      "Before prediction: train_X.shape=(155, 10, 67), train_y.shape=(155, 67), test_X.shape=(52, 10, 67), test_y.shape=(52, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5683 - val_loss: 0.4789\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5669 - val_loss: 0.4779\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5656 - val_loss: 0.4770\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5643 - val_loss: 0.4760\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5631 - val_loss: 0.4751\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5619 - val_loss: 0.4743\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5607 - val_loss: 0.4734\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5595 - val_loss: 0.4726\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5584 - val_loss: 0.4718\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5573 - val_loss: 0.4710\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5562 - val_loss: 0.4702\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5552 - val_loss: 0.4694\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5542 - val_loss: 0.4687\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5532 - val_loss: 0.4679\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5522 - val_loss: 0.4672\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5512 - val_loss: 0.4665\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5502 - val_loss: 0.4658\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5493 - val_loss: 0.4651\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5483 - val_loss: 0.4644\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5474 - val_loss: 0.4637\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5465 - val_loss: 0.4630\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5456 - val_loss: 0.4624\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5447 - val_loss: 0.4617\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5438 - val_loss: 0.4611\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5429 - val_loss: 0.4605\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5421 - val_loss: 0.4598\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5412 - val_loss: 0.4592\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5404 - val_loss: 0.4586\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5396 - val_loss: 0.4579\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5388 - val_loss: 0.4573\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5379 - val_loss: 0.4567\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5371 - val_loss: 0.4561\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5363 - val_loss: 0.4555\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5355 - val_loss: 0.4549\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5348 - val_loss: 0.4543\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5340 - val_loss: 0.4538\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5332 - val_loss: 0.4532\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5324 - val_loss: 0.4526\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5317 - val_loss: 0.4521\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5309 - val_loss: 0.4515\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(52, 67), test_y.shape=(52, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 139.40868807431647, my average MASE = 112470179.11258063\n",
      "Cluster 0, 139.40868807431647\n",
      "Before prediction: train_X.shape=(31, 10, 67), train_y.shape=(31, 67), test_X.shape=(10, 10, 67), test_y.shape=(10, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3364 - val_loss: 0.3469\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3360 - val_loss: 0.3468\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3355 - val_loss: 0.3468\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3350 - val_loss: 0.3467\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3346 - val_loss: 0.3466\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3341 - val_loss: 0.3465\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3337 - val_loss: 0.3464\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3332 - val_loss: 0.3463\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3328 - val_loss: 0.3463\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3323 - val_loss: 0.3462\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3319 - val_loss: 0.3461\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3315 - val_loss: 0.3460\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3310 - val_loss: 0.3459\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3306 - val_loss: 0.3458\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3302 - val_loss: 0.3457\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3298 - val_loss: 0.3457\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3294 - val_loss: 0.3456\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3290 - val_loss: 0.3455\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3286 - val_loss: 0.3454\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3282 - val_loss: 0.3453\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3278 - val_loss: 0.3453\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3274 - val_loss: 0.3452\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3270 - val_loss: 0.3451\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3267 - val_loss: 0.3450\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3263 - val_loss: 0.3450\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3259 - val_loss: 0.3449\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3255 - val_loss: 0.3448\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3252 - val_loss: 0.3448\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3248 - val_loss: 0.3447\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3245 - val_loss: 0.3446\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3242 - val_loss: 0.3445\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3238 - val_loss: 0.3445\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3235 - val_loss: 0.3444\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3231 - val_loss: 0.3443\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3228 - val_loss: 0.3443\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3225 - val_loss: 0.3442\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3222 - val_loss: 0.3441\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3219 - val_loss: 0.3440\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3216 - val_loss: 0.3440\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3213 - val_loss: 0.3439\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "predicted_original.shape=(10, 67), test_y.shape=(10, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3523641.3982873675, my average MASE = 66808534.66599488\n",
      "Cluster 1, 3523641.3982873675\n",
      "Before prediction: train_X.shape=(1939, 10, 67), train_y.shape=(1939, 67), test_X.shape=(646, 10, 67), test_y.shape=(646, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.1130 - val_loss: 0.1021\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1076 - val_loss: 0.1000\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1032 - val_loss: 0.0984\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0995 - val_loss: 0.0974\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0964 - val_loss: 0.0968\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0936 - val_loss: 0.0962\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0912 - val_loss: 0.0958\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0889 - val_loss: 0.0954\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0869 - val_loss: 0.0951\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0851 - val_loss: 0.0948\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0834 - val_loss: 0.0945\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0819 - val_loss: 0.0942\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0804 - val_loss: 0.0940\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0791 - val_loss: 0.0938\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0779 - val_loss: 0.0936\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0768 - val_loss: 0.0934\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0757 - val_loss: 0.0933\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0748 - val_loss: 0.0931\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0739 - val_loss: 0.0930\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0730 - val_loss: 0.0929\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0722 - val_loss: 0.0927\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0715 - val_loss: 0.0926\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0707 - val_loss: 0.0925\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0701 - val_loss: 0.0924\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0694 - val_loss: 0.0923\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0688 - val_loss: 0.0922\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0682 - val_loss: 0.0922\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0677 - val_loss: 0.0922\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0672 - val_loss: 0.0921\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0668 - val_loss: 0.0921\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0663 - val_loss: 0.0920\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0659 - val_loss: 0.0919\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0656 - val_loss: 0.0919\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0652 - val_loss: 0.0918\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0649 - val_loss: 0.0918\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0646 - val_loss: 0.0917\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0643 - val_loss: 0.0917\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0640 - val_loss: 0.0916\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0638 - val_loss: 0.0916\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0635 - val_loss: 0.0915\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(646, 67), test_y.shape=(646, 67)\n",
      "average MASE = 869790828.2820631, my average MASE = 17262168691.962505\n",
      "Cluster 2, 869790828.2820631\n",
      "Before prediction: train_X.shape=(2219, 10, 67), train_y.shape=(2219, 67), test_X.shape=(740, 10, 67), test_y.shape=(740, 67)\n",
      "Epoch 1/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5462 - val_loss: 0.4045\n",
      "Epoch 2/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5391 - val_loss: 0.4010\n",
      "Epoch 3/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5333 - val_loss: 0.3979\n",
      "Epoch 4/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5281 - val_loss: 0.3951\n",
      "Epoch 5/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5234 - val_loss: 0.3925\n",
      "Epoch 6/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.5190 - val_loss: 0.3900\n",
      "Epoch 7/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.5147 - val_loss: 0.3877\n",
      "Epoch 8/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.5107 - val_loss: 0.3856\n",
      "Epoch 9/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.5068 - val_loss: 0.3835\n",
      "Epoch 10/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.5031 - val_loss: 0.3816\n",
      "Epoch 11/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4996 - val_loss: 0.3797\n",
      "Epoch 12/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4961 - val_loss: 0.3780\n",
      "Epoch 13/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4928 - val_loss: 0.3763\n",
      "Epoch 14/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4896 - val_loss: 0.3747\n",
      "Epoch 15/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4865 - val_loss: 0.3732\n",
      "Epoch 16/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4835 - val_loss: 0.3717\n",
      "Epoch 17/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4806 - val_loss: 0.3704\n",
      "Epoch 18/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4778 - val_loss: 0.3691\n",
      "Epoch 19/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4750 - val_loss: 0.3678\n",
      "Epoch 20/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4723 - val_loss: 0.3666\n",
      "Epoch 21/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4697 - val_loss: 0.3655\n",
      "Epoch 22/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4672 - val_loss: 0.3644\n",
      "Epoch 23/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4647 - val_loss: 0.3633\n",
      "Epoch 24/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4623 - val_loss: 0.3624\n",
      "Epoch 25/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4600 - val_loss: 0.3614\n",
      "Epoch 26/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4577 - val_loss: 0.3605\n",
      "Epoch 27/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4556 - val_loss: 0.3596\n",
      "Epoch 28/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4535 - val_loss: 0.3588\n",
      "Epoch 29/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4515 - val_loss: 0.3580\n",
      "Epoch 30/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4495 - val_loss: 0.3572\n",
      "Epoch 31/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4476 - val_loss: 0.3564\n",
      "Epoch 32/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4458 - val_loss: 0.3557\n",
      "Epoch 33/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4440 - val_loss: 0.3551\n",
      "Epoch 34/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4423 - val_loss: 0.3544\n",
      "Epoch 35/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4406 - val_loss: 0.3538\n",
      "Epoch 36/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4390 - val_loss: 0.3532\n",
      "Epoch 37/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4374 - val_loss: 0.3525\n",
      "Epoch 38/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4359 - val_loss: 0.3519\n",
      "Epoch 39/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4343 - val_loss: 0.3514\n",
      "Epoch 40/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4329 - val_loss: 0.3508\n",
      "24/24 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(740, 67), test_y.shape=(740, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 408.78828000932964, my average MASE = 1218.6998180420371\n",
      "Cluster 3, 408.78828000932964\n",
      "Before prediction: train_X.shape=(32, 10, 67), train_y.shape=(32, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5668 - val_loss: 0.4468\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5648 - val_loss: 0.4459\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5628 - val_loss: 0.4451\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5609 - val_loss: 0.4443\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5589 - val_loss: 0.4435\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5570 - val_loss: 0.4427\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5551 - val_loss: 0.4419\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5533 - val_loss: 0.4412\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5514 - val_loss: 0.4404\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5496 - val_loss: 0.4397\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5478 - val_loss: 0.4389\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5460 - val_loss: 0.4382\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5442 - val_loss: 0.4374\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5425 - val_loss: 0.4367\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5408 - val_loss: 0.4359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5391 - val_loss: 0.4352\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5374 - val_loss: 0.4344\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5358 - val_loss: 0.4337\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5342 - val_loss: 0.4329\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5326 - val_loss: 0.4322\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5310 - val_loss: 0.4315\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5295 - val_loss: 0.4308\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5280 - val_loss: 0.4300\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5264 - val_loss: 0.4293\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5250 - val_loss: 0.4286\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5235 - val_loss: 0.4279\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5221 - val_loss: 0.4273\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5207 - val_loss: 0.4266\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5193 - val_loss: 0.4260\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5179 - val_loss: 0.4254\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5165 - val_loss: 0.4248\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5151 - val_loss: 0.4241\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5138 - val_loss: 0.4235\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5125 - val_loss: 0.4229\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5112 - val_loss: 0.4223\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5099 - val_loss: 0.4217\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5086 - val_loss: 0.4212\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5073 - val_loss: 0.4206\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5061 - val_loss: 0.4200\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5048 - val_loss: 0.4194\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2631402082.882854, my average MASE = 5288471031.0194235\n",
      "Cluster 4, 2631402082.882854\n",
      "clusters_labels.shape=(408095,)\n",
      "N_clusters=7, 7, 17, (3243, 67)\n",
      "Before prediction: train_X.shape=(1939, 10, 67), train_y.shape=(1939, 67), test_X.shape=(646, 10, 67), test_y.shape=(646, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1149 - val_loss: 0.1029\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1092 - val_loss: 0.1008\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1047 - val_loss: 0.0992\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1009 - val_loss: 0.0980\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0947 - val_loss: 0.0965\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0922 - val_loss: 0.0960\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0899 - val_loss: 0.0955\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0878 - val_loss: 0.0952\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0859 - val_loss: 0.0948\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0842 - val_loss: 0.0945\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0826 - val_loss: 0.0942\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0812 - val_loss: 0.0939\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0799 - val_loss: 0.0936\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0786 - val_loss: 0.0934\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0775 - val_loss: 0.0931\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0765 - val_loss: 0.0929\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0755 - val_loss: 0.0928\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0746 - val_loss: 0.0926\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0737 - val_loss: 0.0924\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0729 - val_loss: 0.0923\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0721 - val_loss: 0.0922\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0714 - val_loss: 0.0921\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0708 - val_loss: 0.0920\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0702 - val_loss: 0.0919\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0696 - val_loss: 0.0918\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0690 - val_loss: 0.0917\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0685 - val_loss: 0.0916\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0680 - val_loss: 0.0916\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0675 - val_loss: 0.0915\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0670 - val_loss: 0.0915\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0666 - val_loss: 0.0915\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0662 - val_loss: 0.0914\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0658 - val_loss: 0.0914\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0654 - val_loss: 0.0913\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0651 - val_loss: 0.0913\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0648 - val_loss: 0.0912\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0645 - val_loss: 0.0912\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0642 - val_loss: 0.0911\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0639 - val_loss: 0.0911\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(646, 67), test_y.shape=(646, 67)\n",
      "average MASE = 1397145651.915293, my average MASE = 8779028319.064796\n",
      "Cluster 0, 1397145651.915293\n",
      "Before prediction: train_X.shape=(77, 10, 67), train_y.shape=(77, 67), test_X.shape=(26, 10, 67), test_y.shape=(26, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4446 - val_loss: 0.4248\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.4440 - val_loss: 0.4245\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4434 - val_loss: 0.4242\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4429 - val_loss: 0.4239\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4423 - val_loss: 0.4236\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4418 - val_loss: 0.4234\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4413 - val_loss: 0.4231\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4408 - val_loss: 0.4228\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4403 - val_loss: 0.4226\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4398 - val_loss: 0.4223\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4393 - val_loss: 0.4221\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4388 - val_loss: 0.4219\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4383 - val_loss: 0.4216\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4378 - val_loss: 0.4214\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4374 - val_loss: 0.4212\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4369 - val_loss: 0.4209\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4364 - val_loss: 0.4207\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4360 - val_loss: 0.4205\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4355 - val_loss: 0.4202\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4351 - val_loss: 0.4200\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4346 - val_loss: 0.4198\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4342 - val_loss: 0.4195\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4337 - val_loss: 0.4193\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4333 - val_loss: 0.4191\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4329 - val_loss: 0.4189\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4325 - val_loss: 0.4187\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4321 - val_loss: 0.4184\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4317 - val_loss: 0.4182\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4313 - val_loss: 0.4180\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4309 - val_loss: 0.4178\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4305 - val_loss: 0.4176\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4301 - val_loss: 0.4175\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4297 - val_loss: 0.4173\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4293 - val_loss: 0.4171\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4289 - val_loss: 0.4169\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4286 - val_loss: 0.4167\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4282 - val_loss: 0.4165\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4278 - val_loss: 0.4163\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4274 - val_loss: 0.4161\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4271 - val_loss: 0.4159\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(26, 67), test_y.shape=(26, 67)\n",
      "average MASE = 86.00366340249806, my average MASE = 69691247.07168306\n",
      "Cluster 1, 86.00366340249806\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9826 - val_loss: 1.3468\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9801 - val_loss: 1.3460\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9776 - val_loss: 1.3453\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9751 - val_loss: 1.3446\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9726 - val_loss: 1.3439\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9701 - val_loss: 1.3432\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.9677 - val_loss: 1.3425\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9653 - val_loss: 1.3419\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9628 - val_loss: 1.3413\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9604 - val_loss: 1.3408\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9580 - val_loss: 1.3402\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9556 - val_loss: 1.3396\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9532 - val_loss: 1.3390\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9508 - val_loss: 1.3384\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9486 - val_loss: 1.3379\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9463 - val_loss: 1.3374\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9440 - val_loss: 1.3368\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9418 - val_loss: 1.3363\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9397 - val_loss: 1.3358\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9375 - val_loss: 1.3352\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9353 - val_loss: 1.3347\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9331 - val_loss: 1.3341\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9309 - val_loss: 1.3337\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9287 - val_loss: 1.3332\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9266 - val_loss: 1.3328\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9245 - val_loss: 1.3324\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9224 - val_loss: 1.3321\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9203 - val_loss: 1.3317\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9183 - val_loss: 1.3314\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9162 - val_loss: 1.3311\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9141 - val_loss: 1.3308\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9120 - val_loss: 1.3305\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9100 - val_loss: 1.3303\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9079 - val_loss: 1.3300\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9059 - val_loss: 1.3298\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9039 - val_loss: 1.3295\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9019 - val_loss: 1.3293\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9000 - val_loss: 1.3291\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8981 - val_loss: 1.3288\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8963 - val_loss: 1.3286\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1.6636413128548868, my average MASE = 8.526439257891766\n",
      "Cluster 2, 1.6636413128548868\n",
      "Before prediction: train_X.shape=(177, 10, 67), train_y.shape=(177, 67), test_X.shape=(59, 10, 67), test_y.shape=(59, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.7327 - val_loss: 0.5966\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.7311 - val_loss: 0.5956\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7296 - val_loss: 0.5946\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7281 - val_loss: 0.5936\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7266 - val_loss: 0.5927\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7252 - val_loss: 0.5918\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7239 - val_loss: 0.5909\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7225 - val_loss: 0.5901\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7212 - val_loss: 0.5892\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.7199 - val_loss: 0.5884\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7186 - val_loss: 0.5875\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7174 - val_loss: 0.5867\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7162 - val_loss: 0.5859\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7150 - val_loss: 0.5852\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7138 - val_loss: 0.5844\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7127 - val_loss: 0.5837\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7116 - val_loss: 0.5829\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7104 - val_loss: 0.5822\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7093 - val_loss: 0.5815\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7082 - val_loss: 0.5808\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7071 - val_loss: 0.5801\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7061 - val_loss: 0.5794\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7050 - val_loss: 0.5788\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7040 - val_loss: 0.5781\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7030 - val_loss: 0.5775\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7020 - val_loss: 0.5768\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7010 - val_loss: 0.5762\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.7000 - val_loss: 0.5756\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6990 - val_loss: 0.5750\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6981 - val_loss: 0.5744\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6971 - val_loss: 0.5738\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6962 - val_loss: 0.5732\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6953 - val_loss: 0.5726\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6944 - val_loss: 0.5721\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6935 - val_loss: 0.5715\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6925 - val_loss: 0.5710\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6917 - val_loss: 0.5704\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6908 - val_loss: 0.5699\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6899 - val_loss: 0.5694\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6890 - val_loss: 0.5688\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "predicted_original.shape=(59, 67), test_y.shape=(59, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 148.95203283848892, my average MASE = 195949093.5023566\n",
      "Cluster 3, 148.95203283848892\n",
      "Before prediction: train_X.shape=(59, 10, 67), train_y.shape=(59, 67), test_X.shape=(20, 10, 67), test_y.shape=(20, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4242 - val_loss: 0.4717\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4237 - val_loss: 0.4716\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4233 - val_loss: 0.4715\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4228 - val_loss: 0.4714\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4223 - val_loss: 0.4713\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4219 - val_loss: 0.4712\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4214 - val_loss: 0.4711\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4210 - val_loss: 0.4710\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4206 - val_loss: 0.4709\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4201 - val_loss: 0.4708\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4197 - val_loss: 0.4707\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4193 - val_loss: 0.4706\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4189 - val_loss: 0.4705\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4185 - val_loss: 0.4705\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4180 - val_loss: 0.4704\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4176 - val_loss: 0.4703\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4172 - val_loss: 0.4702\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4168 - val_loss: 0.4701\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4164 - val_loss: 0.4700\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4160 - val_loss: 0.4699\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4157 - val_loss: 0.4698\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4153 - val_loss: 0.4698\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4149 - val_loss: 0.4697\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4145 - val_loss: 0.4696\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4141 - val_loss: 0.4695\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4138 - val_loss: 0.4694\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4134 - val_loss: 0.4694\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4130 - val_loss: 0.4693\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4127 - val_loss: 0.4692\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4123 - val_loss: 0.4691\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4119 - val_loss: 0.4690\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4116 - val_loss: 0.4690\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4112 - val_loss: 0.4689\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4108 - val_loss: 0.4688\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4105 - val_loss: 0.4687\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4101 - val_loss: 0.4687\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4098 - val_loss: 0.4686\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4094 - val_loss: 0.4685\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4091 - val_loss: 0.4684\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4087 - val_loss: 0.4684\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(20, 67), test_y.shape=(20, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 138.4214035546147, my average MASE = 76438634.40084383\n",
      "Cluster 4, 138.4214035546147\n",
      "Before prediction: train_X.shape=(6, 10, 67), train_y.shape=(6, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4144 - val_loss: 0.4072\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4116 - val_loss: 0.4056\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4089 - val_loss: 0.4039\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4061 - val_loss: 0.4022\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4034 - val_loss: 0.4005\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4008 - val_loss: 0.3988\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3982 - val_loss: 0.3971\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3957 - val_loss: 0.3954\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3932 - val_loss: 0.3937\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3908 - val_loss: 0.3920\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3885 - val_loss: 0.3903\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3861 - val_loss: 0.3887\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3838 - val_loss: 0.3870\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3816 - val_loss: 0.3854\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3795 - val_loss: 0.3838\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3775 - val_loss: 0.3822\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3755 - val_loss: 0.3807\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3735 - val_loss: 0.3793\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3716 - val_loss: 0.3779\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3697 - val_loss: 0.3765\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3679 - val_loss: 0.3751\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3661 - val_loss: 0.3737\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3644 - val_loss: 0.3723\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3627 - val_loss: 0.3710\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3611 - val_loss: 0.3697\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3594 - val_loss: 0.3684\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3578 - val_loss: 0.3672\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3562 - val_loss: 0.3659\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3547 - val_loss: 0.3646\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3533 - val_loss: 0.3634\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3519 - val_loss: 0.3621\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3506 - val_loss: 0.3608\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3493 - val_loss: 0.3596\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3481 - val_loss: 0.3583\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3468 - val_loss: 0.3571\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3456 - val_loss: 0.3558\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3443 - val_loss: 0.3546\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3431 - val_loss: 0.3534\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3419 - val_loss: 0.3521\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3407 - val_loss: 0.3509\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 697989181.1057402, my average MASE = 1714132782.1928625\n",
      "Cluster 5, 697989181.1057402\n",
      "Before prediction: train_X.shape=(95, 10, 67), train_y.shape=(95, 67), test_X.shape=(32, 10, 67), test_y.shape=(32, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3944 - val_loss: 0.3555\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3937 - val_loss: 0.3552\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3930 - val_loss: 0.3549\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3924 - val_loss: 0.3546\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3917 - val_loss: 0.3543\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3910 - val_loss: 0.3540\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3904 - val_loss: 0.3537\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3898 - val_loss: 0.3534\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3892 - val_loss: 0.3532\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3886 - val_loss: 0.3529\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3880 - val_loss: 0.3526\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3874 - val_loss: 0.3523\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3868 - val_loss: 0.3520\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3863 - val_loss: 0.3518\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3857 - val_loss: 0.3515\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3852 - val_loss: 0.3512\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3846 - val_loss: 0.3510\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3841 - val_loss: 0.3507\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3835 - val_loss: 0.3504\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3830 - val_loss: 0.3502\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3825 - val_loss: 0.3499\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3820 - val_loss: 0.3496\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3815 - val_loss: 0.3494\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3810 - val_loss: 0.3491\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3805 - val_loss: 0.3489\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3799 - val_loss: 0.3486\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3795 - val_loss: 0.3484\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3790 - val_loss: 0.3481\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3785 - val_loss: 0.3479\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3780 - val_loss: 0.3476\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3776 - val_loss: 0.3474\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3771 - val_loss: 0.3471\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3767 - val_loss: 0.3469\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3762 - val_loss: 0.3466\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3758 - val_loss: 0.3464\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3753 - val_loss: 0.3461\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3749 - val_loss: 0.3459\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3745 - val_loss: 0.3456\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3740 - val_loss: 0.3454\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3736 - val_loss: 0.3451\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(32, 67), test_y.shape=(32, 67)\n",
      "average MASE = 265.33452450872096, my average MASE = 56598015.30805221\n",
      "Cluster 6, 265.33452450872096\n",
      "clusters_labels.shape=(408095,)\n",
      "N_clusters=9, 9, 1486, (3, 67)\n",
      "Before prediction: train_X.shape=(97, 10, 67), train_y.shape=(97, 67), test_X.shape=(32, 10, 67), test_y.shape=(32, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4153 - val_loss: 0.4398\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4145 - val_loss: 0.4396\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4138 - val_loss: 0.4393\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4131 - val_loss: 0.4391\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4124 - val_loss: 0.4388\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4118 - val_loss: 0.4386\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4111 - val_loss: 0.4383\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4105 - val_loss: 0.4381\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4099 - val_loss: 0.4378\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4093 - val_loss: 0.4376\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4087 - val_loss: 0.4374\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4081 - val_loss: 0.4371\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4075 - val_loss: 0.4369\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4069 - val_loss: 0.4367\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4064 - val_loss: 0.4364\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4058 - val_loss: 0.4362\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4053 - val_loss: 0.4360\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4047 - val_loss: 0.4358\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4042 - val_loss: 0.4356\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4037 - val_loss: 0.4353\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4032 - val_loss: 0.4351\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4027 - val_loss: 0.4349\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4022 - val_loss: 0.4347\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4017 - val_loss: 0.4345\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4012 - val_loss: 0.4343\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4007 - val_loss: 0.4341\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4003 - val_loss: 0.4339\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3998 - val_loss: 0.4337\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3993 - val_loss: 0.4335\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3989 - val_loss: 0.4334\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3984 - val_loss: 0.4332\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3980 - val_loss: 0.4330\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3976 - val_loss: 0.4328\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3971 - val_loss: 0.4326\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3967 - val_loss: 0.4324\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3963 - val_loss: 0.4322\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3959 - val_loss: 0.4320\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3955 - val_loss: 0.4318\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3950 - val_loss: 0.4316\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3946 - val_loss: 0.4314\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(32, 67), test_y.shape=(32, 67)\n",
      "average MASE = 106.62824359839709, my average MASE = 181907103.45341906\n",
      "Cluster 0, 106.62824359839709\n",
      "Before prediction: train_X.shape=(1415, 10, 67), train_y.shape=(1415, 67), test_X.shape=(472, 10, 67), test_y.shape=(472, 67)\n",
      "Epoch 1/40\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.1435 - val_loss: 0.0272\n",
      "Epoch 2/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1388 - val_loss: 0.0264\n",
      "Epoch 3/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1347 - val_loss: 0.0256\n",
      "Epoch 4/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1311 - val_loss: 0.0250\n",
      "Epoch 5/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1278 - val_loss: 0.0244\n",
      "Epoch 6/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1249 - val_loss: 0.0239\n",
      "Epoch 7/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1221 - val_loss: 0.0234\n",
      "Epoch 8/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1196 - val_loss: 0.0231\n",
      "Epoch 9/40\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.1173 - val_loss: 0.0227\n",
      "Epoch 10/40\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 0.1151 - val_loss: 0.0224\n",
      "Epoch 11/40\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.1132 - val_loss: 0.0221\n",
      "Epoch 12/40\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.1113 - val_loss: 0.0220\n",
      "Epoch 13/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1096 - val_loss: 0.0217\n",
      "Epoch 14/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1080 - val_loss: 0.0216\n",
      "Epoch 15/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1065 - val_loss: 0.0214\n",
      "Epoch 16/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1051 - val_loss: 0.0212\n",
      "Epoch 17/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.1038 - val_loss: 0.0211\n",
      "Epoch 18/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1026 - val_loss: 0.0209\n",
      "Epoch 19/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1014 - val_loss: 0.0208\n",
      "Epoch 20/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.1004 - val_loss: 0.0206\n",
      "Epoch 21/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0993 - val_loss: 0.0206\n",
      "Epoch 22/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0983 - val_loss: 0.0205\n",
      "Epoch 23/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0974 - val_loss: 0.0204\n",
      "Epoch 24/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0965 - val_loss: 0.0202\n",
      "Epoch 25/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0957 - val_loss: 0.0201\n",
      "Epoch 26/40\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.0949 - val_loss: 0.0200\n",
      "Epoch 27/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0941 - val_loss: 0.0200\n",
      "Epoch 28/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0934 - val_loss: 0.0199\n",
      "Epoch 29/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0927 - val_loss: 0.0198\n",
      "Epoch 30/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0920 - val_loss: 0.0197\n",
      "Epoch 31/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0914 - val_loss: 0.0197\n",
      "Epoch 32/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0907 - val_loss: 0.0196\n",
      "Epoch 33/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0902 - val_loss: 0.0195\n",
      "Epoch 34/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0896 - val_loss: 0.0195\n",
      "Epoch 35/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0890 - val_loss: 0.0195\n",
      "Epoch 36/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0885 - val_loss: 0.0194\n",
      "Epoch 37/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0880 - val_loss: 0.0194\n",
      "Epoch 38/40\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 0.0875 - val_loss: 0.0193\n",
      "Epoch 39/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0871 - val_loss: 0.0194\n",
      "Epoch 40/40\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.0867 - val_loss: 0.0193\n",
      "15/15 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(472, 67), test_y.shape=(472, 67)\n",
      "average MASE = 484561401.3912065, my average MASE = 2109803098.5295365\n",
      "Cluster 1, 484561401.3912065\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4184 - val_loss: 0.4085\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4174 - val_loss: 0.4084\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4165 - val_loss: 0.4083\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4155 - val_loss: 0.4082\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4146 - val_loss: 0.4082\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4136 - val_loss: 0.4081\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4127 - val_loss: 0.4080\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4118 - val_loss: 0.4079\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4109 - val_loss: 0.4078\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4100 - val_loss: 0.4077\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4091 - val_loss: 0.4077\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4083 - val_loss: 0.4076\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4074 - val_loss: 0.4075\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4066 - val_loss: 0.4074\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4057 - val_loss: 0.4074\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4049 - val_loss: 0.4073\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4041 - val_loss: 0.4072\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4033 - val_loss: 0.4071\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4025 - val_loss: 0.4070\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4017 - val_loss: 0.4069\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4009 - val_loss: 0.4069\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4001 - val_loss: 0.4068\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3994 - val_loss: 0.4067\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3986 - val_loss: 0.4066\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3979 - val_loss: 0.4065\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3971 - val_loss: 0.4065\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3964 - val_loss: 0.4064\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3957 - val_loss: 0.4063\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3949 - val_loss: 0.4062\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3942 - val_loss: 0.4062\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3936 - val_loss: 0.4061\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3929 - val_loss: 0.4060\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3922 - val_loss: 0.4059\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3915 - val_loss: 0.4058\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3909 - val_loss: 0.4058\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3902 - val_loss: 0.4057\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3896 - val_loss: 0.4056\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3889 - val_loss: 0.4055\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3883 - val_loss: 0.4055\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3877 - val_loss: 0.4054\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 146.41485105295615, my average MASE = 90287364.09726365\n",
      "Cluster 2, 146.41485105295615\n",
      "Before prediction: train_X.shape=(32, 10, 67), train_y.shape=(32, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5429 - val_loss: 0.4489\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5408 - val_loss: 0.4477\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5388 - val_loss: 0.4466\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5367 - val_loss: 0.4454\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5348 - val_loss: 0.4443\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5328 - val_loss: 0.4432\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5308 - val_loss: 0.4421\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5289 - val_loss: 0.4410\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5270 - val_loss: 0.4399\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5251 - val_loss: 0.4389\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5232 - val_loss: 0.4378\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5214 - val_loss: 0.4368\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5195 - val_loss: 0.4357\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5177 - val_loss: 0.4347\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5159 - val_loss: 0.4336\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5140 - val_loss: 0.4326\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5123 - val_loss: 0.4316\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5105 - val_loss: 0.4306\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5087 - val_loss: 0.4296\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5069 - val_loss: 0.4287\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5052 - val_loss: 0.4277\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5034 - val_loss: 0.4268\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5017 - val_loss: 0.4258\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5000 - val_loss: 0.4249\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4983 - val_loss: 0.4240\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4967 - val_loss: 0.4230\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4950 - val_loss: 0.4221\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4934 - val_loss: 0.4212\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4918 - val_loss: 0.4204\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4903 - val_loss: 0.4195\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4887 - val_loss: 0.4186\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4872 - val_loss: 0.4177\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4857 - val_loss: 0.4169\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4843 - val_loss: 0.4161\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4828 - val_loss: 0.4152\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4814 - val_loss: 0.4144\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4800 - val_loss: 0.4136\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4786 - val_loss: 0.4127\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4772 - val_loss: 0.4119\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4758 - val_loss: 0.4111\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1177081/3287276626.py:67: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(12, 10))\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3556660081.750684, my average MASE = 7865462670.190711\n",
      "Cluster 3, 3556660081.750684\n",
      "Before prediction: train_X.shape=(173, 10, 67), train_y.shape=(173, 67), test_X.shape=(58, 10, 67), test_y.shape=(58, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.7286 - val_loss: 0.5747\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7270 - val_loss: 0.5740\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7256 - val_loss: 0.5733\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7243 - val_loss: 0.5725\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7229 - val_loss: 0.5718\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7216 - val_loss: 0.5712\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7203 - val_loss: 0.5705\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7190 - val_loss: 0.5698\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7178 - val_loss: 0.5692\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7166 - val_loss: 0.5686\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7154 - val_loss: 0.5679\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7142 - val_loss: 0.5673\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7130 - val_loss: 0.5667\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7119 - val_loss: 0.5662\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7108 - val_loss: 0.5656\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7096 - val_loss: 0.5650\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7086 - val_loss: 0.5645\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.7075 - val_loss: 0.5639\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7064 - val_loss: 0.5634\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7054 - val_loss: 0.5629\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7044 - val_loss: 0.5624\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7034 - val_loss: 0.5619\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7024 - val_loss: 0.5614\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7014 - val_loss: 0.5609\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7004 - val_loss: 0.5604\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6995 - val_loss: 0.5599\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6986 - val_loss: 0.5595\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6976 - val_loss: 0.5590\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6967 - val_loss: 0.5585\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6958 - val_loss: 0.5581\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6949 - val_loss: 0.5576\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6940 - val_loss: 0.5571\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6931 - val_loss: 0.5567\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6922 - val_loss: 0.5562\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6914 - val_loss: 0.5558\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6905 - val_loss: 0.5553\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6896 - val_loss: 0.5549\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6887 - val_loss: 0.5545\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6879 - val_loss: 0.5540\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6870 - val_loss: 0.5536\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(58, 67), test_y.shape=(58, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 122.19340890523425, my average MASE = 115345826.80728418\n",
      "Cluster 4, 122.19340890523425\n",
      "Before prediction: train_X.shape=(1562, 10, 67), train_y.shape=(1562, 67), test_X.shape=(521, 10, 67), test_y.shape=(521, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2392 - val_loss: 0.2772\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2287 - val_loss: 0.2673\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2207 - val_loss: 0.2595\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2144 - val_loss: 0.2534\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2094 - val_loss: 0.2481\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.2050 - val_loss: 0.2435\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2013 - val_loss: 0.2394\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1978 - val_loss: 0.2357\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1947 - val_loss: 0.2322\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1919 - val_loss: 0.2289\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1892 - val_loss: 0.2259\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1867 - val_loss: 0.2231\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1843 - val_loss: 0.2204\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1821 - val_loss: 0.2179\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1800 - val_loss: 0.2156\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1780 - val_loss: 0.2134\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1761 - val_loss: 0.2114\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1743 - val_loss: 0.2096\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1727 - val_loss: 0.2078\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1712 - val_loss: 0.2062\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1698 - val_loss: 0.2047\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1684 - val_loss: 0.2033\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1672 - val_loss: 0.2020\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1661 - val_loss: 0.2007\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1650 - val_loss: 0.1996\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1640 - val_loss: 0.1985\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1631 - val_loss: 0.1974\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1622 - val_loss: 0.1964\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1614 - val_loss: 0.1954\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1606 - val_loss: 0.1945\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1598 - val_loss: 0.1936\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1591 - val_loss: 0.1928\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1584 - val_loss: 0.1920\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1578 - val_loss: 0.1912\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1572 - val_loss: 0.1904\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1566 - val_loss: 0.1897\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1560 - val_loss: 0.1890\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1554 - val_loss: 0.1883\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1549 - val_loss: 0.1877\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1544 - val_loss: 0.1871\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(521, 67), test_y.shape=(521, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 151.79409601094594, my average MASE = 3866050940.760538\n",
      "Cluster 5, 151.79409601094594\n",
      "Before prediction: train_X.shape=(83, 10, 67), train_y.shape=(83, 67), test_X.shape=(28, 10, 67), test_y.shape=(28, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4279 - val_loss: 0.5018\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4270 - val_loss: 0.5012\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4262 - val_loss: 0.5006\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4254 - val_loss: 0.5000\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4246 - val_loss: 0.4995\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4239 - val_loss: 0.4989\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4231 - val_loss: 0.4984\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4224 - val_loss: 0.4978\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4217 - val_loss: 0.4973\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4210 - val_loss: 0.4968\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4203 - val_loss: 0.4962\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4197 - val_loss: 0.4958\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4191 - val_loss: 0.4953\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4184 - val_loss: 0.4948\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4178 - val_loss: 0.4943\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4172 - val_loss: 0.4939\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4167 - val_loss: 0.4934\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4161 - val_loss: 0.4930\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4155 - val_loss: 0.4926\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4150 - val_loss: 0.4921\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4144 - val_loss: 0.4917\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4139 - val_loss: 0.4913\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4133 - val_loss: 0.4908\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4128 - val_loss: 0.4904\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4123 - val_loss: 0.4900\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4118 - val_loss: 0.4895\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4113 - val_loss: 0.4891\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4107 - val_loss: 0.4887\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4103 - val_loss: 0.4883\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4098 - val_loss: 0.4879\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4093 - val_loss: 0.4875\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4088 - val_loss: 0.4871\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4083 - val_loss: 0.4867\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4079 - val_loss: 0.4863\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4074 - val_loss: 0.4859\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4069 - val_loss: 0.4855\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4065 - val_loss: 0.4851\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4061 - val_loss: 0.4848\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4056 - val_loss: 0.4844\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4052 - val_loss: 0.4840\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "predicted_original.shape=(28, 67), test_y.shape=(28, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 155.68618683798326, my average MASE = 97738271.39222205\n",
      "Cluster 6, 155.68618683798326\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2911 - val_loss: 0.2917\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2907 - val_loss: 0.2915\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2902 - val_loss: 0.2913\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2897 - val_loss: 0.2911\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2892 - val_loss: 0.2909\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2888 - val_loss: 0.2907\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2883 - val_loss: 0.2905\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2878 - val_loss: 0.2903\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2874 - val_loss: 0.2901\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2869 - val_loss: 0.2899\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2865 - val_loss: 0.2898\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2861 - val_loss: 0.2896\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2856 - val_loss: 0.2894\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2852 - val_loss: 0.2892\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2848 - val_loss: 0.2890\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2844 - val_loss: 0.2889\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2840 - val_loss: 0.2887\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2836 - val_loss: 0.2885\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2832 - val_loss: 0.2883\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2828 - val_loss: 0.2882\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2824 - val_loss: 0.2880\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2820 - val_loss: 0.2878\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2816 - val_loss: 0.2877\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2812 - val_loss: 0.2875\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2808 - val_loss: 0.2874\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2804 - val_loss: 0.2872\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2800 - val_loss: 0.2870\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2796 - val_loss: 0.2869\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2793 - val_loss: 0.2867\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2789 - val_loss: 0.2865\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2785 - val_loss: 0.2864\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2781 - val_loss: 0.2862\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2778 - val_loss: 0.2861\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2774 - val_loss: 0.2859\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2770 - val_loss: 0.2858\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2767 - val_loss: 0.2856\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2763 - val_loss: 0.2855\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2759 - val_loss: 0.2853\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2756 - val_loss: 0.2852\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2752 - val_loss: 0.2851\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 112.24086700274312, my average MASE = 119826311.76187374\n",
      "Cluster 8, 112.24086700274312\n",
      "clusters_labels.shape=(408095,)\n",
      "N_clusters=11, 11, 17, (3243, 67)\n",
      "Before prediction: train_X.shape=(1939, 10, 67), train_y.shape=(1939, 67), test_X.shape=(646, 10, 67), test_y.shape=(646, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1118 - val_loss: 0.1019\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1064 - val_loss: 0.0996\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1022 - val_loss: 0.0982\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0987 - val_loss: 0.0973\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0956 - val_loss: 0.0966\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0930 - val_loss: 0.0962\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0906 - val_loss: 0.0958\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0885 - val_loss: 0.0954\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0866 - val_loss: 0.0952\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0848 - val_loss: 0.0949\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0832 - val_loss: 0.0946\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0818 - val_loss: 0.0944\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0804 - val_loss: 0.0941\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0792 - val_loss: 0.0939\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0780 - val_loss: 0.0937\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0769 - val_loss: 0.0936\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0759 - val_loss: 0.0934\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0749 - val_loss: 0.0933\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0740 - val_loss: 0.0932\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0732 - val_loss: 0.0931\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0724 - val_loss: 0.0929\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0716 - val_loss: 0.0928\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0709 - val_loss: 0.0927\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0702 - val_loss: 0.0926\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0696 - val_loss: 0.0926\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0690 - val_loss: 0.0925\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0685 - val_loss: 0.0924\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0679 - val_loss: 0.0923\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0674 - val_loss: 0.0923\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0670 - val_loss: 0.0922\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0666 - val_loss: 0.0921\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0662 - val_loss: 0.0921\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0658 - val_loss: 0.0920\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0654 - val_loss: 0.0919\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0651 - val_loss: 0.0919\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0648 - val_loss: 0.0918\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0645 - val_loss: 0.0917\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0642 - val_loss: 0.0917\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0640 - val_loss: 0.0916\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0637 - val_loss: 0.0916\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(646, 67), test_y.shape=(646, 67)\n",
      "average MASE = 876211806.5152007, my average MASE = 21477700395.128197\n",
      "Cluster 0, 876211806.5152007\n",
      "Before prediction: train_X.shape=(13, 10, 67), train_y.shape=(13, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.7012 - val_loss: 0.5689\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7001 - val_loss: 0.5688\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6990 - val_loss: 0.5687\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6979 - val_loss: 0.5686\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6968 - val_loss: 0.5685\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6958 - val_loss: 0.5684\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6947 - val_loss: 0.5683\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6936 - val_loss: 0.5682\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6926 - val_loss: 0.5681\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6916 - val_loss: 0.5680\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6906 - val_loss: 0.5679\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6896 - val_loss: 0.5678\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6887 - val_loss: 0.5677\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6877 - val_loss: 0.5676\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6868 - val_loss: 0.5675\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6859 - val_loss: 0.5674\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6849 - val_loss: 0.5673\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6840 - val_loss: 0.5672\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6831 - val_loss: 0.5671\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6822 - val_loss: 0.5670\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6813 - val_loss: 0.5669\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6805 - val_loss: 0.5668\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6796 - val_loss: 0.5667\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6787 - val_loss: 0.5666\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6779 - val_loss: 0.5665\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6771 - val_loss: 0.5665\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6762 - val_loss: 0.5664\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6754 - val_loss: 0.5663\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6746 - val_loss: 0.5662\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6738 - val_loss: 0.5661\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6730 - val_loss: 0.5660\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6722 - val_loss: 0.5659\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6715 - val_loss: 0.5658\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6707 - val_loss: 0.5657\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6699 - val_loss: 0.5656\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6691 - val_loss: 0.5655\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6684 - val_loss: 0.5654\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6676 - val_loss: 0.5653\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6668 - val_loss: 0.5652\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6661 - val_loss: 0.5651\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1104923.1929255812, my average MASE = 35973487.14565515\n",
      "Cluster 1, 1104923.1929255812\n",
      "Before prediction: train_X.shape=(134, 10, 67), train_y.shape=(134, 67), test_X.shape=(45, 10, 67), test_y.shape=(45, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4388 - val_loss: 0.7635\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4381 - val_loss: 0.7630\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4375 - val_loss: 0.7625\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4369 - val_loss: 0.7620\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4363 - val_loss: 0.7616\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4358 - val_loss: 0.7611\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4353 - val_loss: 0.7607\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4348 - val_loss: 0.7603\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4343 - val_loss: 0.7598\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4338 - val_loss: 0.7594\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4333 - val_loss: 0.7590\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4328 - val_loss: 0.7585\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4323 - val_loss: 0.7581\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4318 - val_loss: 0.7576\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4314 - val_loss: 0.7572\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4309 - val_loss: 0.7568\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4305 - val_loss: 0.7564\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4301 - val_loss: 0.7561\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4296 - val_loss: 0.7557\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4292 - val_loss: 0.7554\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4288 - val_loss: 0.7550\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4284 - val_loss: 0.7546\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4280 - val_loss: 0.7543\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4276 - val_loss: 0.7539\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4272 - val_loss: 0.7535\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4268 - val_loss: 0.7532\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4264 - val_loss: 0.7528\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4260 - val_loss: 0.7525\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4257 - val_loss: 0.7521\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4253 - val_loss: 0.7517\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4249 - val_loss: 0.7514\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4245 - val_loss: 0.7511\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4242 - val_loss: 0.7508\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4238 - val_loss: 0.7505\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4235 - val_loss: 0.7502\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4231 - val_loss: 0.7499\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4228 - val_loss: 0.7496\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4225 - val_loss: 0.7492\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4221 - val_loss: 0.7489\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4218 - val_loss: 0.7486\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(45, 67), test_y.shape=(45, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 134.01886625392066, my average MASE = 180554408.61752364\n",
      "Cluster 2, 134.01886625392066\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3365 - val_loss: 0.4571\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3346 - val_loss: 0.4566\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3328 - val_loss: 0.4560\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3311 - val_loss: 0.4555\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3293 - val_loss: 0.4550\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3277 - val_loss: 0.4544\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3260 - val_loss: 0.4539\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3244 - val_loss: 0.4534\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3229 - val_loss: 0.4528\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3213 - val_loss: 0.4523\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3197 - val_loss: 0.4518\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3182 - val_loss: 0.4512\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3166 - val_loss: 0.4507\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3152 - val_loss: 0.4501\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3137 - val_loss: 0.4496\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3123 - val_loss: 0.4490\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3109 - val_loss: 0.4485\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3095 - val_loss: 0.4479\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3082 - val_loss: 0.4473\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3070 - val_loss: 0.4468\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3058 - val_loss: 0.4462\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3045 - val_loss: 0.4456\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3033 - val_loss: 0.4450\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3021 - val_loss: 0.4444\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3009 - val_loss: 0.4438\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2998 - val_loss: 0.4432\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2986 - val_loss: 0.4426\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2974 - val_loss: 0.4420\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2963 - val_loss: 0.4414\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2952 - val_loss: 0.4407\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2940 - val_loss: 0.4401\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2929 - val_loss: 0.4395\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2918 - val_loss: 0.4389\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2907 - val_loss: 0.4384\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2896 - val_loss: 0.4379\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2885 - val_loss: 0.4373\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2875 - val_loss: 0.4369\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2865 - val_loss: 0.4364\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2854 - val_loss: 0.4359\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2844 - val_loss: 0.4355\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.28495017155315366, my average MASE = 0.3992586255016426\n",
      "Cluster 3, 0.28495017155315366\n",
      "Before prediction: train_X.shape=(77, 10, 67), train_y.shape=(77, 67), test_X.shape=(26, 10, 67), test_y.shape=(26, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6320 - val_loss: 1.0222\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6307 - val_loss: 1.0218\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6295 - val_loss: 1.0214\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6284 - val_loss: 1.0211\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6273 - val_loss: 1.0208\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6262 - val_loss: 1.0205\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6252 - val_loss: 1.0201\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6241 - val_loss: 1.0198\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6231 - val_loss: 1.0195\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6221 - val_loss: 1.0192\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6212 - val_loss: 1.0188\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6202 - val_loss: 1.0185\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6193 - val_loss: 1.0182\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6183 - val_loss: 1.0179\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6174 - val_loss: 1.0176\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6166 - val_loss: 1.0172\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6157 - val_loss: 1.0169\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6148 - val_loss: 1.0166\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6139 - val_loss: 1.0162\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6131 - val_loss: 1.0159\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6122 - val_loss: 1.0155\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6114 - val_loss: 1.0152\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6106 - val_loss: 1.0148\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6097 - val_loss: 1.0145\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6089 - val_loss: 1.0141\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6081 - val_loss: 1.0138\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6073 - val_loss: 1.0134\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6066 - val_loss: 1.0131\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6057 - val_loss: 1.0127\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6050 - val_loss: 1.0124\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6042 - val_loss: 1.0121\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6035 - val_loss: 1.0118\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6027 - val_loss: 1.0114\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6020 - val_loss: 1.0111\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6013 - val_loss: 1.0108\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6005 - val_loss: 1.0104\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5998 - val_loss: 1.0101\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5991 - val_loss: 1.0098\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5984 - val_loss: 1.0094\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5977 - val_loss: 1.0091\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(26, 67), test_y.shape=(26, 67)\n",
      "average MASE = 98.33747704062574, my average MASE = 204772160.73570046\n",
      "Cluster 4, 98.33747704062574\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4785 - val_loss: 0.4645\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4760 - val_loss: 0.4629\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4734 - val_loss: 0.4612\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4709 - val_loss: 0.4596\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4684 - val_loss: 0.4580\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4661 - val_loss: 0.4564\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4638 - val_loss: 0.4548\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4615 - val_loss: 0.4532\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4593 - val_loss: 0.4516\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4572 - val_loss: 0.4500\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4551 - val_loss: 0.4484\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4530 - val_loss: 0.4468\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4509 - val_loss: 0.4453\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4489 - val_loss: 0.4438\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4469 - val_loss: 0.4422\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4449 - val_loss: 0.4407\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4429 - val_loss: 0.4393\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4410 - val_loss: 0.4380\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4391 - val_loss: 0.4367\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4371 - val_loss: 0.4354\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4353 - val_loss: 0.4343\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4335 - val_loss: 0.4332\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4317 - val_loss: 0.4322\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4300 - val_loss: 0.4311\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4282 - val_loss: 0.4301\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4265 - val_loss: 0.4291\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4248 - val_loss: 0.4281\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4231 - val_loss: 0.4270\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4214 - val_loss: 0.4260\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4197 - val_loss: 0.4250\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4181 - val_loss: 0.4240\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4165 - val_loss: 0.4229\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4149 - val_loss: 0.4218\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4134 - val_loss: 0.4208\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4118 - val_loss: 0.4197\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4103 - val_loss: 0.4187\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4088 - val_loss: 0.4176\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4074 - val_loss: 0.4167\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4060 - val_loss: 0.4157\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4046 - val_loss: 0.4148\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 931833756.986838, my average MASE = 2420970654.4710827\n",
      "Cluster 5, 931833756.986838\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4346 - val_loss: 0.4628\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4334 - val_loss: 0.4625\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4323 - val_loss: 0.4621\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4312 - val_loss: 0.4618\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4301 - val_loss: 0.4615\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4290 - val_loss: 0.4612\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4280 - val_loss: 0.4609\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4269 - val_loss: 0.4605\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4259 - val_loss: 0.4602\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4249 - val_loss: 0.4599\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4239 - val_loss: 0.4596\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4228 - val_loss: 0.4593\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4218 - val_loss: 0.4590\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4208 - val_loss: 0.4587\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4198 - val_loss: 0.4584\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4188 - val_loss: 0.4582\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4179 - val_loss: 0.4579\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4169 - val_loss: 0.4576\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4160 - val_loss: 0.4573\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4151 - val_loss: 0.4570\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4141 - val_loss: 0.4567\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4132 - val_loss: 0.4564\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4123 - val_loss: 0.4561\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4114 - val_loss: 0.4558\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4106 - val_loss: 0.4555\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4097 - val_loss: 0.4552\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4089 - val_loss: 0.4549\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4080 - val_loss: 0.4546\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4072 - val_loss: 0.4544\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4063 - val_loss: 0.4541\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4055 - val_loss: 0.4538\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4047 - val_loss: 0.4536\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4039 - val_loss: 0.4533\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4031 - val_loss: 0.4531\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4023 - val_loss: 0.4529\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4015 - val_loss: 0.4527\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4007 - val_loss: 0.4526\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3999 - val_loss: 0.4524\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3991 - val_loss: 0.4522\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3983 - val_loss: 0.4520\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 806621.4868706047, my average MASE = 27017424.126969155\n",
      "Cluster 6, 806621.4868706047\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8616 - val_loss: 0.8556\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8588 - val_loss: 0.8539\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8561 - val_loss: 0.8522\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8534 - val_loss: 0.8505\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8508 - val_loss: 0.8489\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8484 - val_loss: 0.8472\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8459 - val_loss: 0.8456\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8435 - val_loss: 0.8440\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8412 - val_loss: 0.8424\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8389 - val_loss: 0.8408\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8366 - val_loss: 0.8392\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8343 - val_loss: 0.8376\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8320 - val_loss: 0.8361\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8298 - val_loss: 0.8345\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8276 - val_loss: 0.8329\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8254 - val_loss: 0.8314\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8232 - val_loss: 0.8298\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8210 - val_loss: 0.8283\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8189 - val_loss: 0.8267\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8168 - val_loss: 0.8253\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8146 - val_loss: 0.8238\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8125 - val_loss: 0.8224\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8104 - val_loss: 0.8210\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8084 - val_loss: 0.8197\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8064 - val_loss: 0.8184\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8045 - val_loss: 0.8170\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8025 - val_loss: 0.8157\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8006 - val_loss: 0.8143\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7987 - val_loss: 0.8129\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7969 - val_loss: 0.8115\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7950 - val_loss: 0.8102\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7932 - val_loss: 0.8089\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7913 - val_loss: 0.8077\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7894 - val_loss: 0.8065\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7876 - val_loss: 0.8053\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7857 - val_loss: 0.8041\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7839 - val_loss: 0.8030\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7821 - val_loss: 0.8019\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7803 - val_loss: 0.8008\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7785 - val_loss: 0.7997\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 964635493.0745914, my average MASE = 2107611139.389815\n",
      "Cluster 7, 964635493.0745914\n",
      "Before prediction: train_X.shape=(30, 10, 67), train_y.shape=(30, 67), test_X.shape=(10, 10, 67), test_y.shape=(10, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3952 - val_loss: 0.3689\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3946 - val_loss: 0.3689\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3941 - val_loss: 0.3688\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3935 - val_loss: 0.3687\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3930 - val_loss: 0.3687\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3924 - val_loss: 0.3686\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3919 - val_loss: 0.3685\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3914 - val_loss: 0.3685\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3909 - val_loss: 0.3684\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3903 - val_loss: 0.3684\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3898 - val_loss: 0.3683\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3893 - val_loss: 0.3682\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3888 - val_loss: 0.3682\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3883 - val_loss: 0.3681\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3878 - val_loss: 0.3680\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3873 - val_loss: 0.3680\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3869 - val_loss: 0.3679\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3864 - val_loss: 0.3679\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3859 - val_loss: 0.3678\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3855 - val_loss: 0.3677\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3850 - val_loss: 0.3677\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3845 - val_loss: 0.3676\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3841 - val_loss: 0.3676\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3836 - val_loss: 0.3675\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3832 - val_loss: 0.3674\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3828 - val_loss: 0.3674\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3823 - val_loss: 0.3673\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3819 - val_loss: 0.3673\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3815 - val_loss: 0.3672\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3811 - val_loss: 0.3672\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3806 - val_loss: 0.3671\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3802 - val_loss: 0.3670\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3798 - val_loss: 0.3670\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3794 - val_loss: 0.3669\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3790 - val_loss: 0.3669\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3786 - val_loss: 0.3668\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3782 - val_loss: 0.3668\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3778 - val_loss: 0.3667\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3774 - val_loss: 0.3666\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3770 - val_loss: 0.3666\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(10, 67), test_y.shape=(10, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 51.171495934252626, my average MASE = 30811708.472042836\n",
      "Cluster 8, 51.171495934252626\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4416 - val_loss: 0.4387\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4409 - val_loss: 0.4386\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4402 - val_loss: 0.4384\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4395 - val_loss: 0.4383\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4388 - val_loss: 0.4381\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4382 - val_loss: 0.4380\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4375 - val_loss: 0.4379\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4368 - val_loss: 0.4377\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4362 - val_loss: 0.4376\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4355 - val_loss: 0.4375\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4348 - val_loss: 0.4373\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4342 - val_loss: 0.4372\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4335 - val_loss: 0.4371\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4329 - val_loss: 0.4370\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4323 - val_loss: 0.4368\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4316 - val_loss: 0.4367\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4310 - val_loss: 0.4366\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4304 - val_loss: 0.4365\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4298 - val_loss: 0.4364\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4292 - val_loss: 0.4363\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4286 - val_loss: 0.4362\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4280 - val_loss: 0.4361\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4274 - val_loss: 0.4359\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4268 - val_loss: 0.4358\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4262 - val_loss: 0.4357\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4256 - val_loss: 0.4356\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4250 - val_loss: 0.4355\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4245 - val_loss: 0.4354\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4239 - val_loss: 0.4353\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4233 - val_loss: 0.4352\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4227 - val_loss: 0.4351\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4221 - val_loss: 0.4350\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4216 - val_loss: 0.4349\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4210 - val_loss: 0.4348\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4204 - val_loss: 0.4347\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4199 - val_loss: 0.4346\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4193 - val_loss: 0.4345\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4187 - val_loss: 0.4345\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4182 - val_loss: 0.4344\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4176 - val_loss: 0.4343\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 67.68630348587587, my average MASE = 34346224.36796317\n",
      "Cluster 9, 67.68630348587587\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=2, 2, 18, (30610, 67)\n",
      "Before prediction: train_X.shape=(18359, 10, 67), train_y.shape=(18359, 67), test_X.shape=(6120, 10, 67), test_y.shape=(6120, 67)\n",
      "Epoch 1/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.3046 - val_loss: 0.3213\n",
      "Epoch 2/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2853 - val_loss: 0.3058\n",
      "Epoch 3/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2698 - val_loss: 0.2935\n",
      "Epoch 4/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2588 - val_loss: 0.2851\n",
      "Epoch 5/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2516 - val_loss: 0.2788\n",
      "Epoch 6/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2460 - val_loss: 0.2736\n",
      "Epoch 7/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2412 - val_loss: 0.2691\n",
      "Epoch 8/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2371 - val_loss: 0.2652\n",
      "Epoch 9/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2335 - val_loss: 0.2618\n",
      "Epoch 10/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2301 - val_loss: 0.2588\n",
      "Epoch 11/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2271 - val_loss: 0.2562\n",
      "Epoch 12/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2243 - val_loss: 0.2538\n",
      "Epoch 13/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2219 - val_loss: 0.2516\n",
      "Epoch 14/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2199 - val_loss: 0.2498\n",
      "Epoch 15/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2181 - val_loss: 0.2483\n",
      "Epoch 16/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2166 - val_loss: 0.2469\n",
      "Epoch 17/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2153 - val_loss: 0.2458\n",
      "Epoch 18/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2142 - val_loss: 0.2448\n",
      "Epoch 19/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2131 - val_loss: 0.2438\n",
      "Epoch 20/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2122 - val_loss: 0.2430\n",
      "Epoch 21/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2114 - val_loss: 0.2422\n",
      "Epoch 22/40\n",
      "287/287 [==============================] - 8s 30ms/step - loss: 0.2106 - val_loss: 0.2414\n",
      "Epoch 23/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2099 - val_loss: 0.2407\n",
      "Epoch 24/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2092 - val_loss: 0.2401\n",
      "Epoch 25/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2086 - val_loss: 0.2395\n",
      "Epoch 26/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2080 - val_loss: 0.2389\n",
      "Epoch 27/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2074 - val_loss: 0.2385\n",
      "Epoch 28/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2069 - val_loss: 0.2380\n",
      "Epoch 29/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2064 - val_loss: 0.2376\n",
      "Epoch 30/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2059 - val_loss: 0.2370\n",
      "Epoch 31/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2054 - val_loss: 0.2367\n",
      "Epoch 32/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2050 - val_loss: 0.2362\n",
      "Epoch 33/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2046 - val_loss: 0.2359\n",
      "Epoch 34/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2042 - val_loss: 0.2356\n",
      "Epoch 35/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2038 - val_loss: 0.2351\n",
      "Epoch 36/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2034 - val_loss: 0.2349\n",
      "Epoch 37/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2031 - val_loss: 0.2345\n",
      "Epoch 38/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2028 - val_loss: 0.2343\n",
      "Epoch 39/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2024 - val_loss: 0.2340\n",
      "Epoch 40/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2021 - val_loss: 0.2337\n",
      "192/192 [==============================] - 2s 10ms/step\n",
      "predicted_original.shape=(6120, 67), test_y.shape=(6120, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1675.644817755528, my average MASE = 2652.7812514268226\n",
      "Cluster 0, 1675.644817755528\n",
      "Before prediction: train_X.shape=(8, 10, 67), train_y.shape=(8, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4228 - val_loss: 1.1321\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4209 - val_loss: 1.1320\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4191 - val_loss: 1.1319\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4173 - val_loss: 1.1318\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4156 - val_loss: 1.1317\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4139 - val_loss: 1.1315\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4122 - val_loss: 1.1314\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4106 - val_loss: 1.1313\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4090 - val_loss: 1.1311\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4075 - val_loss: 1.1310\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4059 - val_loss: 1.1308\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4044 - val_loss: 1.1306\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4030 - val_loss: 1.1304\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4015 - val_loss: 1.1302\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4001 - val_loss: 1.1300\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3987 - val_loss: 1.1298\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3973 - val_loss: 1.1296\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3960 - val_loss: 1.1294\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3946 - val_loss: 1.1292\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3933 - val_loss: 1.1290\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3920 - val_loss: 1.1288\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3907 - val_loss: 1.1286\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3895 - val_loss: 1.1283\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3883 - val_loss: 1.1281\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3871 - val_loss: 1.1279\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3859 - val_loss: 1.1277\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3847 - val_loss: 1.1275\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3836 - val_loss: 1.1272\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3825 - val_loss: 1.1270\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3814 - val_loss: 1.1268\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3803 - val_loss: 1.1266\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3792 - val_loss: 1.1264\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3781 - val_loss: 1.1262\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3771 - val_loss: 1.1259\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3761 - val_loss: 1.1257\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3750 - val_loss: 1.1255\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3740 - val_loss: 1.1253\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3730 - val_loss: 1.1251\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3721 - val_loss: 1.1249\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3711 - val_loss: 1.1247\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 993839931.13901, my average MASE = 2723871222.022616\n",
      "Cluster 1, 993839931.13901\n",
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=5, 5, 75, (319, 67)\n",
      "Before prediction: train_X.shape=(185, 10, 67), train_y.shape=(185, 67), test_X.shape=(62, 10, 67), test_y.shape=(62, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.6944 - val_loss: 0.6444\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6929 - val_loss: 0.6434\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6914 - val_loss: 0.6425\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6901 - val_loss: 0.6415\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6887 - val_loss: 0.6406\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6874 - val_loss: 0.6397\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6860 - val_loss: 0.6388\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6847 - val_loss: 0.6380\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6834 - val_loss: 0.6372\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6822 - val_loss: 0.6363\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6810 - val_loss: 0.6355\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6798 - val_loss: 0.6347\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6786 - val_loss: 0.6340\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6775 - val_loss: 0.6332\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6763 - val_loss: 0.6325\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6753 - val_loss: 0.6318\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6742 - val_loss: 0.6310\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6732 - val_loss: 0.6303\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6721 - val_loss: 0.6296\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6711 - val_loss: 0.6289\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6701 - val_loss: 0.6283\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6692 - val_loss: 0.6276\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6682 - val_loss: 0.6270\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6672 - val_loss: 0.6263\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6663 - val_loss: 0.6257\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6654 - val_loss: 0.6251\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6645 - val_loss: 0.6244\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6636 - val_loss: 0.6238\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6627 - val_loss: 0.6232\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6618 - val_loss: 0.6226\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6610 - val_loss: 0.6220\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6601 - val_loss: 0.6214\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6593 - val_loss: 0.6208\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6584 - val_loss: 0.6202\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6576 - val_loss: 0.6196\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6568 - val_loss: 0.6191\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6559 - val_loss: 0.6185\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6551 - val_loss: 0.6179\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6543 - val_loss: 0.6173\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6535 - val_loss: 0.6167\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(62, 67), test_y.shape=(62, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 141.38884780958514, my average MASE = 173942620.82410374\n",
      "Cluster 0, 141.38884780958514\n",
      "Before prediction: train_X.shape=(5954, 10, 67), train_y.shape=(5954, 67), test_X.shape=(1985, 10, 67), test_y.shape=(1985, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0665 - val_loss: 0.0455\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0607 - val_loss: 0.0424\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0575 - val_loss: 0.0402\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0551 - val_loss: 0.0384\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0531 - val_loss: 0.0370\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0515 - val_loss: 0.0358\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0502 - val_loss: 0.0347\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0491 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0481 - val_loss: 0.0331\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0473 - val_loss: 0.0324\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0466 - val_loss: 0.0319\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0460 - val_loss: 0.0315\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0454 - val_loss: 0.0311\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0449 - val_loss: 0.0307\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0445 - val_loss: 0.0304\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0440 - val_loss: 0.0301\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0437 - val_loss: 0.0299\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0433 - val_loss: 0.0297\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0430 - val_loss: 0.0294\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0427 - val_loss: 0.0292\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0424 - val_loss: 0.0290\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0422 - val_loss: 0.0288\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0420 - val_loss: 0.0287\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0418 - val_loss: 0.0285\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0416 - val_loss: 0.0284\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0414 - val_loss: 0.0283\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0412 - val_loss: 0.0281\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0411 - val_loss: 0.0280\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0409 - val_loss: 0.0279\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0408 - val_loss: 0.0278\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0407 - val_loss: 0.0278\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0406 - val_loss: 0.0277\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0404 - val_loss: 0.0276\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0403 - val_loss: 0.0276\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0402 - val_loss: 0.0275\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0401 - val_loss: 0.0274\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0400 - val_loss: 0.0274\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0400 - val_loss: 0.0273\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0399 - val_loss: 0.0273\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0398 - val_loss: 0.0272\n",
      "63/63 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(1985, 67), test_y.shape=(1985, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1157555182.6490364, my average MASE = 73085160895.13536\n",
      "Cluster 1, 1157555182.6490364\n",
      "Before prediction: train_X.shape=(37, 10, 67), train_y.shape=(37, 67), test_X.shape=(12, 10, 67), test_y.shape=(12, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5516 - val_loss: 0.5380\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5498 - val_loss: 0.5365\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5480 - val_loss: 0.5350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5462 - val_loss: 0.5335\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5444 - val_loss: 0.5321\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5427 - val_loss: 0.5306\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5410 - val_loss: 0.5292\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5393 - val_loss: 0.5278\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5376 - val_loss: 0.5264\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5360 - val_loss: 0.5250\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5343 - val_loss: 0.5237\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5328 - val_loss: 0.5223\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5312 - val_loss: 0.5210\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5297 - val_loss: 0.5197\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5281 - val_loss: 0.5184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5266 - val_loss: 0.5171\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5252 - val_loss: 0.5158\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5237 - val_loss: 0.5146\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5223 - val_loss: 0.5133\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5208 - val_loss: 0.5121\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5194 - val_loss: 0.5109\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5181 - val_loss: 0.5098\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5167 - val_loss: 0.5086\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5154 - val_loss: 0.5075\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5141 - val_loss: 0.5063\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5128 - val_loss: 0.5052\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5115 - val_loss: 0.5041\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5103 - val_loss: 0.5031\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5091 - val_loss: 0.5020\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5078 - val_loss: 0.5010\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5066 - val_loss: 0.5000\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5055 - val_loss: 0.4990\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5043 - val_loss: 0.4980\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5031 - val_loss: 0.4970\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5020 - val_loss: 0.4961\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5008 - val_loss: 0.4952\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4997 - val_loss: 0.4943\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4986 - val_loss: 0.4934\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4975 - val_loss: 0.4926\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4964 - val_loss: 0.4917\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(12, 67), test_y.shape=(12, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 4324881186.205308, my average MASE = 10393885205.318085\n",
      "Cluster 2, 4324881186.205308\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4066 - val_loss: 0.3845\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4060 - val_loss: 0.3840\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4054 - val_loss: 0.3836\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4048 - val_loss: 0.3831\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4042 - val_loss: 0.3826\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4036 - val_loss: 0.3822\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4031 - val_loss: 0.3817\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4025 - val_loss: 0.3812\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4019 - val_loss: 0.3808\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4014 - val_loss: 0.3803\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4008 - val_loss: 0.3799\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4003 - val_loss: 0.3795\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3998 - val_loss: 0.3791\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3992 - val_loss: 0.3787\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3987 - val_loss: 0.3783\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3982 - val_loss: 0.3779\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3977 - val_loss: 0.3775\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3971 - val_loss: 0.3772\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3966 - val_loss: 0.3768\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3961 - val_loss: 0.3764\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3956 - val_loss: 0.3761\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3951 - val_loss: 0.3757\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3946 - val_loss: 0.3754\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3942 - val_loss: 0.3751\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3937 - val_loss: 0.3747\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3932 - val_loss: 0.3744\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3927 - val_loss: 0.3741\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3922 - val_loss: 0.3738\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3918 - val_loss: 0.3735\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3913 - val_loss: 0.3732\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3908 - val_loss: 0.3729\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3903 - val_loss: 0.3726\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3899 - val_loss: 0.3723\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3894 - val_loss: 0.3720\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3890 - val_loss: 0.3718\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3885 - val_loss: 0.3715\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3881 - val_loss: 0.3712\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3877 - val_loss: 0.3709\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3872 - val_loss: 0.3706\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3868 - val_loss: 0.3704\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n",
      "average MASE = 1110146.2653270473, my average MASE = 43278995.990564056\n",
      "Cluster 3, 1110146.2653270473\n",
      "Before prediction: train_X.shape=(25, 10, 67), train_y.shape=(25, 67), test_X.shape=(8, 10, 67), test_y.shape=(8, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.5208 - val_loss: 0.4711\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5196 - val_loss: 0.4707\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5184 - val_loss: 0.4702\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5173 - val_loss: 0.4698\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5161 - val_loss: 0.4694\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5150 - val_loss: 0.4690\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5138 - val_loss: 0.4686\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5127 - val_loss: 0.4682\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5116 - val_loss: 0.4678\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5105 - val_loss: 0.4675\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5094 - val_loss: 0.4671\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5084 - val_loss: 0.4667\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5073 - val_loss: 0.4664\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5063 - val_loss: 0.4660\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5052 - val_loss: 0.4657\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5042 - val_loss: 0.4654\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5031 - val_loss: 0.4651\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5021 - val_loss: 0.4648\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5011 - val_loss: 0.4645\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5001 - val_loss: 0.4643\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4991 - val_loss: 0.4641\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4981 - val_loss: 0.4638\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4971 - val_loss: 0.4636\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4962 - val_loss: 0.4634\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4952 - val_loss: 0.4632\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4942 - val_loss: 0.4629\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4933 - val_loss: 0.4627\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4923 - val_loss: 0.4625\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4914 - val_loss: 0.4623\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4904 - val_loss: 0.4621\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4895 - val_loss: 0.4619\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4886 - val_loss: 0.4617\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4876 - val_loss: 0.4616\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4867 - val_loss: 0.4614\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4858 - val_loss: 0.4613\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4849 - val_loss: 0.4611\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4841 - val_loss: 0.4610\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4832 - val_loss: 0.4608\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4823 - val_loss: 0.4607\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4815 - val_loss: 0.4605\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_original.shape=(8, 67), test_y.shape=(8, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 111.6892159594304, my average MASE = 112347014.53903662\n",
      "Cluster 4, 111.6892159594304\n",
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=7, 7, 884, (12, 67)\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(5954, 10, 67), train_y.shape=(5954, 67), test_X.shape=(1985, 10, 67), test_y.shape=(1985, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0672 - val_loss: 0.0440\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0611 - val_loss: 0.0410\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0576 - val_loss: 0.0390\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0552 - val_loss: 0.0374\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0533 - val_loss: 0.0361\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0517 - val_loss: 0.0350\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0504 - val_loss: 0.0340\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0493 - val_loss: 0.0332\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0484 - val_loss: 0.0325\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0476 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0469 - val_loss: 0.0313\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0462 - val_loss: 0.0308\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0457 - val_loss: 0.0303\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0451 - val_loss: 0.0299\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0447 - val_loss: 0.0296\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0442 - val_loss: 0.0293\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0438 - val_loss: 0.0290\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0435 - val_loss: 0.0287\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0431 - val_loss: 0.0285\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0428 - val_loss: 0.0283\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0425 - val_loss: 0.0281\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0423 - val_loss: 0.0280\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0420 - val_loss: 0.0278\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0418 - val_loss: 0.0277\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0416 - val_loss: 0.0275\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0415 - val_loss: 0.0274\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0413 - val_loss: 0.0273\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0411 - val_loss: 0.0271\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0410 - val_loss: 0.0270\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0408 - val_loss: 0.0269\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0407 - val_loss: 0.0268\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0406 - val_loss: 0.0267\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0405 - val_loss: 0.0266\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0403 - val_loss: 0.0265\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0402 - val_loss: 0.0265\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0401 - val_loss: 0.0264\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0400 - val_loss: 0.0263\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0399 - val_loss: 0.0263\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0398 - val_loss: 0.0262\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0398 - val_loss: 0.0261\n",
      "63/63 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(1985, 67), test_y.shape=(1985, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1103404601.9868512, my average MASE = 55700347192.937294\n",
      "Cluster 1, 1103404601.9868512\n",
      "Before prediction: train_X.shape=(34, 10, 67), train_y.shape=(34, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5205 - val_loss: 0.4218\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5186 - val_loss: 0.4208\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5167 - val_loss: 0.4198\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5148 - val_loss: 0.4189\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5129 - val_loss: 0.4179\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5111 - val_loss: 0.4170\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5093 - val_loss: 0.4161\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5074 - val_loss: 0.4152\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5057 - val_loss: 0.4143\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5039 - val_loss: 0.4134\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5022 - val_loss: 0.4125\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5004 - val_loss: 0.4117\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4987 - val_loss: 0.4108\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4971 - val_loss: 0.4100\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4954 - val_loss: 0.4092\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4938 - val_loss: 0.4083\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4922 - val_loss: 0.4075\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4906 - val_loss: 0.4067\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4890 - val_loss: 0.4060\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4875 - val_loss: 0.4052\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4860 - val_loss: 0.4044\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4845 - val_loss: 0.4036\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4830 - val_loss: 0.4028\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4815 - val_loss: 0.4021\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4800 - val_loss: 0.4013\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4786 - val_loss: 0.4006\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4771 - val_loss: 0.3999\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4757 - val_loss: 0.3992\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4743 - val_loss: 0.3985\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4730 - val_loss: 0.3978\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4716 - val_loss: 0.3971\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4703 - val_loss: 0.3964\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4689 - val_loss: 0.3958\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4676 - val_loss: 0.3951\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4663 - val_loss: 0.3945\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4650 - val_loss: 0.3939\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4637 - val_loss: 0.3932\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4625 - val_loss: 0.3926\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4612 - val_loss: 0.3920\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4600 - val_loss: 0.3913\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3560489905.356454, my average MASE = 9030960108.787361\n",
      "Cluster 2, 3560489905.356454\n",
      "Before prediction: train_X.shape=(96, 10, 67), train_y.shape=(96, 67), test_X.shape=(32, 10, 67), test_y.shape=(32, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3774 - val_loss: 0.3465\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3767 - val_loss: 0.3460\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3759 - val_loss: 0.3456\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3752 - val_loss: 0.3452\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3744 - val_loss: 0.3448\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3737 - val_loss: 0.3444\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3730 - val_loss: 0.3440\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3723 - val_loss: 0.3436\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3716 - val_loss: 0.3432\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3710 - val_loss: 0.3429\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3703 - val_loss: 0.3425\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3696 - val_loss: 0.3421\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3690 - val_loss: 0.3418\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3683 - val_loss: 0.3415\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3677 - val_loss: 0.3411\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3671 - val_loss: 0.3408\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3665 - val_loss: 0.3405\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3659 - val_loss: 0.3401\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3653 - val_loss: 0.3398\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3647 - val_loss: 0.3395\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3641 - val_loss: 0.3392\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3635 - val_loss: 0.3389\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3630 - val_loss: 0.3386\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3624 - val_loss: 0.3383\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3619 - val_loss: 0.3380\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3613 - val_loss: 0.3377\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3608 - val_loss: 0.3374\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3602 - val_loss: 0.3371\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3597 - val_loss: 0.3368\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3592 - val_loss: 0.3365\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3587 - val_loss: 0.3362\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3581 - val_loss: 0.3359\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3576 - val_loss: 0.3356\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3571 - val_loss: 0.3353\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3566 - val_loss: 0.3350\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3561 - val_loss: 0.3347\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3556 - val_loss: 0.3344\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3551 - val_loss: 0.3342\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3546 - val_loss: 0.3339\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3541 - val_loss: 0.3336\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(32, 67), test_y.shape=(32, 67)\n",
      "average MASE = 332.8608286989534, my average MASE = 85442913.95523748\n",
      "Cluster 3, 332.8608286989534\n",
      "Before prediction: train_X.shape=(179, 10, 67), train_y.shape=(179, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.7187 - val_loss: 0.5712\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7172 - val_loss: 0.5706\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7158 - val_loss: 0.5700\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7145 - val_loss: 0.5694\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7132 - val_loss: 0.5688\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7119 - val_loss: 0.5682\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7107 - val_loss: 0.5677\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7095 - val_loss: 0.5671\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7083 - val_loss: 0.5666\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7071 - val_loss: 0.5660\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7060 - val_loss: 0.5655\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7049 - val_loss: 0.5650\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7038 - val_loss: 0.5646\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7028 - val_loss: 0.5641\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7018 - val_loss: 0.5636\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7008 - val_loss: 0.5631\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6999 - val_loss: 0.5627\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6989 - val_loss: 0.5622\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6980 - val_loss: 0.5618\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6971 - val_loss: 0.5613\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6962 - val_loss: 0.5609\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6953 - val_loss: 0.5604\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6944 - val_loss: 0.5600\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6935 - val_loss: 0.5596\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6927 - val_loss: 0.5592\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6918 - val_loss: 0.5587\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6910 - val_loss: 0.5583\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6902 - val_loss: 0.5579\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6894 - val_loss: 0.5576\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6886 - val_loss: 0.5572\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6878 - val_loss: 0.5568\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6870 - val_loss: 0.5564\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6862 - val_loss: 0.5560\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6854 - val_loss: 0.5557\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6846 - val_loss: 0.5553\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6839 - val_loss: 0.5549\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6831 - val_loss: 0.5546\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6824 - val_loss: 0.5542\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6816 - val_loss: 0.5538\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6809 - val_loss: 0.5535\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 139.45484282658396, my average MASE = 350849744.5231373\n",
      "Cluster 4, 139.45484282658396\n",
      "Before prediction: train_X.shape=(133, 10, 67), train_y.shape=(133, 67), test_X.shape=(44, 10, 67), test_y.shape=(44, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4266 - val_loss: 0.6651\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4259 - val_loss: 0.6645\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4253 - val_loss: 0.6639\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4247 - val_loss: 0.6633\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4242 - val_loss: 0.6628\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4236 - val_loss: 0.6623\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4231 - val_loss: 0.6618\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4226 - val_loss: 0.6613\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4221 - val_loss: 0.6608\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4216 - val_loss: 0.6603\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4211 - val_loss: 0.6599\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4206 - val_loss: 0.6594\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4201 - val_loss: 0.6590\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4197 - val_loss: 0.6585\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4192 - val_loss: 0.6581\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4188 - val_loss: 0.6576\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4184 - val_loss: 0.6572\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4180 - val_loss: 0.6568\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4175 - val_loss: 0.6564\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4171 - val_loss: 0.6559\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4167 - val_loss: 0.6554\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4163 - val_loss: 0.6550\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4159 - val_loss: 0.6545\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4155 - val_loss: 0.6541\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4151 - val_loss: 0.6537\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4147 - val_loss: 0.6533\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4143 - val_loss: 0.6529\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4139 - val_loss: 0.6525\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4135 - val_loss: 0.6521\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4131 - val_loss: 0.6518\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4128 - val_loss: 0.6514\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4124 - val_loss: 0.6511\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4120 - val_loss: 0.6507\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4117 - val_loss: 0.6504\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4114 - val_loss: 0.6501\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4110 - val_loss: 0.6498\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4107 - val_loss: 0.6494\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4103 - val_loss: 0.6491\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4100 - val_loss: 0.6487\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4096 - val_loss: 0.6484\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(44, 67), test_y.shape=(44, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 116.65244791791511, my average MASE = 142786035.78420427\n",
      "Cluster 5, 116.65244791791511\n",
      "Before prediction: train_X.shape=(78, 10, 67), train_y.shape=(78, 67), test_X.shape=(26, 10, 67), test_y.shape=(26, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4078 - val_loss: 0.4868\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4070 - val_loss: 0.4862\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4062 - val_loss: 0.4856\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4055 - val_loss: 0.4850\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4048 - val_loss: 0.4845\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4041 - val_loss: 0.4839\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4034 - val_loss: 0.4833\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4027 - val_loss: 0.4828\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4021 - val_loss: 0.4823\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4015 - val_loss: 0.4818\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4008 - val_loss: 0.4813\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4002 - val_loss: 0.4808\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3997 - val_loss: 0.4803\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3991 - val_loss: 0.4798\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3985 - val_loss: 0.4794\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3980 - val_loss: 0.4789\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3974 - val_loss: 0.4785\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3969 - val_loss: 0.4780\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3964 - val_loss: 0.4776\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3959 - val_loss: 0.4772\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3954 - val_loss: 0.4768\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3949 - val_loss: 0.4764\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3944 - val_loss: 0.4760\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3939 - val_loss: 0.4757\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3934 - val_loss: 0.4753\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3930 - val_loss: 0.4750\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3925 - val_loss: 0.4747\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3921 - val_loss: 0.4743\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3916 - val_loss: 0.4740\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3912 - val_loss: 0.4737\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3907 - val_loss: 0.4733\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3903 - val_loss: 0.4730\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3899 - val_loss: 0.4727\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3895 - val_loss: 0.4724\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3890 - val_loss: 0.4721\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3886 - val_loss: 0.4718\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3882 - val_loss: 0.4715\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3878 - val_loss: 0.4712\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3874 - val_loss: 0.4709\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3870 - val_loss: 0.4706\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(26, 67), test_y.shape=(26, 67)\n",
      "average MASE = 221.65144919995234, my average MASE = 102271349.10812314\n",
      "Cluster 6, 221.65144919995234\n",
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=9, 9, 14, (3245, 67)\n",
      "Before prediction: train_X.shape=(1940, 10, 67), train_y.shape=(1940, 67), test_X.shape=(647, 10, 67), test_y.shape=(647, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.1141 - val_loss: 0.1031\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1086 - val_loss: 0.1010\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1042 - val_loss: 0.0995\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1005 - val_loss: 0.0982\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0973 - val_loss: 0.0973\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0945 - val_loss: 0.0965\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0920 - val_loss: 0.0960\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0897 - val_loss: 0.0955\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0877 - val_loss: 0.0950\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0858 - val_loss: 0.0947\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0841 - val_loss: 0.0943\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0826 - val_loss: 0.0941\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0812 - val_loss: 0.0938\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0798 - val_loss: 0.0935\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0786 - val_loss: 0.0933\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0775 - val_loss: 0.0930\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0765 - val_loss: 0.0928\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0755 - val_loss: 0.0926\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0746 - val_loss: 0.0925\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0737 - val_loss: 0.0923\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0729 - val_loss: 0.0921\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0722 - val_loss: 0.0920\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0715 - val_loss: 0.0919\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0708 - val_loss: 0.0917\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0702 - val_loss: 0.0916\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0696 - val_loss: 0.0915\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0691 - val_loss: 0.0914\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0686 - val_loss: 0.0913\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0681 - val_loss: 0.0912\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0676 - val_loss: 0.0911\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0672 - val_loss: 0.0910\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0668 - val_loss: 0.0910\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0664 - val_loss: 0.0909\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0660 - val_loss: 0.0909\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0656 - val_loss: 0.0908\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0653 - val_loss: 0.0908\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0650 - val_loss: 0.0907\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0647 - val_loss: 0.0907\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0644 - val_loss: 0.0907\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0641 - val_loss: 0.0906\n",
      "21/21 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(647, 67), test_y.shape=(647, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1289233887.8234715, my average MASE = 15636865244.557127\n",
      "Cluster 0, 1289233887.8234715\n",
      "Before prediction: train_X.shape=(29, 10, 67), train_y.shape=(29, 67), test_X.shape=(10, 10, 67), test_y.shape=(10, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4693 - val_loss: 0.6570\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4687 - val_loss: 0.6567\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4680 - val_loss: 0.6563\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4674 - val_loss: 0.6560\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4668 - val_loss: 0.6557\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4662 - val_loss: 0.6554\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4656 - val_loss: 0.6551\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4650 - val_loss: 0.6548\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4644 - val_loss: 0.6545\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4638 - val_loss: 0.6542\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4633 - val_loss: 0.6539\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4627 - val_loss: 0.6536\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4621 - val_loss: 0.6533\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4616 - val_loss: 0.6530\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4610 - val_loss: 0.6527\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4605 - val_loss: 0.6524\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4599 - val_loss: 0.6521\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4594 - val_loss: 0.6519\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4588 - val_loss: 0.6516\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4583 - val_loss: 0.6513\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4578 - val_loss: 0.6510\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4573 - val_loss: 0.6508\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4568 - val_loss: 0.6505\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4563 - val_loss: 0.6502\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4558 - val_loss: 0.6499\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4553 - val_loss: 0.6496\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4548 - val_loss: 0.6493\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4543 - val_loss: 0.6491\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4538 - val_loss: 0.6488\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4533 - val_loss: 0.6485\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4528 - val_loss: 0.6483\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4524 - val_loss: 0.6480\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4519 - val_loss: 0.6478\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4514 - val_loss: 0.6475\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4510 - val_loss: 0.6473\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4505 - val_loss: 0.6470\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4500 - val_loss: 0.6468\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4496 - val_loss: 0.6465\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4492 - val_loss: 0.6463\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4487 - val_loss: 0.6460\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(10, 67), test_y.shape=(10, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 10049120.449906044, my average MASE = 246733533.9412494\n",
      "Cluster 1, 10049120.449906044\n",
      "Before prediction: train_X.shape=(1564, 10, 67), train_y.shape=(1564, 67), test_X.shape=(521, 10, 67), test_y.shape=(521, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2359 - val_loss: 0.2748\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2264 - val_loss: 0.2656\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2192 - val_loss: 0.2584\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.2135 - val_loss: 0.2523\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2087 - val_loss: 0.2471\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.2045 - val_loss: 0.2424\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2007 - val_loss: 0.2382\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1973 - val_loss: 0.2342\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1941 - val_loss: 0.2307\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1912 - val_loss: 0.2273\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1884 - val_loss: 0.2242\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1858 - val_loss: 0.2214\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1833 - val_loss: 0.2187\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1810 - val_loss: 0.2163\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1789 - val_loss: 0.2140\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1769 - val_loss: 0.2119\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1750 - val_loss: 0.2100\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1733 - val_loss: 0.2082\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1717 - val_loss: 0.2066\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1702 - val_loss: 0.2051\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1689 - val_loss: 0.2038\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1677 - val_loss: 0.2024\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1666 - val_loss: 0.2012\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1655 - val_loss: 0.2001\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1645 - val_loss: 0.1989\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1636 - val_loss: 0.1979\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1627 - val_loss: 0.1969\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1619 - val_loss: 0.1959\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1611 - val_loss: 0.1950\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1603 - val_loss: 0.1941\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1596 - val_loss: 0.1933\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1589 - val_loss: 0.1925\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1582 - val_loss: 0.1917\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1576 - val_loss: 0.1910\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1570 - val_loss: 0.1903\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1564 - val_loss: 0.1895\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.1558 - val_loss: 0.1889\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1553 - val_loss: 0.1883\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1548 - val_loss: 0.1877\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1543 - val_loss: 0.1871\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(521, 67), test_y.shape=(521, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 187.65491616156584, my average MASE = 1727876346.7517292\n",
      "Cluster 2, 187.65491616156584\n",
      "Before prediction: train_X.shape=(34, 10, 67), train_y.shape=(34, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5179 - val_loss: 0.4772\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5158 - val_loss: 0.4757\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5138 - val_loss: 0.4743\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5118 - val_loss: 0.4729\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5098 - val_loss: 0.4715\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5079 - val_loss: 0.4701\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5059 - val_loss: 0.4687\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5040 - val_loss: 0.4673\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5021 - val_loss: 0.4660\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5002 - val_loss: 0.4646\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4984 - val_loss: 0.4633\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4966 - val_loss: 0.4620\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4948 - val_loss: 0.4607\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4931 - val_loss: 0.4594\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4913 - val_loss: 0.4581\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4896 - val_loss: 0.4568\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4879 - val_loss: 0.4555\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4862 - val_loss: 0.4543\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4845 - val_loss: 0.4531\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4828 - val_loss: 0.4518\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4812 - val_loss: 0.4506\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4795 - val_loss: 0.4495\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4779 - val_loss: 0.4483\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4763 - val_loss: 0.4471\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4747 - val_loss: 0.4460\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4731 - val_loss: 0.4449\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4716 - val_loss: 0.4438\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4700 - val_loss: 0.4427\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4685 - val_loss: 0.4416\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4670 - val_loss: 0.4405\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4655 - val_loss: 0.4395\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4640 - val_loss: 0.4385\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4626 - val_loss: 0.4374\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4611 - val_loss: 0.4364\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4597 - val_loss: 0.4354\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4582 - val_loss: 0.4344\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4568 - val_loss: 0.4334\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4555 - val_loss: 0.4324\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4541 - val_loss: 0.4314\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4527 - val_loss: 0.4304\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3258333520.23763, my average MASE = 7487984704.271554\n",
      "Cluster 3, 3258333520.23763\n",
      "Before prediction: train_X.shape=(11, 10, 67), train_y.shape=(11, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3367 - val_loss: 0.3564\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3362 - val_loss: 0.3564\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3356 - val_loss: 0.3564\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3351 - val_loss: 0.3563\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3346 - val_loss: 0.3563\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3340 - val_loss: 0.3563\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3335 - val_loss: 0.3563\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3330 - val_loss: 0.3562\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3325 - val_loss: 0.3562\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3319 - val_loss: 0.3562\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3314 - val_loss: 0.3561\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3309 - val_loss: 0.3561\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3304 - val_loss: 0.3561\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3299 - val_loss: 0.3560\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3294 - val_loss: 0.3560\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3289 - val_loss: 0.3560\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3285 - val_loss: 0.3559\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3280 - val_loss: 0.3559\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3275 - val_loss: 0.3559\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3270 - val_loss: 0.3559\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3266 - val_loss: 0.3558\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3261 - val_loss: 0.3558\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3256 - val_loss: 0.3558\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3252 - val_loss: 0.3558\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3248 - val_loss: 0.3558\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3243 - val_loss: 0.3557\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3239 - val_loss: 0.3557\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3235 - val_loss: 0.3557\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3230 - val_loss: 0.3557\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3226 - val_loss: 0.3557\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3222 - val_loss: 0.3556\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3218 - val_loss: 0.3556\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3214 - val_loss: 0.3556\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3209 - val_loss: 0.3556\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3205 - val_loss: 0.3556\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3201 - val_loss: 0.3556\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3197 - val_loss: 0.3556\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3193 - val_loss: 0.3555\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3189 - val_loss: 0.3555\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3185 - val_loss: 0.3555\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 544.8910068645505, my average MASE = 25915172.7591836\n",
      "Cluster 4, 544.8910068645505\n",
      "Before prediction: train_X.shape=(17, 10, 67), train_y.shape=(17, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5189 - val_loss: 0.8510\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5183 - val_loss: 0.8507\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5176 - val_loss: 0.8503\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5170 - val_loss: 0.8500\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5163 - val_loss: 0.8496\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5157 - val_loss: 0.8493\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5151 - val_loss: 0.8490\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5145 - val_loss: 0.8487\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5138 - val_loss: 0.8484\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5132 - val_loss: 0.8481\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5126 - val_loss: 0.8478\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5120 - val_loss: 0.8476\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5114 - val_loss: 0.8473\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5108 - val_loss: 0.8471\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5102 - val_loss: 0.8468\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5096 - val_loss: 0.8466\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5090 - val_loss: 0.8463\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5084 - val_loss: 0.8461\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5078 - val_loss: 0.8458\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5072 - val_loss: 0.8456\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5066 - val_loss: 0.8454\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5060 - val_loss: 0.8451\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5055 - val_loss: 0.8449\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5049 - val_loss: 0.8447\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5043 - val_loss: 0.8445\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5038 - val_loss: 0.8443\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5032 - val_loss: 0.8441\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5027 - val_loss: 0.8439\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5021 - val_loss: 0.8437\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5016 - val_loss: 0.8435\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5010 - val_loss: 0.8433\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5005 - val_loss: 0.8431\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4999 - val_loss: 0.8429\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4994 - val_loss: 0.8428\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4989 - val_loss: 0.8426\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4983 - val_loss: 0.8424\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4978 - val_loss: 0.8422\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4973 - val_loss: 0.8420\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4967 - val_loss: 0.8419\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4962 - val_loss: 0.8417\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1963484.3822421303, my average MASE = 64800710.430740915\n",
      "Cluster 5, 1963484.3822421303\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8495 - val_loss: 4.3516\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8473 - val_loss: 4.3510\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8451 - val_loss: 4.3503\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8429 - val_loss: 4.3497\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8408 - val_loss: 4.3491\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8386 - val_loss: 4.3485\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8365 - val_loss: 4.3479\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8344 - val_loss: 4.3473\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8323 - val_loss: 4.3468\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8301 - val_loss: 4.3462\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8280 - val_loss: 4.3456\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8259 - val_loss: 4.3451\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8238 - val_loss: 4.3445\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8217 - val_loss: 4.3440\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8197 - val_loss: 4.3434\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8176 - val_loss: 4.3428\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8156 - val_loss: 4.3423\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8136 - val_loss: 4.3417\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8117 - val_loss: 4.3412\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8098 - val_loss: 4.3407\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8079 - val_loss: 4.3403\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8061 - val_loss: 4.3399\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8042 - val_loss: 4.3395\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8023 - val_loss: 4.3390\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8004 - val_loss: 4.3386\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7986 - val_loss: 4.3382\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7967 - val_loss: 4.3378\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7949 - val_loss: 4.3374\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7930 - val_loss: 4.3370\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7912 - val_loss: 4.3365\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7893 - val_loss: 4.3361\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7874 - val_loss: 4.3357\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7856 - val_loss: 4.3353\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7839 - val_loss: 4.3349\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7822 - val_loss: 4.3345\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7805 - val_loss: 4.3341\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7787 - val_loss: 4.3338\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7771 - val_loss: 4.3334\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7754 - val_loss: 4.3330\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7738 - val_loss: 4.3327\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 4.116072897890656, my average MASE = 12.21206426084865\n",
      "Cluster 6, 4.116072897890656\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(96, 10, 67), train_y.shape=(96, 67), test_X.shape=(32, 10, 67), test_y.shape=(32, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3660 - val_loss: 0.3458\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3652 - val_loss: 0.3455\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3645 - val_loss: 0.3451\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3638 - val_loss: 0.3448\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3632 - val_loss: 0.3445\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3625 - val_loss: 0.3441\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3619 - val_loss: 0.3438\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3612 - val_loss: 0.3435\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3606 - val_loss: 0.3431\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3599 - val_loss: 0.3428\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3593 - val_loss: 0.3425\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3587 - val_loss: 0.3422\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3581 - val_loss: 0.3419\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3576 - val_loss: 0.3416\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3570 - val_loss: 0.3413\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3564 - val_loss: 0.3410\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3558 - val_loss: 0.3407\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3553 - val_loss: 0.3404\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3547 - val_loss: 0.3401\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3542 - val_loss: 0.3398\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3536 - val_loss: 0.3395\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3531 - val_loss: 0.3393\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3526 - val_loss: 0.3390\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3521 - val_loss: 0.3387\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3516 - val_loss: 0.3384\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3511 - val_loss: 0.3382\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3506 - val_loss: 0.3379\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3501 - val_loss: 0.3376\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3496 - val_loss: 0.3373\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3491 - val_loss: 0.3371\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3487 - val_loss: 0.3368\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3482 - val_loss: 0.3366\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3477 - val_loss: 0.3363\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3473 - val_loss: 0.3360\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3468 - val_loss: 0.3358\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3464 - val_loss: 0.3355\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3459 - val_loss: 0.3353\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3455 - val_loss: 0.3350\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3451 - val_loss: 0.3348\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3446 - val_loss: 0.3345\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(32, 67), test_y.shape=(32, 67)\n",
      "average MASE = 266.09609302896547, my average MASE = 71685898.00003502\n",
      "Cluster 8, 266.09609302896547\n",
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=11, 11, 663, (11, 67)\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3610 - val_loss: 0.4969\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3601 - val_loss: 0.4969\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3592 - val_loss: 0.4970\n",
      "Epoch 3: early stopping\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.35712691602972574, my average MASE = 0.5241910168311086\n",
      "Cluster 0, 0.35712691602972574\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2752 - val_loss: 0.2714\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2747 - val_loss: 0.2712\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2742 - val_loss: 0.2710\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2737 - val_loss: 0.2707\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2733 - val_loss: 0.2705\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2728 - val_loss: 0.2703\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2723 - val_loss: 0.2701\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2718 - val_loss: 0.2699\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2714 - val_loss: 0.2697\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2709 - val_loss: 0.2695\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2704 - val_loss: 0.2693\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2700 - val_loss: 0.2691\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2695 - val_loss: 0.2690\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2691 - val_loss: 0.2688\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2687 - val_loss: 0.2686\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2682 - val_loss: 0.2684\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2678 - val_loss: 0.2682\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2673 - val_loss: 0.2680\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2669 - val_loss: 0.2678\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2665 - val_loss: 0.2676\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2661 - val_loss: 0.2674\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2657 - val_loss: 0.2672\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2653 - val_loss: 0.2670\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2649 - val_loss: 0.2669\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2645 - val_loss: 0.2667\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2641 - val_loss: 0.2665\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2637 - val_loss: 0.2663\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2633 - val_loss: 0.2662\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2629 - val_loss: 0.2660\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2625 - val_loss: 0.2658\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2621 - val_loss: 0.2656\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2618 - val_loss: 0.2655\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2614 - val_loss: 0.2653\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2610 - val_loss: 0.2652\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2606 - val_loss: 0.2650\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2602 - val_loss: 0.2649\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2599 - val_loss: 0.2647\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2595 - val_loss: 0.2646\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2591 - val_loss: 0.2644\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2588 - val_loss: 0.2643\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 114.92828938190037, my average MASE = 64775351.69732988\n",
      "Cluster 1, 114.92828938190037\n",
      "Before prediction: train_X.shape=(21, 10, 67), train_y.shape=(21, 67), test_X.shape=(7, 10, 67), test_y.shape=(7, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6587 - val_loss: 0.2666\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6569 - val_loss: 0.2655\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6551 - val_loss: 0.2645\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6533 - val_loss: 0.2635\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6515 - val_loss: 0.2624\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6497 - val_loss: 0.2614\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6480 - val_loss: 0.2604\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6463 - val_loss: 0.2594\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6446 - val_loss: 0.2584\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6429 - val_loss: 0.2574\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6412 - val_loss: 0.2565\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6395 - val_loss: 0.2555\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6379 - val_loss: 0.2546\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6363 - val_loss: 0.2538\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6346 - val_loss: 0.2529\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6331 - val_loss: 0.2520\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6315 - val_loss: 0.2512\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6300 - val_loss: 0.2504\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6285 - val_loss: 0.2496\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6270 - val_loss: 0.2488\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6255 - val_loss: 0.2480\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6240 - val_loss: 0.2473\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6225 - val_loss: 0.2466\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6211 - val_loss: 0.2458\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6197 - val_loss: 0.2451\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6182 - val_loss: 0.2444\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6168 - val_loss: 0.2437\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6155 - val_loss: 0.2430\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6141 - val_loss: 0.2423\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6127 - val_loss: 0.2416\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6114 - val_loss: 0.2410\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6101 - val_loss: 0.2403\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6088 - val_loss: 0.2397\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6075 - val_loss: 0.2390\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6063 - val_loss: 0.2384\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6050 - val_loss: 0.2378\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6038 - val_loss: 0.2372\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6026 - val_loss: 0.2365\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6014 - val_loss: 0.2359\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6002 - val_loss: 0.2353\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_original.shape=(7, 67), test_y.shape=(7, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 26456338.054780327, my average MASE = 1170397376.6808207\n",
      "Cluster 2, 26456338.054780327\n",
      "Before prediction: train_X.shape=(62, 10, 67), train_y.shape=(62, 67), test_X.shape=(21, 10, 67), test_y.shape=(21, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3828 - val_loss: 0.5067\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3824 - val_loss: 0.5064\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3820 - val_loss: 0.5062\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3816 - val_loss: 0.5059\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3812 - val_loss: 0.5056\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3808 - val_loss: 0.5053\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3804 - val_loss: 0.5051\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3801 - val_loss: 0.5048\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3797 - val_loss: 0.5045\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3793 - val_loss: 0.5043\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3790 - val_loss: 0.5040\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3786 - val_loss: 0.5038\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3783 - val_loss: 0.5035\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3779 - val_loss: 0.5032\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3776 - val_loss: 0.5030\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3772 - val_loss: 0.5027\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3769 - val_loss: 0.5025\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3765 - val_loss: 0.5022\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3762 - val_loss: 0.5020\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3758 - val_loss: 0.5017\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3755 - val_loss: 0.5015\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3752 - val_loss: 0.5012\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3748 - val_loss: 0.5010\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3745 - val_loss: 0.5008\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3742 - val_loss: 0.5005\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3738 - val_loss: 0.5003\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3735 - val_loss: 0.5000\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3732 - val_loss: 0.4998\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3729 - val_loss: 0.4995\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3726 - val_loss: 0.4993\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3722 - val_loss: 0.4991\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3719 - val_loss: 0.4988\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3716 - val_loss: 0.4986\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3713 - val_loss: 0.4984\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3710 - val_loss: 0.4982\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3707 - val_loss: 0.4979\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3704 - val_loss: 0.4977\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3701 - val_loss: 0.4975\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3698 - val_loss: 0.4973\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3695 - val_loss: 0.4971\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "predicted_original.shape=(21, 67), test_y.shape=(21, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 88.873907317969, my average MASE = 105114880.66517946\n",
      "Cluster 3, 88.873907317969\n",
      "Before prediction: train_X.shape=(58, 10, 67), train_y.shape=(58, 67), test_X.shape=(19, 10, 67), test_y.shape=(19, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.6113 - val_loss: 0.3397\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6107 - val_loss: 0.3395\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6100 - val_loss: 0.3394\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6094 - val_loss: 0.3393\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6087 - val_loss: 0.3392\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6081 - val_loss: 0.3391\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6075 - val_loss: 0.3390\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6069 - val_loss: 0.3389\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6062 - val_loss: 0.3388\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6056 - val_loss: 0.3387\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6050 - val_loss: 0.3386\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6044 - val_loss: 0.3385\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6038 - val_loss: 0.3384\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6032 - val_loss: 0.3383\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6026 - val_loss: 0.3382\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6020 - val_loss: 0.3381\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6014 - val_loss: 0.3380\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6009 - val_loss: 0.3379\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6003 - val_loss: 0.3378\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5997 - val_loss: 0.3377\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5992 - val_loss: 0.3376\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5986 - val_loss: 0.3375\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5981 - val_loss: 0.3374\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5976 - val_loss: 0.3373\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5970 - val_loss: 0.3372\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5965 - val_loss: 0.3372\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5960 - val_loss: 0.3371\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5955 - val_loss: 0.3370\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5949 - val_loss: 0.3369\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5944 - val_loss: 0.3369\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5939 - val_loss: 0.3368\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5934 - val_loss: 0.3367\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5929 - val_loss: 0.3366\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5924 - val_loss: 0.3366\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5919 - val_loss: 0.3365\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5915 - val_loss: 0.3364\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5910 - val_loss: 0.3364\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5905 - val_loss: 0.3363\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5900 - val_loss: 0.3362\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5895 - val_loss: 0.3362\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(19, 67), test_y.shape=(19, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2288977.9462372796, my average MASE = 275894539.3328722\n",
      "Cluster 4, 2288977.9462372796\n",
      "Before prediction: train_X.shape=(22, 10, 67), train_y.shape=(22, 67), test_X.shape=(7, 10, 67), test_y.shape=(7, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6507 - val_loss: 0.5866\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6499 - val_loss: 0.5864\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6491 - val_loss: 0.5862\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6483 - val_loss: 0.5861\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6475 - val_loss: 0.5859\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6467 - val_loss: 0.5857\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6459 - val_loss: 0.5855\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6451 - val_loss: 0.5854\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6443 - val_loss: 0.5852\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6435 - val_loss: 0.5851\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6427 - val_loss: 0.5849\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6420 - val_loss: 0.5848\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6412 - val_loss: 0.5846\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6405 - val_loss: 0.5844\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6397 - val_loss: 0.5843\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6389 - val_loss: 0.5841\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6382 - val_loss: 0.5840\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6374 - val_loss: 0.5838\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6367 - val_loss: 0.5837\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6359 - val_loss: 0.5836\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6352 - val_loss: 0.5834\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6345 - val_loss: 0.5833\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6337 - val_loss: 0.5832\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6330 - val_loss: 0.5830\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6323 - val_loss: 0.5829\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6316 - val_loss: 0.5827\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6309 - val_loss: 0.5826\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6302 - val_loss: 0.5825\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6295 - val_loss: 0.5824\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6288 - val_loss: 0.5822\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6281 - val_loss: 0.5821\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6275 - val_loss: 0.5820\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6268 - val_loss: 0.5819\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6261 - val_loss: 0.5818\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6254 - val_loss: 0.5816\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6248 - val_loss: 0.5815\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6241 - val_loss: 0.5814\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6235 - val_loss: 0.5813\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6228 - val_loss: 0.5811\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6222 - val_loss: 0.5810\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(7, 67), test_y.shape=(7, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 56.939779008571485, my average MASE = 37107546.21609385\n",
      "Cluster 5, 56.939779008571485\n",
      "Before prediction: train_X.shape=(4681, 10, 67), train_y.shape=(4681, 67), test_X.shape=(1560, 10, 67), test_y.shape=(1560, 67)\n",
      "Epoch 1/40\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 0.0785 - val_loss: 0.0245\n",
      "Epoch 2/40\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 0.0726 - val_loss: 0.0222\n",
      "Epoch 3/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0688 - val_loss: 0.0210\n",
      "Epoch 4/40\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 0.0661 - val_loss: 0.0201\n",
      "Epoch 5/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0638 - val_loss: 0.0195\n",
      "Epoch 6/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0620 - val_loss: 0.0189\n",
      "Epoch 7/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0604 - val_loss: 0.0185\n",
      "Epoch 8/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0590 - val_loss: 0.0182\n",
      "Epoch 9/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0579 - val_loss: 0.0179\n",
      "Epoch 10/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0569 - val_loss: 0.0176\n",
      "Epoch 11/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0560 - val_loss: 0.0174\n",
      "Epoch 12/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0552 - val_loss: 0.0172\n",
      "Epoch 13/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0545 - val_loss: 0.0171\n",
      "Epoch 14/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0539 - val_loss: 0.0169\n",
      "Epoch 15/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0533 - val_loss: 0.0168\n",
      "Epoch 16/40\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 0.0528 - val_loss: 0.0167\n",
      "Epoch 17/40\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 0.0524 - val_loss: 0.0166\n",
      "Epoch 18/40\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 0.0519 - val_loss: 0.0165\n",
      "Epoch 19/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0516 - val_loss: 0.0165\n",
      "Epoch 20/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0512 - val_loss: 0.0164\n",
      "Epoch 21/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0509 - val_loss: 0.0163\n",
      "Epoch 22/40\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 0.0505 - val_loss: 0.0163\n",
      "Epoch 23/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0502 - val_loss: 0.0162\n",
      "Epoch 24/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0500 - val_loss: 0.0162\n",
      "Epoch 25/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0497 - val_loss: 0.0161\n",
      "Epoch 26/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0495 - val_loss: 0.0161\n",
      "Epoch 27/40\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 0.0492 - val_loss: 0.0161\n",
      "Epoch 28/40\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 0.0490 - val_loss: 0.0160\n",
      "Epoch 29/40\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.0488 - val_loss: 0.0160\n",
      "Epoch 30/40\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 0.0486 - val_loss: 0.0159\n",
      "Epoch 31/40\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 0.0485 - val_loss: 0.0159\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 0.0483 - val_loss: 0.0158\n",
      "Epoch 33/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0481 - val_loss: 0.0158\n",
      "Epoch 34/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0480 - val_loss: 0.0158\n",
      "Epoch 35/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0479 - val_loss: 0.0157\n",
      "Epoch 36/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0477 - val_loss: 0.0157\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0476 - val_loss: 0.0157\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0475 - val_loss: 0.0156\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0474 - val_loss: 0.0156\n",
      "Epoch 40/40\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 0.0473 - val_loss: 0.0156\n",
      "49/49 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(1560, 67), test_y.shape=(1560, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 149408746.52177918, my average MASE = 566572768.961764\n",
      "Cluster 6, 149408746.52177918\n",
      "Before prediction: train_X.shape=(34, 10, 67), train_y.shape=(34, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5377 - val_loss: 0.4682\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5358 - val_loss: 0.4667\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5339 - val_loss: 0.4653\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5320 - val_loss: 0.4638\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5302 - val_loss: 0.4624\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5283 - val_loss: 0.4609\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5265 - val_loss: 0.4595\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5247 - val_loss: 0.4581\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5229 - val_loss: 0.4567\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5211 - val_loss: 0.4554\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5194 - val_loss: 0.4540\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5177 - val_loss: 0.4527\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5160 - val_loss: 0.4514\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5143 - val_loss: 0.4501\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5126 - val_loss: 0.4488\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5110 - val_loss: 0.4475\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5093 - val_loss: 0.4462\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5077 - val_loss: 0.4450\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5062 - val_loss: 0.4438\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5046 - val_loss: 0.4425\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5030 - val_loss: 0.4413\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5015 - val_loss: 0.4401\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5000 - val_loss: 0.4390\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4985 - val_loss: 0.4378\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4970 - val_loss: 0.4367\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4955 - val_loss: 0.4355\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4940 - val_loss: 0.4344\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4925 - val_loss: 0.4334\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4911 - val_loss: 0.4323\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4897 - val_loss: 0.4312\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4882 - val_loss: 0.4302\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4868 - val_loss: 0.4292\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4854 - val_loss: 0.4281\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4840 - val_loss: 0.4271\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4826 - val_loss: 0.4261\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4813 - val_loss: 0.4251\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4799 - val_loss: 0.4242\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4786 - val_loss: 0.4232\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4772 - val_loss: 0.4223\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4759 - val_loss: 0.4214\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3453236106.5511885, my average MASE = 6985920382.132476\n",
      "Cluster 7, 3453236106.5511885\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3084 - val_loss: 0.3104\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3075 - val_loss: 0.3100\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3066 - val_loss: 0.3095\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3058 - val_loss: 0.3091\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3050 - val_loss: 0.3086\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3042 - val_loss: 0.3082\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3034 - val_loss: 0.3077\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3026 - val_loss: 0.3073\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3018 - val_loss: 0.3068\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3010 - val_loss: 0.3064\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3002 - val_loss: 0.3059\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2995 - val_loss: 0.3055\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2987 - val_loss: 0.3051\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2980 - val_loss: 0.3047\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2973 - val_loss: 0.3043\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2965 - val_loss: 0.3039\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2958 - val_loss: 0.3035\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2951 - val_loss: 0.3031\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2944 - val_loss: 0.3027\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2937 - val_loss: 0.3024\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2930 - val_loss: 0.3020\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2924 - val_loss: 0.3017\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2917 - val_loss: 0.3014\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2911 - val_loss: 0.3011\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2904 - val_loss: 0.3008\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2898 - val_loss: 0.3006\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2891 - val_loss: 0.3003\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2885 - val_loss: 0.3001\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2878 - val_loss: 0.2998\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2872 - val_loss: 0.2996\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2866 - val_loss: 0.2994\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2859 - val_loss: 0.2992\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2853 - val_loss: 0.2990\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2846 - val_loss: 0.2988\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2840 - val_loss: 0.2985\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2834 - val_loss: 0.2983\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2827 - val_loss: 0.2981\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2822 - val_loss: 0.2979\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2816 - val_loss: 0.2977\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2810 - val_loss: 0.2975\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.2112740905793214, my average MASE = 0.3813496945040607\n",
      "Cluster 8, 0.2112740905793214\n",
      "Before prediction: train_X.shape=(33, 10, 67), train_y.shape=(33, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5052 - val_loss: 0.6961\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5045 - val_loss: 0.6956\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5038 - val_loss: 0.6950\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5030 - val_loss: 0.6945\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5023 - val_loss: 0.6939\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5016 - val_loss: 0.6934\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5009 - val_loss: 0.6928\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5002 - val_loss: 0.6923\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4995 - val_loss: 0.6918\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4988 - val_loss: 0.6913\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4981 - val_loss: 0.6908\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4975 - val_loss: 0.6903\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4968 - val_loss: 0.6898\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4961 - val_loss: 0.6893\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4955 - val_loss: 0.6888\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4948 - val_loss: 0.6883\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4942 - val_loss: 0.6879\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4935 - val_loss: 0.6874\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4929 - val_loss: 0.6869\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4923 - val_loss: 0.6865\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4917 - val_loss: 0.6860\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4910 - val_loss: 0.6855\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4904 - val_loss: 0.6851\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4898 - val_loss: 0.6846\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4892 - val_loss: 0.6842\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4886 - val_loss: 0.6837\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4880 - val_loss: 0.6833\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4874 - val_loss: 0.6829\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4868 - val_loss: 0.6824\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4863 - val_loss: 0.6820\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4857 - val_loss: 0.6816\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4851 - val_loss: 0.6811\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4845 - val_loss: 0.6807\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4840 - val_loss: 0.6803\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4834 - val_loss: 0.6799\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4828 - val_loss: 0.6794\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4823 - val_loss: 0.6790\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4817 - val_loss: 0.6786\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4811 - val_loss: 0.6782\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4806 - val_loss: 0.6778\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 74.79960857799604, my average MASE = 23518054.91533832\n",
      "Cluster 9, 74.79960857799604\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3521 - val_loss: 0.2993\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3508 - val_loss: 0.2987\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3496 - val_loss: 0.2982\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3483 - val_loss: 0.2976\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3470 - val_loss: 0.2971\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3458 - val_loss: 0.2966\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3446 - val_loss: 0.2961\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3435 - val_loss: 0.2957\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3423 - val_loss: 0.2954\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3412 - val_loss: 0.2950\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3400 - val_loss: 0.2946\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3389 - val_loss: 0.2943\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3378 - val_loss: 0.2939\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3368 - val_loss: 0.2935\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3357 - val_loss: 0.2932\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3347 - val_loss: 0.2928\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3337 - val_loss: 0.2925\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3326 - val_loss: 0.2921\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3317 - val_loss: 0.2918\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3307 - val_loss: 0.2915\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3298 - val_loss: 0.2912\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3289 - val_loss: 0.2910\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3279 - val_loss: 0.2908\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3270 - val_loss: 0.2905\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3261 - val_loss: 0.2903\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3252 - val_loss: 0.2901\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3244 - val_loss: 0.2899\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3235 - val_loss: 0.2897\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3227 - val_loss: 0.2895\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3219 - val_loss: 0.2894\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3211 - val_loss: 0.2892\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3203 - val_loss: 0.2891\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3195 - val_loss: 0.2890\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3187 - val_loss: 0.2889\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3179 - val_loss: 0.2888\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3171 - val_loss: 0.2887\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3163 - val_loss: 0.2886\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3155 - val_loss: 0.2886\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3147 - val_loss: 0.2885\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3139 - val_loss: 0.2885\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.1864435054204945, my average MASE = 0.27675363785512175\n",
      "Cluster 10, 0.1864435054204945\n",
      "clusters_labels.shape=(408091,)\n",
      "N_clusters=2, 2, 18, (30612, 67)\n",
      "Before prediction: train_X.shape=(18361, 10, 67), train_y.shape=(18361, 67), test_X.shape=(6120, 10, 67), test_y.shape=(6120, 67)\n",
      "Epoch 1/40\n",
      "287/287 [==============================] - 8s 30ms/step - loss: 0.3015 - val_loss: 0.3194\n",
      "Epoch 2/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2836 - val_loss: 0.3051\n",
      "Epoch 3/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2695 - val_loss: 0.2935\n",
      "Epoch 4/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2585 - val_loss: 0.2851\n",
      "Epoch 5/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2510 - val_loss: 0.2789\n",
      "Epoch 6/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2456 - val_loss: 0.2737\n",
      "Epoch 7/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2410 - val_loss: 0.2693\n",
      "Epoch 8/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2369 - val_loss: 0.2653\n",
      "Epoch 9/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2332 - val_loss: 0.2616\n",
      "Epoch 10/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2298 - val_loss: 0.2584\n",
      "Epoch 11/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2267 - val_loss: 0.2555\n",
      "Epoch 12/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2238 - val_loss: 0.2531\n",
      "Epoch 13/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2212 - val_loss: 0.2509\n",
      "Epoch 14/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2190 - val_loss: 0.2490\n",
      "Epoch 15/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2172 - val_loss: 0.2474\n",
      "Epoch 16/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2156 - val_loss: 0.2461\n",
      "Epoch 17/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2143 - val_loss: 0.2448\n",
      "Epoch 18/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2131 - val_loss: 0.2438\n",
      "Epoch 19/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2120 - val_loss: 0.2428\n",
      "Epoch 20/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2110 - val_loss: 0.2420\n",
      "Epoch 21/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2101 - val_loss: 0.2412\n",
      "Epoch 22/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2092 - val_loss: 0.2404\n",
      "Epoch 23/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2084 - val_loss: 0.2395\n",
      "Epoch 24/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2077 - val_loss: 0.2390\n",
      "Epoch 25/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2070 - val_loss: 0.2383\n",
      "Epoch 26/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2064 - val_loss: 0.2377\n",
      "Epoch 27/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2058 - val_loss: 0.2371\n",
      "Epoch 28/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2052 - val_loss: 0.2365\n",
      "Epoch 29/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2047 - val_loss: 0.2361\n",
      "Epoch 30/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2041 - val_loss: 0.2356\n",
      "Epoch 31/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2037 - val_loss: 0.2352\n",
      "Epoch 32/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2032 - val_loss: 0.2348\n",
      "Epoch 33/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2028 - val_loss: 0.2343\n",
      "Epoch 34/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2024 - val_loss: 0.2340\n",
      "Epoch 35/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2020 - val_loss: 0.2336\n",
      "Epoch 36/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2016 - val_loss: 0.2332\n",
      "Epoch 37/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2013 - val_loss: 0.2329\n",
      "Epoch 38/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2009 - val_loss: 0.2327\n",
      "Epoch 39/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2006 - val_loss: 0.2324\n",
      "Epoch 40/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2003 - val_loss: 0.2321\n",
      "192/192 [==============================] - 2s 10ms/step\n",
      "predicted_original.shape=(6120, 67), test_y.shape=(6120, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2046.5122369835897, my average MASE = 2805.5341357338157\n",
      "Cluster 0, 2046.5122369835897\n",
      "Before prediction: train_X.shape=(10, 10, 67), train_y.shape=(10, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5756 - val_loss: 1.2952\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5738 - val_loss: 1.2950\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5721 - val_loss: 1.2949\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5703 - val_loss: 1.2948\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5686 - val_loss: 1.2947\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5670 - val_loss: 1.2946\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5653 - val_loss: 1.2945\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5637 - val_loss: 1.2944\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5621 - val_loss: 1.2943\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5605 - val_loss: 1.2941\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5589 - val_loss: 1.2940\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5573 - val_loss: 1.2939\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5558 - val_loss: 1.2937\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5542 - val_loss: 1.2936\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5527 - val_loss: 1.2934\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5512 - val_loss: 1.2932\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5497 - val_loss: 1.2930\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5482 - val_loss: 1.2929\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5468 - val_loss: 1.2927\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5454 - val_loss: 1.2925\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5440 - val_loss: 1.2924\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5426 - val_loss: 1.2922\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5413 - val_loss: 1.2921\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5400 - val_loss: 1.2920\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5386 - val_loss: 1.2919\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5374 - val_loss: 1.2918\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5361 - val_loss: 1.2918\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5349 - val_loss: 1.2917\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5337 - val_loss: 1.2916\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5325 - val_loss: 1.2915\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5314 - val_loss: 1.2914\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5302 - val_loss: 1.2912\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5290 - val_loss: 1.2911\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5279 - val_loss: 1.2909\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5268 - val_loss: 1.2907\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5256 - val_loss: 1.2905\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5246 - val_loss: 1.2903\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5235 - val_loss: 1.2902\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5224 - val_loss: 1.2901\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5213 - val_loss: 1.2900\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 950350834.5172157, my average MASE = 2745439088.6097794\n",
      "Cluster 1, 950350834.5172157\n",
      "clusters_labels.shape=(408091,)\n",
      "N_clusters=5, 5, 599, (8, 67)\n",
      "Before prediction: train_X.shape=(13, 10, 67), train_y.shape=(13, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3119 - val_loss: 0.2829\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3116 - val_loss: 0.2829\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3112 - val_loss: 0.2828\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3109 - val_loss: 0.2828\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3106 - val_loss: 0.2828\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3102 - val_loss: 0.2827\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3099 - val_loss: 0.2827\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3096 - val_loss: 0.2827\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3092 - val_loss: 0.2827\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3089 - val_loss: 0.2826\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3086 - val_loss: 0.2826\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3083 - val_loss: 0.2826\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3080 - val_loss: 0.2826\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3077 - val_loss: 0.2825\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3074 - val_loss: 0.2825\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3070 - val_loss: 0.2825\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3067 - val_loss: 0.2825\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3064 - val_loss: 0.2824\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3061 - val_loss: 0.2824\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3058 - val_loss: 0.2824\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3055 - val_loss: 0.2823\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3052 - val_loss: 0.2823\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3049 - val_loss: 0.2823\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3047 - val_loss: 0.2822\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3044 - val_loss: 0.2822\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3041 - val_loss: 0.2822\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3038 - val_loss: 0.2821\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3035 - val_loss: 0.2821\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3032 - val_loss: 0.2821\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3029 - val_loss: 0.2820\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3027 - val_loss: 0.2820\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3024 - val_loss: 0.2820\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3021 - val_loss: 0.2819\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3018 - val_loss: 0.2819\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3016 - val_loss: 0.2818\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3013 - val_loss: 0.2818\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3010 - val_loss: 0.2818\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3008 - val_loss: 0.2817\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3005 - val_loss: 0.2817\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3002 - val_loss: 0.2816\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 104.97029321074898, my average MASE = 15924142.879668588\n",
      "Cluster 0, 104.97029321074898\n",
      "Before prediction: train_X.shape=(35, 10, 67), train_y.shape=(35, 67), test_X.shape=(12, 10, 67), test_y.shape=(12, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5420 - val_loss: 0.4071\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5399 - val_loss: 0.4062\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5378 - val_loss: 0.4054\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5357 - val_loss: 0.4045\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5337 - val_loss: 0.4037\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5316 - val_loss: 0.4029\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5296 - val_loss: 0.4021\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5276 - val_loss: 0.4013\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5257 - val_loss: 0.4005\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5237 - val_loss: 0.3998\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5218 - val_loss: 0.3990\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5199 - val_loss: 0.3983\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5179 - val_loss: 0.3975\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5161 - val_loss: 0.3968\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5142 - val_loss: 0.3961\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5124 - val_loss: 0.3954\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5106 - val_loss: 0.3947\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5088 - val_loss: 0.3940\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5070 - val_loss: 0.3933\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5052 - val_loss: 0.3926\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5035 - val_loss: 0.3919\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5018 - val_loss: 0.3913\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5001 - val_loss: 0.3906\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4985 - val_loss: 0.3899\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4968 - val_loss: 0.3892\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4952 - val_loss: 0.3886\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4936 - val_loss: 0.3879\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4920 - val_loss: 0.3873\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4905 - val_loss: 0.3866\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4889 - val_loss: 0.3860\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4874 - val_loss: 0.3853\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4858 - val_loss: 0.3847\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4843 - val_loss: 0.3840\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4828 - val_loss: 0.3834\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4813 - val_loss: 0.3828\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4798 - val_loss: 0.3821\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4783 - val_loss: 0.3815\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4769 - val_loss: 0.3809\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4754 - val_loss: 0.3803\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4740 - val_loss: 0.3797\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(12, 67), test_y.shape=(12, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3145368812.0062547, my average MASE = 5974295545.526059\n",
      "Cluster 1, 3145368812.0062547\n",
      "Before prediction: train_X.shape=(2220, 10, 67), train_y.shape=(2220, 67), test_X.shape=(740, 10, 67), test_y.shape=(740, 67)\n",
      "Epoch 1/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.5043 - val_loss: 0.3721\n",
      "Epoch 2/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4967 - val_loss: 0.3684\n",
      "Epoch 3/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4907 - val_loss: 0.3653\n",
      "Epoch 4/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4855 - val_loss: 0.3625\n",
      "Epoch 5/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4810 - val_loss: 0.3601\n",
      "Epoch 6/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4769 - val_loss: 0.3579\n",
      "Epoch 7/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4731 - val_loss: 0.3558\n",
      "Epoch 8/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4695 - val_loss: 0.3540\n",
      "Epoch 9/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4661 - val_loss: 0.3522\n",
      "Epoch 10/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4629 - val_loss: 0.3506\n",
      "Epoch 11/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4598 - val_loss: 0.3490\n",
      "Epoch 12/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4568 - val_loss: 0.3476\n",
      "Epoch 13/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4539 - val_loss: 0.3462\n",
      "Epoch 14/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4510 - val_loss: 0.3448\n",
      "Epoch 15/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4483 - val_loss: 0.3435\n",
      "Epoch 16/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4455 - val_loss: 0.3422\n",
      "Epoch 17/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4429 - val_loss: 0.3410\n",
      "Epoch 18/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4402 - val_loss: 0.3398\n",
      "Epoch 19/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4376 - val_loss: 0.3386\n",
      "Epoch 20/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4351 - val_loss: 0.3375\n",
      "Epoch 21/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4326 - val_loss: 0.3363\n",
      "Epoch 22/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4302 - val_loss: 0.3352\n",
      "Epoch 23/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4278 - val_loss: 0.3342\n",
      "Epoch 24/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4255 - val_loss: 0.3331\n",
      "Epoch 25/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4232 - val_loss: 0.3321\n",
      "Epoch 26/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4211 - val_loss: 0.3311\n",
      "Epoch 27/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4189 - val_loss: 0.3302\n",
      "Epoch 28/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4169 - val_loss: 0.3292\n",
      "Epoch 29/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4149 - val_loss: 0.3284\n",
      "Epoch 30/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4129 - val_loss: 0.3276\n",
      "Epoch 31/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4111 - val_loss: 0.3268\n",
      "Epoch 32/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4093 - val_loss: 0.3260\n",
      "Epoch 33/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4076 - val_loss: 0.3253\n",
      "Epoch 34/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4060 - val_loss: 0.3246\n",
      "Epoch 35/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4044 - val_loss: 0.3240\n",
      "Epoch 36/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4029 - val_loss: 0.3234\n",
      "Epoch 37/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4014 - val_loss: 0.3228\n",
      "Epoch 38/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4000 - val_loss: 0.3222\n",
      "Epoch 39/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.3986 - val_loss: 0.3217\n",
      "Epoch 40/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.3972 - val_loss: 0.3211\n",
      "24/24 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(740, 67), test_y.shape=(740, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 487.922050733615, my average MASE = 1880.6635092100216\n",
      "Cluster 2, 487.922050733615\n",
      "Before prediction: train_X.shape=(1942, 10, 67), train_y.shape=(1942, 67), test_X.shape=(647, 10, 67), test_y.shape=(647, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.1089 - val_loss: 0.0991\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1039 - val_loss: 0.0970\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0998 - val_loss: 0.0956\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0965 - val_loss: 0.0945\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0936 - val_loss: 0.0938\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0911 - val_loss: 0.0932\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0869 - val_loss: 0.0924\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0852 - val_loss: 0.0922\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0836 - val_loss: 0.0920\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0821 - val_loss: 0.0918\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0808 - val_loss: 0.0917\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0795 - val_loss: 0.0915\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0783 - val_loss: 0.0913\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0773 - val_loss: 0.0912\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0763 - val_loss: 0.0910\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0753 - val_loss: 0.0908\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0744 - val_loss: 0.0907\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0736 - val_loss: 0.0905\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0729 - val_loss: 0.0903\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0721 - val_loss: 0.0901\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0714 - val_loss: 0.0900\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0708 - val_loss: 0.0899\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0702 - val_loss: 0.0897\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0696 - val_loss: 0.0896\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0690 - val_loss: 0.0895\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0685 - val_loss: 0.0894\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0680 - val_loss: 0.0893\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0675 - val_loss: 0.0892\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0670 - val_loss: 0.0892\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0666 - val_loss: 0.0891\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0661 - val_loss: 0.0890\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0658 - val_loss: 0.0890\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0654 - val_loss: 0.0889\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0650 - val_loss: 0.0889\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0647 - val_loss: 0.0888\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0644 - val_loss: 0.0888\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0641 - val_loss: 0.0888\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0638 - val_loss: 0.0887\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0636 - val_loss: 0.0887\n",
      "21/21 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(647, 67), test_y.shape=(647, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1152398695.5185678, my average MASE = 7118534712.468365\n",
      "Cluster 3, 1152398695.5185678\n",
      "Before prediction: train_X.shape=(156, 10, 67), train_y.shape=(156, 67), test_X.shape=(52, 10, 67), test_y.shape=(52, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5146 - val_loss: 0.4378\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5133 - val_loss: 0.4370\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5122 - val_loss: 0.4361\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5111 - val_loss: 0.4352\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5100 - val_loss: 0.4344\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5089 - val_loss: 0.4335\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5078 - val_loss: 0.4327\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5068 - val_loss: 0.4319\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5057 - val_loss: 0.4311\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5047 - val_loss: 0.4303\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5037 - val_loss: 0.4296\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5028 - val_loss: 0.4288\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5018 - val_loss: 0.4281\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5008 - val_loss: 0.4273\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4999 - val_loss: 0.4266\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4990 - val_loss: 0.4259\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4981 - val_loss: 0.4252\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4972 - val_loss: 0.4245\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4963 - val_loss: 0.4238\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4954 - val_loss: 0.4232\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4945 - val_loss: 0.4225\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4937 - val_loss: 0.4219\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4929 - val_loss: 0.4213\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4920 - val_loss: 0.4207\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4912 - val_loss: 0.4201\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4905 - val_loss: 0.4195\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4897 - val_loss: 0.4189\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4889 - val_loss: 0.4183\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4881 - val_loss: 0.4177\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4874 - val_loss: 0.4172\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4867 - val_loss: 0.4166\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4859 - val_loss: 0.4161\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4852 - val_loss: 0.4155\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4845 - val_loss: 0.4150\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4838 - val_loss: 0.4145\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4831 - val_loss: 0.4139\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4824 - val_loss: 0.4134\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4817 - val_loss: 0.4129\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4811 - val_loss: 0.4124\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4804 - val_loss: 0.4119\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(52, 67), test_y.shape=(52, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 117.29272898284657, my average MASE = 138077149.3198319\n",
      "Cluster 4, 117.29272898284657\n",
      "clusters_labels.shape=(408091,)\n",
      "N_clusters=7, 7, 49, (317, 67)\n",
      "Before prediction: train_X.shape=(184, 10, 67), train_y.shape=(184, 67), test_X.shape=(61, 10, 67), test_y.shape=(61, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.6997 - val_loss: 0.6374\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6982 - val_loss: 0.6363\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6968 - val_loss: 0.6352\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6954 - val_loss: 0.6341\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6941 - val_loss: 0.6331\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6927 - val_loss: 0.6320\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6914 - val_loss: 0.6310\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6901 - val_loss: 0.6300\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6889 - val_loss: 0.6290\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6876 - val_loss: 0.6281\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6864 - val_loss: 0.6272\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6852 - val_loss: 0.6262\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6840 - val_loss: 0.6253\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6829 - val_loss: 0.6244\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6818 - val_loss: 0.6235\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6807 - val_loss: 0.6227\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6796 - val_loss: 0.6218\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6785 - val_loss: 0.6210\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6774 - val_loss: 0.6202\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6763 - val_loss: 0.6194\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6753 - val_loss: 0.6186\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6743 - val_loss: 0.6178\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6733 - val_loss: 0.6171\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6723 - val_loss: 0.6163\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6713 - val_loss: 0.6156\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6704 - val_loss: 0.6149\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6694 - val_loss: 0.6142\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6685 - val_loss: 0.6135\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6676 - val_loss: 0.6127\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6666 - val_loss: 0.6121\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6658 - val_loss: 0.6114\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6649 - val_loss: 0.6107\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6640 - val_loss: 0.6100\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6631 - val_loss: 0.6093\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6622 - val_loss: 0.6086\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6614 - val_loss: 0.6079\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6605 - val_loss: 0.6073\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6597 - val_loss: 0.6066\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6588 - val_loss: 0.6060\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6580 - val_loss: 0.6053\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(61, 67), test_y.shape=(61, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 140.79883300988942, my average MASE = 132025753.41047312\n",
      "Cluster 0, 140.79883300988942\n",
      "Before prediction: train_X.shape=(35, 10, 67), train_y.shape=(35, 67), test_X.shape=(12, 10, 67), test_y.shape=(12, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5180 - val_loss: 0.4305\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5161 - val_loss: 0.4294\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5142 - val_loss: 0.4283\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5123 - val_loss: 0.4272\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5104 - val_loss: 0.4261\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5085 - val_loss: 0.4251\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5067 - val_loss: 0.4241\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5049 - val_loss: 0.4230\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5031 - val_loss: 0.4221\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5013 - val_loss: 0.4211\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4996 - val_loss: 0.4202\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4978 - val_loss: 0.4193\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4961 - val_loss: 0.4184\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4944 - val_loss: 0.4175\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4927 - val_loss: 0.4166\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4911 - val_loss: 0.4158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4895 - val_loss: 0.4149\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4879 - val_loss: 0.4141\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4863 - val_loss: 0.4133\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4847 - val_loss: 0.4126\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4832 - val_loss: 0.4118\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4816 - val_loss: 0.4110\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4801 - val_loss: 0.4103\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4787 - val_loss: 0.4095\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4772 - val_loss: 0.4088\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4758 - val_loss: 0.4080\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4743 - val_loss: 0.4073\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4729 - val_loss: 0.4066\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4715 - val_loss: 0.4058\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4701 - val_loss: 0.4051\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4687 - val_loss: 0.4044\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4673 - val_loss: 0.4038\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4660 - val_loss: 0.4031\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4646 - val_loss: 0.4024\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4633 - val_loss: 0.4017\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4620 - val_loss: 0.4010\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4607 - val_loss: 0.4004\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4594 - val_loss: 0.3997\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4582 - val_loss: 0.3991\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4569 - val_loss: 0.3985\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(12, 67), test_y.shape=(12, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3225148693.3924103, my average MASE = 6345532768.562708\n",
      "Cluster 1, 3225148693.3924103\n",
      "Before prediction: train_X.shape=(1941, 10, 67), train_y.shape=(1941, 67), test_X.shape=(647, 10, 67), test_y.shape=(647, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1129 - val_loss: 0.1001\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1075 - val_loss: 0.0981\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1032 - val_loss: 0.0968\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0996 - val_loss: 0.0960\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0965 - val_loss: 0.0953\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0938 - val_loss: 0.0949\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0913 - val_loss: 0.0945\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0892 - val_loss: 0.0941\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0872 - val_loss: 0.0938\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0854 - val_loss: 0.0935\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0838 - val_loss: 0.0933\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0823 - val_loss: 0.0931\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0809 - val_loss: 0.0928\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0796 - val_loss: 0.0926\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0784 - val_loss: 0.0924\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0773 - val_loss: 0.0921\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0762 - val_loss: 0.0919\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0752 - val_loss: 0.0917\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0743 - val_loss: 0.0915\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0734 - val_loss: 0.0913\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0726 - val_loss: 0.0912\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0719 - val_loss: 0.0910\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0712 - val_loss: 0.0908\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0705 - val_loss: 0.0907\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0699 - val_loss: 0.0906\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0693 - val_loss: 0.0904\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0687 - val_loss: 0.0903\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0682 - val_loss: 0.0903\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0677 - val_loss: 0.0902\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0672 - val_loss: 0.0901\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0668 - val_loss: 0.0901\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0663 - val_loss: 0.0900\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0659 - val_loss: 0.0899\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0655 - val_loss: 0.0899\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0652 - val_loss: 0.0898\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0648 - val_loss: 0.0897\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0645 - val_loss: 0.0897\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0642 - val_loss: 0.0896\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0639 - val_loss: 0.0895\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0636 - val_loss: 0.0895\n",
      "21/21 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(647, 67), test_y.shape=(647, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1209643109.1461504, my average MASE = 20216236756.2103\n",
      "Cluster 2, 1209643109.1461504\n",
      "Before prediction: train_X.shape=(47, 10, 67), train_y.shape=(47, 67), test_X.shape=(16, 10, 67), test_y.shape=(16, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.4112 - val_loss: 0.3544\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4108 - val_loss: 0.3542\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4104 - val_loss: 0.3540\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4100 - val_loss: 0.3538\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4095 - val_loss: 0.3536\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4091 - val_loss: 0.3534\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4087 - val_loss: 0.3533\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4083 - val_loss: 0.3531\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4079 - val_loss: 0.3529\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4075 - val_loss: 0.3527\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4071 - val_loss: 0.3525\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4067 - val_loss: 0.3524\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4063 - val_loss: 0.3522\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4059 - val_loss: 0.3520\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4055 - val_loss: 0.3519\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4051 - val_loss: 0.3517\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4047 - val_loss: 0.3515\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4043 - val_loss: 0.3514\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4039 - val_loss: 0.3512\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4035 - val_loss: 0.3511\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4032 - val_loss: 0.3509\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4028 - val_loss: 0.3508\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4024 - val_loss: 0.3506\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4020 - val_loss: 0.3505\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4017 - val_loss: 0.3503\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4013 - val_loss: 0.3502\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4009 - val_loss: 0.3501\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4006 - val_loss: 0.3499\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4002 - val_loss: 0.3498\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3999 - val_loss: 0.3497\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3995 - val_loss: 0.3495\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3992 - val_loss: 0.3494\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3988 - val_loss: 0.3493\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3985 - val_loss: 0.3492\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3982 - val_loss: 0.3491\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3978 - val_loss: 0.3489\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3975 - val_loss: 0.3488\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3972 - val_loss: 0.3487\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3968 - val_loss: 0.3486\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3965 - val_loss: 0.3485\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(16, 67), test_y.shape=(16, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 66.79284837728264, my average MASE = 54990938.33317367\n",
      "Cluster 3, 66.79284837728264\n",
      "Before prediction: train_X.shape=(155, 10, 67), train_y.shape=(155, 67), test_X.shape=(52, 10, 67), test_y.shape=(52, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5009 - val_loss: 0.4222\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4998 - val_loss: 0.4214\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4988 - val_loss: 0.4207\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4978 - val_loss: 0.4200\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4968 - val_loss: 0.4193\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4958 - val_loss: 0.4186\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4949 - val_loss: 0.4179\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4939 - val_loss: 0.4172\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4930 - val_loss: 0.4165\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4921 - val_loss: 0.4159\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4912 - val_loss: 0.4153\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4903 - val_loss: 0.4146\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4894 - val_loss: 0.4140\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4886 - val_loss: 0.4134\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4877 - val_loss: 0.4128\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4869 - val_loss: 0.4122\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4861 - val_loss: 0.4116\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4853 - val_loss: 0.4111\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4845 - val_loss: 0.4105\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4837 - val_loss: 0.4100\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4829 - val_loss: 0.4094\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4822 - val_loss: 0.4089\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4814 - val_loss: 0.4084\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4807 - val_loss: 0.4078\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4799 - val_loss: 0.4073\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4792 - val_loss: 0.4068\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4785 - val_loss: 0.4063\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4777 - val_loss: 0.4058\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4770 - val_loss: 0.4052\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4763 - val_loss: 0.4047\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4756 - val_loss: 0.4042\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4749 - val_loss: 0.4037\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4742 - val_loss: 0.4032\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4735 - val_loss: 0.4027\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4728 - val_loss: 0.4023\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4721 - val_loss: 0.4018\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4715 - val_loss: 0.4013\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4708 - val_loss: 0.4008\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4701 - val_loss: 0.4003\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4695 - val_loss: 0.3999\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(52, 67), test_y.shape=(52, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 127.45341483860946, my average MASE = 95916870.45268953\n",
      "Cluster 4, 127.45341483860946\n",
      "Before prediction: train_X.shape=(1565, 10, 67), train_y.shape=(1565, 67), test_X.shape=(522, 10, 67), test_y.shape=(522, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2476 - val_loss: 0.2883\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2353 - val_loss: 0.2762\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2258 - val_loss: 0.2666\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2185 - val_loss: 0.2589\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.2127 - val_loss: 0.2526\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2078 - val_loss: 0.2473\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2036 - val_loss: 0.2426\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1999 - val_loss: 0.2383\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1966 - val_loss: 0.2346\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1935 - val_loss: 0.2311\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1907 - val_loss: 0.2280\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1881 - val_loss: 0.2251\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1857 - val_loss: 0.2225\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1834 - val_loss: 0.2200\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1813 - val_loss: 0.2178\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1794 - val_loss: 0.2157\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1777 - val_loss: 0.2137\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1760 - val_loss: 0.2120\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1746 - val_loss: 0.2103\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1732 - val_loss: 0.2087\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1719 - val_loss: 0.2072\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1707 - val_loss: 0.2058\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1695 - val_loss: 0.2045\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1684 - val_loss: 0.2032\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1674 - val_loss: 0.2020\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1664 - val_loss: 0.2009\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1655 - val_loss: 0.1998\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1646 - val_loss: 0.1988\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1638 - val_loss: 0.1978\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1630 - val_loss: 0.1968\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1622 - val_loss: 0.1960\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1614 - val_loss: 0.1951\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1607 - val_loss: 0.1943\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1600 - val_loss: 0.1935\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1594 - val_loss: 0.1927\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1587 - val_loss: 0.1920\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1581 - val_loss: 0.1913\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1575 - val_loss: 0.1906\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1570 - val_loss: 0.1899\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1564 - val_loss: 0.1893\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(522, 67), test_y.shape=(522, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 224.71735313764052, my average MASE = 374017026.3746446\n",
      "Cluster 5, 224.71735313764052\n",
      "Before prediction: train_X.shape=(51, 10, 67), train_y.shape=(51, 67), test_X.shape=(17, 10, 67), test_y.shape=(17, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4065 - val_loss: 0.3790\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4060 - val_loss: 0.3788\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4055 - val_loss: 0.3787\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4049 - val_loss: 0.3786\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4044 - val_loss: 0.3784\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4038 - val_loss: 0.3783\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4033 - val_loss: 0.3782\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4028 - val_loss: 0.3780\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4023 - val_loss: 0.3779\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4018 - val_loss: 0.3778\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4013 - val_loss: 0.3776\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4008 - val_loss: 0.3775\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4003 - val_loss: 0.3774\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3998 - val_loss: 0.3773\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3993 - val_loss: 0.3771\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3988 - val_loss: 0.3770\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3984 - val_loss: 0.3769\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3979 - val_loss: 0.3768\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3975 - val_loss: 0.3767\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3970 - val_loss: 0.3765\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3966 - val_loss: 0.3764\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3962 - val_loss: 0.3763\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3958 - val_loss: 0.3762\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3953 - val_loss: 0.3761\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3949 - val_loss: 0.3760\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3945 - val_loss: 0.3759\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3941 - val_loss: 0.3757\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3937 - val_loss: 0.3756\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3933 - val_loss: 0.3755\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3929 - val_loss: 0.3754\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3925 - val_loss: 0.3753\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3921 - val_loss: 0.3752\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3917 - val_loss: 0.3751\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3913 - val_loss: 0.3750\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3909 - val_loss: 0.3749\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3905 - val_loss: 0.3748\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3902 - val_loss: 0.3747\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3898 - val_loss: 0.3745\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3894 - val_loss: 0.3744\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3891 - val_loss: 0.3743\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(17, 67), test_y.shape=(17, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 82.26261385879712, my average MASE = 20260153.97451399\n",
      "Cluster 6, 82.26261385879712\n",
      "clusters_labels.shape=(408091,)\n",
      "N_clusters=9, 9, 48, (301, 67)\n",
      "Before prediction: train_X.shape=(174, 10, 67), train_y.shape=(174, 67), test_X.shape=(58, 10, 67), test_y.shape=(58, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.7089 - val_loss: 0.5627\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7074 - val_loss: 0.5619\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7059 - val_loss: 0.5612\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7045 - val_loss: 0.5605\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7031 - val_loss: 0.5598\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7017 - val_loss: 0.5591\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7004 - val_loss: 0.5585\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6991 - val_loss: 0.5578\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6979 - val_loss: 0.5572\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6966 - val_loss: 0.5566\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6954 - val_loss: 0.5559\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6941 - val_loss: 0.5553\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6930 - val_loss: 0.5548\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6918 - val_loss: 0.5542\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6907 - val_loss: 0.5536\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6895 - val_loss: 0.5531\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6884 - val_loss: 0.5525\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6873 - val_loss: 0.5520\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6863 - val_loss: 0.5515\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6852 - val_loss: 0.5509\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6842 - val_loss: 0.5504\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6831 - val_loss: 0.5499\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6821 - val_loss: 0.5494\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6811 - val_loss: 0.5489\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6801 - val_loss: 0.5484\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6792 - val_loss: 0.5479\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6782 - val_loss: 0.5475\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6772 - val_loss: 0.5470\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6763 - val_loss: 0.5465\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6753 - val_loss: 0.5461\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6744 - val_loss: 0.5456\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6735 - val_loss: 0.5452\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6726 - val_loss: 0.5447\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6716 - val_loss: 0.5443\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6707 - val_loss: 0.5439\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6698 - val_loss: 0.5434\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6689 - val_loss: 0.5430\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6680 - val_loss: 0.5426\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6671 - val_loss: 0.5422\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6663 - val_loss: 0.5417\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(58, 67), test_y.shape=(58, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 144.45290178213432, my average MASE = 192607450.5447097\n",
      "Cluster 0, 144.45290178213432\n",
      "Before prediction: train_X.shape=(86, 10, 67), train_y.shape=(86, 67), test_X.shape=(29, 10, 67), test_y.shape=(29, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3954 - val_loss: 0.4825\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3947 - val_loss: 0.4820\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3940 - val_loss: 0.4815\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3933 - val_loss: 0.4810\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3927 - val_loss: 0.4806\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3921 - val_loss: 0.4801\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3915 - val_loss: 0.4797\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3909 - val_loss: 0.4793\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3903 - val_loss: 0.4789\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3898 - val_loss: 0.4785\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3892 - val_loss: 0.4781\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3887 - val_loss: 0.4777\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3882 - val_loss: 0.4774\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3877 - val_loss: 0.4770\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3872 - val_loss: 0.4767\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3867 - val_loss: 0.4763\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3862 - val_loss: 0.4760\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3857 - val_loss: 0.4757\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3853 - val_loss: 0.4753\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3848 - val_loss: 0.4750\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3844 - val_loss: 0.4747\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3840 - val_loss: 0.4744\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3835 - val_loss: 0.4741\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3831 - val_loss: 0.4738\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3827 - val_loss: 0.4735\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3822 - val_loss: 0.4732\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3818 - val_loss: 0.4729\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3814 - val_loss: 0.4726\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3810 - val_loss: 0.4723\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3806 - val_loss: 0.4720\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3802 - val_loss: 0.4717\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3798 - val_loss: 0.4714\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3794 - val_loss: 0.4711\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3790 - val_loss: 0.4708\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3786 - val_loss: 0.4705\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3783 - val_loss: 0.4703\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3779 - val_loss: 0.4700\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3775 - val_loss: 0.4697\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3771 - val_loss: 0.4695\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3768 - val_loss: 0.4692\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(29, 67), test_y.shape=(29, 67)\n",
      "average MASE = 261.83661307224526, my average MASE = 165152999.01271424\n",
      "Cluster 1, 261.83661307224526\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3860 - val_loss: 0.4764\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3839 - val_loss: 0.4752\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3818 - val_loss: 0.4741\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3798 - val_loss: 0.4730\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3778 - val_loss: 0.4719\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3758 - val_loss: 0.4709\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3738 - val_loss: 0.4699\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3718 - val_loss: 0.4689\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3699 - val_loss: 0.4680\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3680 - val_loss: 0.4670\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3661 - val_loss: 0.4660\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3643 - val_loss: 0.4650\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3624 - val_loss: 0.4640\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3606 - val_loss: 0.4630\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3588 - val_loss: 0.4620\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3570 - val_loss: 0.4610\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3553 - val_loss: 0.4599\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3535 - val_loss: 0.4588\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3518 - val_loss: 0.4578\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3501 - val_loss: 0.4567\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3484 - val_loss: 0.4557\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3468 - val_loss: 0.4547\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3452 - val_loss: 0.4537\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3437 - val_loss: 0.4528\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3422 - val_loss: 0.4521\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3407 - val_loss: 0.4513\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3393 - val_loss: 0.4506\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3378 - val_loss: 0.4498\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3363 - val_loss: 0.4493\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3349 - val_loss: 0.4487\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3336 - val_loss: 0.4481\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3323 - val_loss: 0.4475\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3311 - val_loss: 0.4470\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3299 - val_loss: 0.4464\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3287 - val_loss: 0.4459\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3276 - val_loss: 0.4453\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3264 - val_loss: 0.4447\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3253 - val_loss: 0.4441\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3241 - val_loss: 0.4436\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3231 - val_loss: 0.4430\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.29600762869721386, my average MASE = 0.4272698543687938\n",
      "Cluster 2, 0.29600762869721386\n",
      "Before prediction: train_X.shape=(35, 10, 67), train_y.shape=(35, 67), test_X.shape=(12, 10, 67), test_y.shape=(12, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4866 - val_loss: 0.4310\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4848 - val_loss: 0.4300\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4830 - val_loss: 0.4290\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4812 - val_loss: 0.4280\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4794 - val_loss: 0.4271\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4777 - val_loss: 0.4261\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4760 - val_loss: 0.4252\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4744 - val_loss: 0.4243\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4727 - val_loss: 0.4235\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4711 - val_loss: 0.4226\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4695 - val_loss: 0.4217\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4679 - val_loss: 0.4209\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4663 - val_loss: 0.4200\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4648 - val_loss: 0.4192\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4633 - val_loss: 0.4184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4618 - val_loss: 0.4175\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4603 - val_loss: 0.4167\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4589 - val_loss: 0.4159\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4574 - val_loss: 0.4150\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4560 - val_loss: 0.4142\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4546 - val_loss: 0.4134\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4532 - val_loss: 0.4126\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4518 - val_loss: 0.4118\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4504 - val_loss: 0.4109\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4491 - val_loss: 0.4101\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4477 - val_loss: 0.4093\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4464 - val_loss: 0.4085\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4451 - val_loss: 0.4077\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4438 - val_loss: 0.4069\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4425 - val_loss: 0.4061\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4412 - val_loss: 0.4053\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4399 - val_loss: 0.4045\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4387 - val_loss: 0.4037\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4374 - val_loss: 0.4029\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4362 - val_loss: 0.4021\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4350 - val_loss: 0.4014\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4337 - val_loss: 0.4006\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4325 - val_loss: 0.3998\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4313 - val_loss: 0.3991\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4301 - val_loss: 0.3983\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(12, 67), test_y.shape=(12, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3346396028.9972167, my average MASE = 7814754000.575592\n",
      "Cluster 3, 3346396028.9972167\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3142 - val_loss: 0.7771\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3138 - val_loss: 0.7770\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3135 - val_loss: 0.7769\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3131 - val_loss: 0.7768\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3127 - val_loss: 0.7767\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3123 - val_loss: 0.7765\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3119 - val_loss: 0.7764\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3116 - val_loss: 0.7763\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3112 - val_loss: 0.7762\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3108 - val_loss: 0.7761\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3105 - val_loss: 0.7759\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3101 - val_loss: 0.7758\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3097 - val_loss: 0.7757\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3094 - val_loss: 0.7756\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3090 - val_loss: 0.7755\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3087 - val_loss: 0.7754\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3083 - val_loss: 0.7753\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3080 - val_loss: 0.7752\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3076 - val_loss: 0.7751\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3073 - val_loss: 0.7749\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3069 - val_loss: 0.7748\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3066 - val_loss: 0.7747\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3063 - val_loss: 0.7746\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3059 - val_loss: 0.7746\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3056 - val_loss: 0.7745\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3053 - val_loss: 0.7744\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3049 - val_loss: 0.7743\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3046 - val_loss: 0.7742\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3043 - val_loss: 0.7741\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3040 - val_loss: 0.7740\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3037 - val_loss: 0.7739\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3033 - val_loss: 0.7739\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3030 - val_loss: 0.7738\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3027 - val_loss: 0.7737\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3024 - val_loss: 0.7736\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3021 - val_loss: 0.7735\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3018 - val_loss: 0.7734\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3015 - val_loss: 0.7733\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3012 - val_loss: 0.7732\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3009 - val_loss: 0.7731\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 93.69104182165661, my average MASE = 20945629.206220664\n",
      "Cluster 4, 93.69104182165661\n",
      "Before prediction: train_X.shape=(89, 10, 67), train_y.shape=(89, 67), test_X.shape=(30, 10, 67), test_y.shape=(30, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3424 - val_loss: 0.3360\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3416 - val_loss: 0.3355\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3408 - val_loss: 0.3350\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3400 - val_loss: 0.3346\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3393 - val_loss: 0.3341\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3386 - val_loss: 0.3337\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3379 - val_loss: 0.3333\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3372 - val_loss: 0.3329\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3365 - val_loss: 0.3324\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3358 - val_loss: 0.3320\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3351 - val_loss: 0.3316\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3344 - val_loss: 0.3312\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3338 - val_loss: 0.3309\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3331 - val_loss: 0.3305\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3325 - val_loss: 0.3301\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3319 - val_loss: 0.3297\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3312 - val_loss: 0.3293\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3306 - val_loss: 0.3290\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3300 - val_loss: 0.3286\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3294 - val_loss: 0.3282\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3288 - val_loss: 0.3279\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3282 - val_loss: 0.3275\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3276 - val_loss: 0.3272\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3271 - val_loss: 0.3268\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3265 - val_loss: 0.3265\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3260 - val_loss: 0.3262\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3254 - val_loss: 0.3258\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3249 - val_loss: 0.3255\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3244 - val_loss: 0.3252\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3239 - val_loss: 0.3249\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3234 - val_loss: 0.3246\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3228 - val_loss: 0.3243\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3223 - val_loss: 0.3240\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3219 - val_loss: 0.3237\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3214 - val_loss: 0.3234\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3209 - val_loss: 0.3231\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3204 - val_loss: 0.3228\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3199 - val_loss: 0.3225\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3195 - val_loss: 0.3222\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3190 - val_loss: 0.3219\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(30, 67), test_y.shape=(30, 67)\n",
      "average MASE = 188.45046029060867, my average MASE = 121680879.21353056\n",
      "Cluster 5, 188.45046029060867\n",
      "Before prediction: train_X.shape=(1942, 10, 67), train_y.shape=(1942, 67), test_X.shape=(647, 10, 67), test_y.shape=(647, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1120 - val_loss: 0.1007\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1067 - val_loss: 0.0987\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1025 - val_loss: 0.0974\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0990 - val_loss: 0.0963\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0960 - val_loss: 0.0954\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0933 - val_loss: 0.0947\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0909 - val_loss: 0.0942\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0888 - val_loss: 0.0938\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0868 - val_loss: 0.0934\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0851 - val_loss: 0.0931\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0834 - val_loss: 0.0928\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0819 - val_loss: 0.0926\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0806 - val_loss: 0.0923\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0793 - val_loss: 0.0921\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0781 - val_loss: 0.0919\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0770 - val_loss: 0.0917\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0759 - val_loss: 0.0916\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0749 - val_loss: 0.0914\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0740 - val_loss: 0.0912\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0732 - val_loss: 0.0911\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0724 - val_loss: 0.0910\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0716 - val_loss: 0.0909\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0709 - val_loss: 0.0908\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0702 - val_loss: 0.0908\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0695 - val_loss: 0.0907\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0689 - val_loss: 0.0906\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0683 - val_loss: 0.0906\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0678 - val_loss: 0.0905\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0673 - val_loss: 0.0904\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0668 - val_loss: 0.0903\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0664 - val_loss: 0.0902\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0660 - val_loss: 0.0902\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0656 - val_loss: 0.0901\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0652 - val_loss: 0.0900\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0649 - val_loss: 0.0899\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0645 - val_loss: 0.0898\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0642 - val_loss: 0.0897\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0640 - val_loss: 0.0896\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0637 - val_loss: 0.0895\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0634 - val_loss: 0.0895\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(647, 67), test_y.shape=(647, 67)\n",
      "average MASE = 1073797047.1669116, my average MASE = 13075073766.705614\n",
      "Cluster 6, 1073797047.1669116\n",
      "Before prediction: train_X.shape=(1565, 10, 67), train_y.shape=(1565, 67), test_X.shape=(522, 10, 67), test_y.shape=(522, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2348 - val_loss: 0.2753\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2252 - val_loss: 0.2653\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2178 - val_loss: 0.2575\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2120 - val_loss: 0.2511\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2071 - val_loss: 0.2457\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2029 - val_loss: 0.2409\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1991 - val_loss: 0.2366\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1956 - val_loss: 0.2328\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1924 - val_loss: 0.2293\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1895 - val_loss: 0.2261\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1868 - val_loss: 0.2231\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1843 - val_loss: 0.2203\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1820 - val_loss: 0.2178\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1799 - val_loss: 0.2155\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1780 - val_loss: 0.2134\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1762 - val_loss: 0.2114\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1745 - val_loss: 0.2096\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1730 - val_loss: 0.2079\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1715 - val_loss: 0.2063\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1702 - val_loss: 0.2048\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1689 - val_loss: 0.2033\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1678 - val_loss: 0.2020\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1666 - val_loss: 0.2007\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1656 - val_loss: 0.1995\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1646 - val_loss: 0.1984\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1636 - val_loss: 0.1973\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1627 - val_loss: 0.1962\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1619 - val_loss: 0.1952\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1610 - val_loss: 0.1943\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1602 - val_loss: 0.1934\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1595 - val_loss: 0.1925\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1588 - val_loss: 0.1917\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1581 - val_loss: 0.1909\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1574 - val_loss: 0.1901\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1568 - val_loss: 0.1894\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1561 - val_loss: 0.1886\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1556 - val_loss: 0.1880\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1550 - val_loss: 0.1873\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1544 - val_loss: 0.1867\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1539 - val_loss: 0.1860\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(522, 67), test_y.shape=(522, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 180.26488918007823, my average MASE = 179278352.44392422\n",
      "Cluster 7, 180.26488918007823\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "clusters_labels.shape=(408091,)\n",
      "N_clusters=11, 11, 904, (6, 67)\n",
      "Before prediction: train_X.shape=(88, 10, 67), train_y.shape=(88, 67), test_X.shape=(29, 10, 67), test_y.shape=(29, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3401 - val_loss: 0.3875\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3395 - val_loss: 0.3872\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3390 - val_loss: 0.3870\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3385 - val_loss: 0.3868\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3380 - val_loss: 0.3866\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3375 - val_loss: 0.3863\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3370 - val_loss: 0.3861\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3366 - val_loss: 0.3859\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3361 - val_loss: 0.3856\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3357 - val_loss: 0.3854\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3352 - val_loss: 0.3852\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3348 - val_loss: 0.3850\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3343 - val_loss: 0.3848\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3339 - val_loss: 0.3846\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3335 - val_loss: 0.3844\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3331 - val_loss: 0.3842\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3327 - val_loss: 0.3840\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3323 - val_loss: 0.3837\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3318 - val_loss: 0.3835\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3314 - val_loss: 0.3833\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3311 - val_loss: 0.3831\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3307 - val_loss: 0.3829\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3303 - val_loss: 0.3827\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3299 - val_loss: 0.3825\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3295 - val_loss: 0.3823\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3292 - val_loss: 0.3821\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3288 - val_loss: 0.3820\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3284 - val_loss: 0.3818\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3281 - val_loss: 0.3816\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3277 - val_loss: 0.3814\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3274 - val_loss: 0.3812\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3270 - val_loss: 0.3810\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3267 - val_loss: 0.3808\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3263 - val_loss: 0.3806\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3260 - val_loss: 0.3805\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3257 - val_loss: 0.3803\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3254 - val_loss: 0.3801\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3250 - val_loss: 0.3799\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3247 - val_loss: 0.3798\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3244 - val_loss: 0.3796\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(29, 67), test_y.shape=(29, 67)\n",
      "average MASE = 122.2598699731945, my average MASE = 164200403.28208458\n",
      "Cluster 0, 122.2598699731945\n",
      "Before prediction: train_X.shape=(15, 10, 67), train_y.shape=(15, 67), test_X.shape=(5, 10, 67), test_y.shape=(5, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5011 - val_loss: 0.7038\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5005 - val_loss: 0.7036\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4998 - val_loss: 0.7034\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4991 - val_loss: 0.7032\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4984 - val_loss: 0.7030\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4978 - val_loss: 0.7028\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4971 - val_loss: 0.7026\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4964 - val_loss: 0.7024\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4958 - val_loss: 0.7022\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4951 - val_loss: 0.7020\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4945 - val_loss: 0.7017\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4938 - val_loss: 0.7015\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4932 - val_loss: 0.7014\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4926 - val_loss: 0.7012\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4919 - val_loss: 0.7010\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4913 - val_loss: 0.7008\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4907 - val_loss: 0.7006\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4901 - val_loss: 0.7004\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4894 - val_loss: 0.7002\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4888 - val_loss: 0.7000\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4882 - val_loss: 0.6998\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4877 - val_loss: 0.6997\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4871 - val_loss: 0.6995\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4865 - val_loss: 0.6993\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4859 - val_loss: 0.6991\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4853 - val_loss: 0.6989\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4847 - val_loss: 0.6988\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4842 - val_loss: 0.6986\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4836 - val_loss: 0.6984\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4831 - val_loss: 0.6983\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4825 - val_loss: 0.6981\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4820 - val_loss: 0.6979\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4815 - val_loss: 0.6978\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4809 - val_loss: 0.6976\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4804 - val_loss: 0.6974\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4799 - val_loss: 0.6973\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4794 - val_loss: 0.6971\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4789 - val_loss: 0.6970\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4783 - val_loss: 0.6968\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4778 - val_loss: 0.6966\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(5, 67), test_y.shape=(5, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2966824.6919978233, my average MASE = 79205816.45072892\n",
      "Cluster 1, 2966824.6919978233\n",
      "Before prediction: train_X.shape=(11, 10, 67), train_y.shape=(11, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1998 - val_loss: 0.3035\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1993 - val_loss: 0.3034\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1988 - val_loss: 0.3034\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1982 - val_loss: 0.3033\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1977 - val_loss: 0.3033\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1972 - val_loss: 0.3032\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1967 - val_loss: 0.3032\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1962 - val_loss: 0.3032\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1957 - val_loss: 0.3031\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1952 - val_loss: 0.3031\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1947 - val_loss: 0.3031\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1942 - val_loss: 0.3031\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1938 - val_loss: 0.3030\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1933 - val_loss: 0.3030\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1928 - val_loss: 0.3030\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1924 - val_loss: 0.3030\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1919 - val_loss: 0.3030\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1915 - val_loss: 0.3030\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1911 - val_loss: 0.3030\n",
      "Epoch 19: early stopping\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 130.97936905502368, my average MASE = 132554988.34305501\n",
      "Cluster 2, 130.97936905502368\n",
      "Before prediction: train_X.shape=(23, 10, 67), train_y.shape=(23, 67), test_X.shape=(8, 10, 67), test_y.shape=(8, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4496 - val_loss: 0.4061\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4490 - val_loss: 0.4060\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4484 - val_loss: 0.4058\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4478 - val_loss: 0.4057\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4472 - val_loss: 0.4056\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4466 - val_loss: 0.4055\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4461 - val_loss: 0.4054\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4455 - val_loss: 0.4053\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4450 - val_loss: 0.4052\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4444 - val_loss: 0.4051\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4439 - val_loss: 0.4050\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4433 - val_loss: 0.4049\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4428 - val_loss: 0.4048\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4423 - val_loss: 0.4047\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4417 - val_loss: 0.4046\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4412 - val_loss: 0.4045\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4407 - val_loss: 0.4044\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4402 - val_loss: 0.4043\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4397 - val_loss: 0.4042\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4392 - val_loss: 0.4041\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4387 - val_loss: 0.4040\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4382 - val_loss: 0.4039\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4377 - val_loss: 0.4038\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4372 - val_loss: 0.4037\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4367 - val_loss: 0.4036\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4362 - val_loss: 0.4035\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4357 - val_loss: 0.4034\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4352 - val_loss: 0.4033\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4348 - val_loss: 0.4032\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4343 - val_loss: 0.4031\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4338 - val_loss: 0.4030\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4333 - val_loss: 0.4029\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4329 - val_loss: 0.4028\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4324 - val_loss: 0.4028\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4320 - val_loss: 0.4027\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4315 - val_loss: 0.4026\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4311 - val_loss: 0.4025\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4306 - val_loss: 0.4024\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4302 - val_loss: 0.4023\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4297 - val_loss: 0.4022\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(8, 67), test_y.shape=(8, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1437.7621485075726, my average MASE = 33408169.939470924\n",
      "Cluster 3, 1437.7621485075726\n",
      "Before prediction: train_X.shape=(10, 10, 67), train_y.shape=(10, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3791 - val_loss: 0.4796\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3768 - val_loss: 0.4780\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3745 - val_loss: 0.4765\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3722 - val_loss: 0.4750\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3700 - val_loss: 0.4735\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3678 - val_loss: 0.4721\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3657 - val_loss: 0.4707\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3636 - val_loss: 0.4693\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3616 - val_loss: 0.4679\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3595 - val_loss: 0.4665\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3576 - val_loss: 0.4652\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3556 - val_loss: 0.4639\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3537 - val_loss: 0.4626\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3519 - val_loss: 0.4614\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3500 - val_loss: 0.4602\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3482 - val_loss: 0.4590\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3464 - val_loss: 0.4578\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3447 - val_loss: 0.4567\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3430 - val_loss: 0.4556\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3414 - val_loss: 0.4545\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3398 - val_loss: 0.4534\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3382 - val_loss: 0.4524\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3366 - val_loss: 0.4513\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3351 - val_loss: 0.4504\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3335 - val_loss: 0.4494\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3320 - val_loss: 0.4484\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3305 - val_loss: 0.4475\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3291 - val_loss: 0.4466\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3277 - val_loss: 0.4457\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3263 - val_loss: 0.4448\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3249 - val_loss: 0.4439\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3236 - val_loss: 0.4431\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3222 - val_loss: 0.4422\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3209 - val_loss: 0.4413\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3197 - val_loss: 0.4404\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3184 - val_loss: 0.4395\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3171 - val_loss: 0.4386\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3159 - val_loss: 0.4378\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3147 - val_loss: 0.4369\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3135 - val_loss: 0.4360\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1399316882.5162897, my average MASE = 3857319737.5045514\n",
      "Cluster 4, 1399316882.5162897\n",
      "Before prediction: train_X.shape=(1565, 10, 67), train_y.shape=(1565, 67), test_X.shape=(522, 10, 67), test_y.shape=(522, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2312 - val_loss: 0.2708\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2231 - val_loss: 0.2627\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2165 - val_loss: 0.2559\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2110 - val_loss: 0.2500\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2061 - val_loss: 0.2449\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.2018 - val_loss: 0.2403\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1979 - val_loss: 0.2361\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1944 - val_loss: 0.2322\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1912 - val_loss: 0.2287\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1882 - val_loss: 0.2255\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1854 - val_loss: 0.2224\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1829 - val_loss: 0.2197\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1806 - val_loss: 0.2172\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1785 - val_loss: 0.2150\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1767 - val_loss: 0.2129\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1750 - val_loss: 0.2110\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1734 - val_loss: 0.2093\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1720 - val_loss: 0.2077\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1707 - val_loss: 0.2061\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1695 - val_loss: 0.2047\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1683 - val_loss: 0.2033\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1672 - val_loss: 0.2021\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.1661 - val_loss: 0.2009\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1651 - val_loss: 0.1997\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1641 - val_loss: 0.1986\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1632 - val_loss: 0.1975\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1623 - val_loss: 0.1965\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1615 - val_loss: 0.1955\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1606 - val_loss: 0.1945\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1599 - val_loss: 0.1937\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1591 - val_loss: 0.1928\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1584 - val_loss: 0.1920\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1577 - val_loss: 0.1912\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1570 - val_loss: 0.1904\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1564 - val_loss: 0.1897\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1558 - val_loss: 0.1890\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1552 - val_loss: 0.1883\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1547 - val_loss: 0.1877\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1542 - val_loss: 0.1871\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1537 - val_loss: 0.1864\n",
      "17/17 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(522, 67), test_y.shape=(522, 67)\n",
      "average MASE = 146.93947390831542, my average MASE = 534465305.08164215\n",
      "Cluster 5, 146.93947390831542\n",
      "Before prediction: train_X.shape=(7, 10, 67), train_y.shape=(7, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3864 - val_loss: 0.3280\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3857 - val_loss: 0.3279\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3849 - val_loss: 0.3278\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3842 - val_loss: 0.3277\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3834 - val_loss: 0.3276\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3827 - val_loss: 0.3275\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3820 - val_loss: 0.3274\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3813 - val_loss: 0.3274\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3806 - val_loss: 0.3273\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3799 - val_loss: 0.3272\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3792 - val_loss: 0.3272\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3785 - val_loss: 0.3271\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3778 - val_loss: 0.3270\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3772 - val_loss: 0.3270\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3765 - val_loss: 0.3269\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3759 - val_loss: 0.3269\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3753 - val_loss: 0.3268\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3746 - val_loss: 0.3268\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3740 - val_loss: 0.3267\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3734 - val_loss: 0.3266\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3728 - val_loss: 0.3266\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3722 - val_loss: 0.3265\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3717 - val_loss: 0.3265\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3711 - val_loss: 0.3264\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3705 - val_loss: 0.3264\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3699 - val_loss: 0.3263\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3694 - val_loss: 0.3263\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3688 - val_loss: 0.3262\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3682 - val_loss: 0.3262\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3677 - val_loss: 0.3262\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3671 - val_loss: 0.3261\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3666 - val_loss: 0.3261\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3660 - val_loss: 0.3260\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3655 - val_loss: 0.3260\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3649 - val_loss: 0.3260\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3643 - val_loss: 0.3259\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3638 - val_loss: 0.3259\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3632 - val_loss: 0.3259\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3627 - val_loss: 0.3259\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3621 - val_loss: 0.3259\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 304.5484747914777, my average MASE = 15252291.46578898\n",
      "Cluster 6, 304.5484747914777\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.8703 - val_loss: 8.9197\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8672 - val_loss: 8.9197\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8642 - val_loss: 8.9196\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8613 - val_loss: 8.9196\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8586 - val_loss: 8.9196\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8558 - val_loss: 8.9195\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8532 - val_loss: 8.9194\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8506 - val_loss: 8.9194\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8480 - val_loss: 8.9193\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8455 - val_loss: 8.9192\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8431 - val_loss: 8.9192\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8407 - val_loss: 8.9191\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8383 - val_loss: 8.9190\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8360 - val_loss: 8.9189\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8337 - val_loss: 8.9188\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8315 - val_loss: 8.9187\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8293 - val_loss: 8.9187\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8271 - val_loss: 8.9186\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8250 - val_loss: 8.9185\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8229 - val_loss: 8.9184\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8209 - val_loss: 8.9184\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8189 - val_loss: 8.9183\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8170 - val_loss: 8.9182\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8150 - val_loss: 8.9181\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8131 - val_loss: 8.9180\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8113 - val_loss: 8.9179\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8095 - val_loss: 8.9179\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8079 - val_loss: 8.9178\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8062 - val_loss: 8.9177\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8046 - val_loss: 8.9176\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8031 - val_loss: 8.9175\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8016 - val_loss: 8.9175\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8000 - val_loss: 8.9174\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7985 - val_loss: 8.9173\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7970 - val_loss: 8.9172\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7955 - val_loss: 8.9171\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7941 - val_loss: 8.9171\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7927 - val_loss: 8.9170\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7913 - val_loss: 8.9170\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7899 - val_loss: 8.9170\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 192382848.4747458, my average MASE = 882590618.8453726\n",
      "Cluster 7, 192382848.4747458\n",
      "Before prediction: train_X.shape=(1941, 10, 67), train_y.shape=(1941, 67), test_X.shape=(647, 10, 67), test_y.shape=(647, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1119 - val_loss: 0.1001\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1067 - val_loss: 0.0986\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1024 - val_loss: 0.0973\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0988 - val_loss: 0.0963\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0957 - val_loss: 0.0956\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0930 - val_loss: 0.0950\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0905 - val_loss: 0.0945\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0883 - val_loss: 0.0941\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0863 - val_loss: 0.0937\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0845 - val_loss: 0.0933\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0828 - val_loss: 0.0930\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0813 - val_loss: 0.0928\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0799 - val_loss: 0.0925\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0787 - val_loss: 0.0924\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0775 - val_loss: 0.0922\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0764 - val_loss: 0.0920\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0754 - val_loss: 0.0918\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0744 - val_loss: 0.0917\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0735 - val_loss: 0.0915\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0727 - val_loss: 0.0914\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0719 - val_loss: 0.0913\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0711 - val_loss: 0.0912\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0705 - val_loss: 0.0910\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0698 - val_loss: 0.0909\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0692 - val_loss: 0.0908\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0687 - val_loss: 0.0907\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0682 - val_loss: 0.0906\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0677 - val_loss: 0.0905\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0672 - val_loss: 0.0904\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0668 - val_loss: 0.0903\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0664 - val_loss: 0.0902\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0660 - val_loss: 0.0901\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0656 - val_loss: 0.0900\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0653 - val_loss: 0.0899\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0649 - val_loss: 0.0899\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0646 - val_loss: 0.0898\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0643 - val_loss: 0.0898\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0640 - val_loss: 0.0897\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0638 - val_loss: 0.0896\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0635 - val_loss: 0.0896\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(647, 67), test_y.shape=(647, 67)\n",
      "average MASE = 1222984943.0994627, my average MASE = 34180380931.05725\n",
      "Cluster 8, 1222984943.0994627\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(7, 10, 67), train_y.shape=(7, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3097 - val_loss: 0.3956\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3089 - val_loss: 0.3954\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3081 - val_loss: 0.3952\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3074 - val_loss: 0.3950\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3066 - val_loss: 0.3948\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3059 - val_loss: 0.3945\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3051 - val_loss: 0.3943\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3044 - val_loss: 0.3941\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3037 - val_loss: 0.3939\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3030 - val_loss: 0.3937\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3023 - val_loss: 0.3935\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3017 - val_loss: 0.3934\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3010 - val_loss: 0.3933\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3003 - val_loss: 0.3931\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2997 - val_loss: 0.3930\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2990 - val_loss: 0.3929\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2984 - val_loss: 0.3927\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2977 - val_loss: 0.3926\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2971 - val_loss: 0.3925\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2965 - val_loss: 0.3924\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2959 - val_loss: 0.3923\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2953 - val_loss: 0.3922\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2947 - val_loss: 0.3921\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2941 - val_loss: 0.3920\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2935 - val_loss: 0.3919\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2930 - val_loss: 0.3918\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2924 - val_loss: 0.3917\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2918 - val_loss: 0.3917\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2913 - val_loss: 0.3916\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2907 - val_loss: 0.3915\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2902 - val_loss: 0.3914\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2896 - val_loss: 0.3914\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2891 - val_loss: 0.3913\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2885 - val_loss: 0.3912\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2880 - val_loss: 0.3911\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2875 - val_loss: 0.3910\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2870 - val_loss: 0.3909\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2865 - val_loss: 0.3908\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2859 - val_loss: 0.3907\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2854 - val_loss: 0.3907\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 561.7864488366092, my average MASE = 24722978.57949824\n",
      "Cluster 10, 561.7864488366092\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=2, 2, 13, (10045, 67)\n",
      "Before prediction: train_X.shape=(6020, 10, 67), train_y.shape=(6020, 67), test_X.shape=(2007, 10, 67), test_y.shape=(2007, 67)\n",
      "Epoch 1/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0647 - val_loss: 0.0484\n",
      "Epoch 2/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0596 - val_loss: 0.0453\n",
      "Epoch 3/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0565 - val_loss: 0.0429\n",
      "Epoch 4/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0542 - val_loss: 0.0410\n",
      "Epoch 5/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0523 - val_loss: 0.0394\n",
      "Epoch 6/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0508 - val_loss: 0.0381\n",
      "Epoch 7/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0495 - val_loss: 0.0370\n",
      "Epoch 8/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0484 - val_loss: 0.0360\n",
      "Epoch 9/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0475 - val_loss: 0.0351\n",
      "Epoch 10/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0467 - val_loss: 0.0344\n",
      "Epoch 11/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0461 - val_loss: 0.0338\n",
      "Epoch 12/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0454 - val_loss: 0.0333\n",
      "Epoch 13/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0449 - val_loss: 0.0328\n",
      "Epoch 14/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0444 - val_loss: 0.0323\n",
      "Epoch 15/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0440 - val_loss: 0.0320\n",
      "Epoch 16/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0435 - val_loss: 0.0316\n",
      "Epoch 17/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0431 - val_loss: 0.0313\n",
      "Epoch 18/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0428 - val_loss: 0.0311\n",
      "Epoch 19/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0424 - val_loss: 0.0308\n",
      "Epoch 20/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0421 - val_loss: 0.0306\n",
      "Epoch 21/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0418 - val_loss: 0.0304\n",
      "Epoch 22/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0415 - val_loss: 0.0303\n",
      "Epoch 23/40\n",
      "95/95 [==============================] - 3s 32ms/step - loss: 0.0412 - val_loss: 0.0301\n",
      "Epoch 24/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0410 - val_loss: 0.0300\n",
      "Epoch 25/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0408 - val_loss: 0.0298\n",
      "Epoch 26/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0406 - val_loss: 0.0297\n",
      "Epoch 27/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0404 - val_loss: 0.0296\n",
      "Epoch 28/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0402 - val_loss: 0.0294\n",
      "Epoch 29/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0400 - val_loss: 0.0293\n",
      "Epoch 30/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0399 - val_loss: 0.0292\n",
      "Epoch 31/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0397 - val_loss: 0.0291\n",
      "Epoch 32/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0396 - val_loss: 0.0291\n",
      "Epoch 33/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0395 - val_loss: 0.0290\n",
      "Epoch 34/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0394 - val_loss: 0.0289\n",
      "Epoch 35/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0393 - val_loss: 0.0289\n",
      "Epoch 36/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0391 - val_loss: 0.0288\n",
      "Epoch 37/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0391 - val_loss: 0.0287\n",
      "Epoch 38/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0390 - val_loss: 0.0287\n",
      "Epoch 39/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0389 - val_loss: 0.0286\n",
      "Epoch 40/40\n",
      "95/95 [==============================] - 3s 29ms/step - loss: 0.0388 - val_loss: 0.0286\n",
      "63/63 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(2007, 67), test_y.shape=(2007, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 124611449.52884135, my average MASE = 52097635280.213745\n",
      "Cluster 0, 124611449.52884135\n",
      "Before prediction: train_X.shape=(18364, 10, 67), train_y.shape=(18364, 67), test_X.shape=(6121, 10, 67), test_y.shape=(6121, 67)\n",
      "Epoch 1/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.3019 - val_loss: 0.3171\n",
      "Epoch 2/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2812 - val_loss: 0.3011\n",
      "Epoch 3/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2652 - val_loss: 0.2888\n",
      "Epoch 4/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2545 - val_loss: 0.2808\n",
      "Epoch 5/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2479 - val_loss: 0.2750\n",
      "Epoch 6/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2428 - val_loss: 0.2703\n",
      "Epoch 7/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2385 - val_loss: 0.2663\n",
      "Epoch 8/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2348 - val_loss: 0.2629\n",
      "Epoch 9/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2315 - val_loss: 0.2599\n",
      "Epoch 10/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2283 - val_loss: 0.2570\n",
      "Epoch 11/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2254 - val_loss: 0.2544\n",
      "Epoch 12/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2226 - val_loss: 0.2520\n",
      "Epoch 13/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2201 - val_loss: 0.2499\n",
      "Epoch 14/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2179 - val_loss: 0.2480\n",
      "Epoch 15/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2161 - val_loss: 0.2464\n",
      "Epoch 16/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2145 - val_loss: 0.2449\n",
      "Epoch 17/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2131 - val_loss: 0.2436\n",
      "Epoch 18/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2118 - val_loss: 0.2425\n",
      "Epoch 19/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2107 - val_loss: 0.2414\n",
      "Epoch 20/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2096 - val_loss: 0.2403\n",
      "Epoch 21/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2087 - val_loss: 0.2395\n",
      "Epoch 22/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2078 - val_loss: 0.2387\n",
      "Epoch 23/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2070 - val_loss: 0.2379\n",
      "Epoch 24/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2063 - val_loss: 0.2371\n",
      "Epoch 25/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2056 - val_loss: 0.2364\n",
      "Epoch 26/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2049 - val_loss: 0.2358\n",
      "Epoch 27/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2043 - val_loss: 0.2352\n",
      "Epoch 28/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2038 - val_loss: 0.2346\n",
      "Epoch 29/40\n",
      "287/287 [==============================] - 8s 28ms/step - loss: 0.2032 - val_loss: 0.2342\n",
      "Epoch 30/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2027 - val_loss: 0.2337\n",
      "Epoch 31/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2023 - val_loss: 0.2332\n",
      "Epoch 32/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2018 - val_loss: 0.2329\n",
      "Epoch 33/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2014 - val_loss: 0.2325\n",
      "Epoch 34/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2010 - val_loss: 0.2321\n",
      "Epoch 35/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2006 - val_loss: 0.2318\n",
      "Epoch 36/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2003 - val_loss: 0.2314\n",
      "Epoch 37/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.1999 - val_loss: 0.2311\n",
      "Epoch 38/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.1996 - val_loss: 0.2308\n",
      "Epoch 39/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.1993 - val_loss: 0.2305\n",
      "Epoch 40/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.1990 - val_loss: 0.2302\n",
      "192/192 [==============================] - 2s 11ms/step\n",
      "predicted_original.shape=(6121, 67), test_y.shape=(6121, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1515.0877642124253, my average MASE = 2752.3099444505265\n",
      "Cluster 1, 1515.0877642124253\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=5, 5, 12, (3253, 67)\n",
      "Before prediction: train_X.shape=(1945, 10, 67), train_y.shape=(1945, 67), test_X.shape=(648, 10, 67), test_y.shape=(648, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1101 - val_loss: 0.0983\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1050 - val_loss: 0.0966\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1009 - val_loss: 0.0954\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0975 - val_loss: 0.0944\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0945 - val_loss: 0.0937\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0919 - val_loss: 0.0931\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0895 - val_loss: 0.0927\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0874 - val_loss: 0.0923\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0855 - val_loss: 0.0921\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0838 - val_loss: 0.0917\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0822 - val_loss: 0.0915\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0807 - val_loss: 0.0912\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0794 - val_loss: 0.0910\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0782 - val_loss: 0.0908\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0771 - val_loss: 0.0906\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0760 - val_loss: 0.0904\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0750 - val_loss: 0.0902\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0741 - val_loss: 0.0901\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0733 - val_loss: 0.0899\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0725 - val_loss: 0.0898\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0717 - val_loss: 0.0897\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0710 - val_loss: 0.0895\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0704 - val_loss: 0.0895\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0697 - val_loss: 0.0894\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0691 - val_loss: 0.0893\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0686 - val_loss: 0.0892\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0681 - val_loss: 0.0892\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0676 - val_loss: 0.0891\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0671 - val_loss: 0.0891\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0666 - val_loss: 0.0890\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0662 - val_loss: 0.0889\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0658 - val_loss: 0.0889\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0654 - val_loss: 0.0888\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0651 - val_loss: 0.0887\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0647 - val_loss: 0.0887\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0644 - val_loss: 0.0886\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0641 - val_loss: 0.0885\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0638 - val_loss: 0.0885\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0636 - val_loss: 0.0884\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0633 - val_loss: 0.0883\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(648, 67), test_y.shape=(648, 67)\n",
      "average MASE = 1286975259.8033035, my average MASE = 26869301348.284866\n",
      "Cluster 0, 1286975259.8033035\n",
      "Before prediction: train_X.shape=(2221, 10, 67), train_y.shape=(2221, 67), test_X.shape=(740, 10, 67), test_y.shape=(740, 67)\n",
      "Epoch 1/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.5056 - val_loss: 0.3755\n",
      "Epoch 2/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4985 - val_loss: 0.3720\n",
      "Epoch 3/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4926 - val_loss: 0.3688\n",
      "Epoch 4/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4874 - val_loss: 0.3660\n",
      "Epoch 5/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4828 - val_loss: 0.3635\n",
      "Epoch 6/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4785 - val_loss: 0.3611\n",
      "Epoch 7/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4745 - val_loss: 0.3590\n",
      "Epoch 8/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4707 - val_loss: 0.3570\n",
      "Epoch 9/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4671 - val_loss: 0.3552\n",
      "Epoch 10/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4637 - val_loss: 0.3535\n",
      "Epoch 11/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4604 - val_loss: 0.3519\n",
      "Epoch 12/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4572 - val_loss: 0.3504\n",
      "Epoch 13/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4542 - val_loss: 0.3489\n",
      "Epoch 14/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4512 - val_loss: 0.3475\n",
      "Epoch 15/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4483 - val_loss: 0.3462\n",
      "Epoch 16/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4455 - val_loss: 0.3449\n",
      "Epoch 17/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4428 - val_loss: 0.3436\n",
      "Epoch 18/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4401 - val_loss: 0.3424\n",
      "Epoch 19/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4375 - val_loss: 0.3412\n",
      "Epoch 20/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4350 - val_loss: 0.3401\n",
      "Epoch 21/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4325 - val_loss: 0.3390\n",
      "Epoch 22/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4301 - val_loss: 0.3380\n",
      "Epoch 23/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4278 - val_loss: 0.3370\n",
      "Epoch 24/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4255 - val_loss: 0.3360\n",
      "Epoch 25/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4233 - val_loss: 0.3350\n",
      "Epoch 26/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4211 - val_loss: 0.3341\n",
      "Epoch 27/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4190 - val_loss: 0.3332\n",
      "Epoch 28/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4170 - val_loss: 0.3324\n",
      "Epoch 29/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4150 - val_loss: 0.3315\n",
      "Epoch 30/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4131 - val_loss: 0.3307\n",
      "Epoch 31/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4113 - val_loss: 0.3299\n",
      "Epoch 32/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4095 - val_loss: 0.3292\n",
      "Epoch 33/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4078 - val_loss: 0.3285\n",
      "Epoch 34/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4061 - val_loss: 0.3278\n",
      "Epoch 35/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4045 - val_loss: 0.3271\n",
      "Epoch 36/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4030 - val_loss: 0.3265\n",
      "Epoch 37/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4014 - val_loss: 0.3259\n",
      "Epoch 38/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3999 - val_loss: 0.3253\n",
      "Epoch 39/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3985 - val_loss: 0.3247\n",
      "Epoch 40/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.3971 - val_loss: 0.3242\n",
      "24/24 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(740, 67), test_y.shape=(740, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 455.57479189703486, my average MASE = 880.739926655778\n",
      "Cluster 1, 455.57479189703486\n",
      "Before prediction: train_X.shape=(41, 10, 67), train_y.shape=(41, 67), test_X.shape=(14, 10, 67), test_y.shape=(14, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4961 - val_loss: 0.6170\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4945 - val_loss: 0.6160\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4928 - val_loss: 0.6151\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4912 - val_loss: 0.6141\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4896 - val_loss: 0.6132\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4880 - val_loss: 0.6123\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4864 - val_loss: 0.6114\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4849 - val_loss: 0.6105\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4834 - val_loss: 0.6096\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4818 - val_loss: 0.6088\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4803 - val_loss: 0.6080\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4788 - val_loss: 0.6071\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4774 - val_loss: 0.6063\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4759 - val_loss: 0.6055\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4745 - val_loss: 0.6047\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4731 - val_loss: 0.6040\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4717 - val_loss: 0.6033\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4703 - val_loss: 0.6025\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4689 - val_loss: 0.6018\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4676 - val_loss: 0.6012\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4663 - val_loss: 0.6005\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4649 - val_loss: 0.5998\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4636 - val_loss: 0.5992\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4624 - val_loss: 0.5985\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4611 - val_loss: 0.5979\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4598 - val_loss: 0.5973\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4586 - val_loss: 0.5967\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4574 - val_loss: 0.5961\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4561 - val_loss: 0.5955\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4549 - val_loss: 0.5950\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4537 - val_loss: 0.5944\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4525 - val_loss: 0.5939\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4513 - val_loss: 0.5933\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4501 - val_loss: 0.5928\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4489 - val_loss: 0.5922\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4478 - val_loss: 0.5917\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4466 - val_loss: 0.5912\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4455 - val_loss: 0.5907\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4443 - val_loss: 0.5902\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4432 - val_loss: 0.5897\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(14, 67), test_y.shape=(14, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3633974576.8812943, my average MASE = 9316750037.661146\n",
      "Cluster 2, 3633974576.8812943\n",
      "Before prediction: train_X.shape=(158, 10, 67), train_y.shape=(158, 67), test_X.shape=(53, 10, 67), test_y.shape=(53, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5152 - val_loss: 0.4273\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5139 - val_loss: 0.4265\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5128 - val_loss: 0.4257\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5117 - val_loss: 0.4249\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5105 - val_loss: 0.4241\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5095 - val_loss: 0.4234\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5084 - val_loss: 0.4226\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5074 - val_loss: 0.4219\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5064 - val_loss: 0.4212\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5054 - val_loss: 0.4204\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5044 - val_loss: 0.4197\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5034 - val_loss: 0.4191\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5025 - val_loss: 0.4184\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5015 - val_loss: 0.4178\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5006 - val_loss: 0.4171\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4997 - val_loss: 0.4165\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4988 - val_loss: 0.4159\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4979 - val_loss: 0.4153\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4970 - val_loss: 0.4147\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4961 - val_loss: 0.4141\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4953 - val_loss: 0.4135\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4944 - val_loss: 0.4129\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4936 - val_loss: 0.4123\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4928 - val_loss: 0.4118\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4920 - val_loss: 0.4112\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4912 - val_loss: 0.4107\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4904 - val_loss: 0.4101\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4897 - val_loss: 0.4096\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4889 - val_loss: 0.4091\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4882 - val_loss: 0.4086\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4874 - val_loss: 0.4080\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4867 - val_loss: 0.4075\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4860 - val_loss: 0.4070\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4852 - val_loss: 0.4065\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4845 - val_loss: 0.4060\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4838 - val_loss: 0.4055\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4831 - val_loss: 0.4050\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4824 - val_loss: 0.4045\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4818 - val_loss: 0.4040\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4811 - val_loss: 0.4036\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(53, 67), test_y.shape=(53, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 113.07393682707622, my average MASE = 188079219.47717956\n",
      "Cluster 3, 113.07393682707622\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7117 - val_loss: 0.7129\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7087 - val_loss: 0.7117\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7057 - val_loss: 0.7104\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7028 - val_loss: 0.7094\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7000 - val_loss: 0.7084\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6972 - val_loss: 0.7075\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6945 - val_loss: 0.7066\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6918 - val_loss: 0.7056\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6891 - val_loss: 0.7047\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6865 - val_loss: 0.7037\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6838 - val_loss: 0.7028\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6811 - val_loss: 0.7018\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6785 - val_loss: 0.7009\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6758 - val_loss: 0.6999\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6732 - val_loss: 0.6990\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6706 - val_loss: 0.6981\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6680 - val_loss: 0.6971\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6654 - val_loss: 0.6961\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6628 - val_loss: 0.6951\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6604 - val_loss: 0.6942\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6580 - val_loss: 0.6934\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6556 - val_loss: 0.6926\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6534 - val_loss: 0.6918\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6512 - val_loss: 0.6910\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6491 - val_loss: 0.6902\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6470 - val_loss: 0.6894\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6449 - val_loss: 0.6886\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6429 - val_loss: 0.6877\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6409 - val_loss: 0.6869\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6389 - val_loss: 0.6861\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6369 - val_loss: 0.6853\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6350 - val_loss: 0.6847\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6332 - val_loss: 0.6841\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6314 - val_loss: 0.6836\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6297 - val_loss: 0.6831\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6280 - val_loss: 0.6827\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6263 - val_loss: 0.6825\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6246 - val_loss: 0.6824\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6229 - val_loss: 0.6823\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6212 - val_loss: 0.6822\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.2798887382418345, my average MASE = 0.5334982550486497\n",
      "Cluster 4, 0.2798887382418345\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=7, 7, 431, (13, 67)\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(180, 10, 67), train_y.shape=(180, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6852 - val_loss: 0.5575\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6838 - val_loss: 0.5569\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6824 - val_loss: 0.5563\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6811 - val_loss: 0.5557\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6799 - val_loss: 0.5551\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6786 - val_loss: 0.5545\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6774 - val_loss: 0.5540\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6762 - val_loss: 0.5534\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6750 - val_loss: 0.5528\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6739 - val_loss: 0.5523\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6727 - val_loss: 0.5518\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6716 - val_loss: 0.5512\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6705 - val_loss: 0.5507\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6694 - val_loss: 0.5501\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6684 - val_loss: 0.5496\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6673 - val_loss: 0.5491\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6663 - val_loss: 0.5486\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6653 - val_loss: 0.5481\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6643 - val_loss: 0.5475\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6632 - val_loss: 0.5470\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6623 - val_loss: 0.5465\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6613 - val_loss: 0.5461\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6603 - val_loss: 0.5456\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6594 - val_loss: 0.5451\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6584 - val_loss: 0.5446\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6575 - val_loss: 0.5441\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6566 - val_loss: 0.5437\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6557 - val_loss: 0.5432\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6548 - val_loss: 0.5427\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6539 - val_loss: 0.5423\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6531 - val_loss: 0.5418\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6522 - val_loss: 0.5414\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6513 - val_loss: 0.5409\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6505 - val_loss: 0.5405\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6497 - val_loss: 0.5400\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6489 - val_loss: 0.5396\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6480 - val_loss: 0.5392\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6472 - val_loss: 0.5387\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6464 - val_loss: 0.5383\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6456 - val_loss: 0.5379\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 161.0036674860674, my average MASE = 412096886.9283237\n",
      "Cluster 1, 161.0036674860674\n",
      "Before prediction: train_X.shape=(3, 10, 67), train_y.shape=(3, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6031 - val_loss: 0.4390\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6002 - val_loss: 0.4369\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5975 - val_loss: 0.4349\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5947 - val_loss: 0.4328\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5919 - val_loss: 0.4307\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5892 - val_loss: 0.4287\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5864 - val_loss: 0.4267\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5838 - val_loss: 0.4248\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5812 - val_loss: 0.4230\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5786 - val_loss: 0.4213\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5761 - val_loss: 0.4196\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5736 - val_loss: 0.4180\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5712 - val_loss: 0.4164\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5687 - val_loss: 0.4148\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5663 - val_loss: 0.4132\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5638 - val_loss: 0.4116\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5614 - val_loss: 0.4101\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5590 - val_loss: 0.4085\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5566 - val_loss: 0.4070\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5542 - val_loss: 0.4054\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5518 - val_loss: 0.4039\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5494 - val_loss: 0.4024\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5471 - val_loss: 0.4008\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5448 - val_loss: 0.3992\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5424 - val_loss: 0.3977\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5401 - val_loss: 0.3961\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5378 - val_loss: 0.3945\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5355 - val_loss: 0.3930\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5333 - val_loss: 0.3914\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5310 - val_loss: 0.3898\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5288 - val_loss: 0.3883\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5266 - val_loss: 0.3868\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5243 - val_loss: 0.3855\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5221 - val_loss: 0.3842\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5199 - val_loss: 0.3830\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5176 - val_loss: 0.3817\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5154 - val_loss: 0.3804\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5133 - val_loss: 0.3792\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5111 - val_loss: 0.3780\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5089 - val_loss: 0.3770\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.22679251401977166, my average MASE = 0.3179573135228401\n",
      "Cluster 2, 0.22679251401977166\n",
      "Before prediction: train_X.shape=(39, 10, 67), train_y.shape=(39, 67), test_X.shape=(13, 10, 67), test_y.shape=(13, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4783 - val_loss: 0.4457\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4766 - val_loss: 0.4444\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4750 - val_loss: 0.4432\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4734 - val_loss: 0.4420\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4719 - val_loss: 0.4408\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4703 - val_loss: 0.4397\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4687 - val_loss: 0.4385\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4672 - val_loss: 0.4374\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4657 - val_loss: 0.4362\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4642 - val_loss: 0.4351\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4627 - val_loss: 0.4340\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4613 - val_loss: 0.4329\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4598 - val_loss: 0.4318\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4584 - val_loss: 0.4307\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4569 - val_loss: 0.4296\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4555 - val_loss: 0.4286\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4541 - val_loss: 0.4275\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4527 - val_loss: 0.4265\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4513 - val_loss: 0.4255\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4499 - val_loss: 0.4245\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4486 - val_loss: 0.4235\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4472 - val_loss: 0.4225\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4459 - val_loss: 0.4215\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4445 - val_loss: 0.4205\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4432 - val_loss: 0.4195\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4419 - val_loss: 0.4186\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4406 - val_loss: 0.4176\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4393 - val_loss: 0.4167\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4380 - val_loss: 0.4158\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4368 - val_loss: 0.4149\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4355 - val_loss: 0.4139\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4343 - val_loss: 0.4130\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4330 - val_loss: 0.4121\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4318 - val_loss: 0.4112\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4306 - val_loss: 0.4103\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4294 - val_loss: 0.4094\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4282 - val_loss: 0.4085\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4270 - val_loss: 0.4077\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4258 - val_loss: 0.4068\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4246 - val_loss: 0.4059\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(13, 67), test_y.shape=(13, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3532936323.38913, my average MASE = 7800622104.848636\n",
      "Cluster 3, 3532936323.38913\n",
      "Before prediction: train_X.shape=(152, 10, 67), train_y.shape=(152, 67), test_X.shape=(51, 10, 67), test_y.shape=(51, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3354 - val_loss: 0.2774\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3344 - val_loss: 0.2769\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3336 - val_loss: 0.2763\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3328 - val_loss: 0.2758\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3320 - val_loss: 0.2753\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3312 - val_loss: 0.2747\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3304 - val_loss: 0.2742\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3297 - val_loss: 0.2737\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3290 - val_loss: 0.2733\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3282 - val_loss: 0.2728\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3275 - val_loss: 0.2723\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.3268 - val_loss: 0.2718\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3262 - val_loss: 0.2714\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3255 - val_loss: 0.2709\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3248 - val_loss: 0.2705\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3242 - val_loss: 0.2700\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3235 - val_loss: 0.2696\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3229 - val_loss: 0.2692\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3223 - val_loss: 0.2687\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3217 - val_loss: 0.2683\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3210 - val_loss: 0.2678\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3204 - val_loss: 0.2674\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3198 - val_loss: 0.2670\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3192 - val_loss: 0.2666\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3187 - val_loss: 0.2662\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3181 - val_loss: 0.2658\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3175 - val_loss: 0.2654\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3170 - val_loss: 0.2650\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3164 - val_loss: 0.2647\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3158 - val_loss: 0.2643\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3153 - val_loss: 0.2639\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3148 - val_loss: 0.2635\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3142 - val_loss: 0.2632\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3137 - val_loss: 0.2628\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3132 - val_loss: 0.2624\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3127 - val_loss: 0.2621\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3122 - val_loss: 0.2617\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3117 - val_loss: 0.2614\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3112 - val_loss: 0.2610\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3107 - val_loss: 0.2607\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(51, 67), test_y.shape=(51, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 185.09520141259705, my average MASE = 132066880.5161765\n",
      "Cluster 4, 185.09520141259705\n",
      "Before prediction: train_X.shape=(5958, 10, 67), train_y.shape=(5958, 67), test_X.shape=(1986, 10, 67), test_y.shape=(1986, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0679 - val_loss: 0.0461\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0619 - val_loss: 0.0426\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0586 - val_loss: 0.0401\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0561 - val_loss: 0.0381\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0542 - val_loss: 0.0366\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0527 - val_loss: 0.0352\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0514 - val_loss: 0.0341\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0503 - val_loss: 0.0333\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0494 - val_loss: 0.0325\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0486 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0479 - val_loss: 0.0312\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0473 - val_loss: 0.0307\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0468 - val_loss: 0.0303\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0463 - val_loss: 0.0299\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0459 - val_loss: 0.0296\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0455 - val_loss: 0.0293\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0452 - val_loss: 0.0290\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0448 - val_loss: 0.0288\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0445 - val_loss: 0.0286\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0443 - val_loss: 0.0284\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0440 - val_loss: 0.0283\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0438 - val_loss: 0.0281\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0436 - val_loss: 0.0279\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0434 - val_loss: 0.0278\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0432 - val_loss: 0.0277\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0430 - val_loss: 0.0276\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0429 - val_loss: 0.0274\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0427 - val_loss: 0.0273\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0426 - val_loss: 0.0273\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0425 - val_loss: 0.0272\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0423 - val_loss: 0.0271\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0422 - val_loss: 0.0270\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0421 - val_loss: 0.0269\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 34ms/step - loss: 0.0420 - val_loss: 0.0269\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0419 - val_loss: 0.0268\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0418 - val_loss: 0.0268\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0417 - val_loss: 0.0267\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0416 - val_loss: 0.0267\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0415 - val_loss: 0.0266\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0415 - val_loss: 0.0266\n",
      "63/63 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(1986, 67), test_y.shape=(1986, 67)\n",
      "average MASE = 1042799882.6542007, my average MASE = 20591462481.802387\n",
      "Cluster 5, 1042799882.6542007\n",
      "Before prediction: train_X.shape=(82, 10, 67), train_y.shape=(82, 67), test_X.shape=(27, 10, 67), test_y.shape=(27, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3924 - val_loss: 0.4859\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3915 - val_loss: 0.4852\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3907 - val_loss: 0.4846\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3899 - val_loss: 0.4840\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3891 - val_loss: 0.4835\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3883 - val_loss: 0.4829\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3876 - val_loss: 0.4824\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3868 - val_loss: 0.4818\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3861 - val_loss: 0.4813\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3854 - val_loss: 0.4808\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3847 - val_loss: 0.4802\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3841 - val_loss: 0.4797\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3834 - val_loss: 0.4792\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3828 - val_loss: 0.4787\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3821 - val_loss: 0.4783\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3815 - val_loss: 0.4778\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3809 - val_loss: 0.4773\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3804 - val_loss: 0.4769\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3798 - val_loss: 0.4764\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3792 - val_loss: 0.4760\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3787 - val_loss: 0.4755\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3781 - val_loss: 0.4751\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3776 - val_loss: 0.4747\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3771 - val_loss: 0.4743\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3766 - val_loss: 0.4738\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3761 - val_loss: 0.4734\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3756 - val_loss: 0.4731\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3751 - val_loss: 0.4727\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3746 - val_loss: 0.4723\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3741 - val_loss: 0.4719\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3737 - val_loss: 0.4715\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3732 - val_loss: 0.4711\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3728 - val_loss: 0.4707\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3723 - val_loss: 0.4703\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3719 - val_loss: 0.4699\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3715 - val_loss: 0.4696\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3710 - val_loss: 0.4692\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3706 - val_loss: 0.4688\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3702 - val_loss: 0.4684\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3698 - val_loss: 0.4681\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(27, 67), test_y.shape=(27, 67)\n",
      "average MASE = 307.7375965485456, my average MASE = 158440912.42565766\n",
      "Cluster 6, 307.7375965485456\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=9, 9, 32, (76, 67)\n",
      "Before prediction: train_X.shape=(39, 10, 67), train_y.shape=(39, 67), test_X.shape=(13, 10, 67), test_y.shape=(13, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4763 - val_loss: 0.4497\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4746 - val_loss: 0.4487\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4729 - val_loss: 0.4477\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4713 - val_loss: 0.4467\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4696 - val_loss: 0.4458\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4680 - val_loss: 0.4448\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4663 - val_loss: 0.4439\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4647 - val_loss: 0.4429\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4631 - val_loss: 0.4420\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4616 - val_loss: 0.4410\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4600 - val_loss: 0.4401\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4585 - val_loss: 0.4392\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4569 - val_loss: 0.4382\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4554 - val_loss: 0.4373\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4539 - val_loss: 0.4364\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4524 - val_loss: 0.4355\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4510 - val_loss: 0.4345\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4495 - val_loss: 0.4336\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4481 - val_loss: 0.4327\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4467 - val_loss: 0.4319\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4453 - val_loss: 0.4310\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4439 - val_loss: 0.4301\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4425 - val_loss: 0.4293\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4412 - val_loss: 0.4284\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4398 - val_loss: 0.4276\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4385 - val_loss: 0.4267\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4371 - val_loss: 0.4259\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4358 - val_loss: 0.4251\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4345 - val_loss: 0.4243\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4332 - val_loss: 0.4234\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4320 - val_loss: 0.4226\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4307 - val_loss: 0.4218\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4294 - val_loss: 0.4210\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4282 - val_loss: 0.4202\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4270 - val_loss: 0.4195\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4258 - val_loss: 0.4187\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4246 - val_loss: 0.4179\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4234 - val_loss: 0.4172\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4222 - val_loss: 0.4164\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4211 - val_loss: 0.4157\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(13, 67), test_y.shape=(13, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3502979890.5184355, my average MASE = 5633924620.870799\n",
      "Cluster 0, 3502979890.5184355\n",
      "Before prediction: train_X.shape=(1945, 10, 67), train_y.shape=(1945, 67), test_X.shape=(648, 10, 67), test_y.shape=(648, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.1163 - val_loss: 0.0998\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1111 - val_loss: 0.0974\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1069 - val_loss: 0.0957\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1034 - val_loss: 0.0946\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1003 - val_loss: 0.0937\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0976 - val_loss: 0.0929\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0952 - val_loss: 0.0924\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0929 - val_loss: 0.0919\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0909 - val_loss: 0.0914\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0891 - val_loss: 0.0911\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0874 - val_loss: 0.0907\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0859 - val_loss: 0.0904\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0845 - val_loss: 0.0902\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0831 - val_loss: 0.0900\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0819 - val_loss: 0.0898\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0808 - val_loss: 0.0896\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0797 - val_loss: 0.0894\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0787 - val_loss: 0.0893\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0778 - val_loss: 0.0892\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0769 - val_loss: 0.0890\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0761 - val_loss: 0.0890\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0753 - val_loss: 0.0889\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0745 - val_loss: 0.0888\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0738 - val_loss: 0.0887\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0731 - val_loss: 0.0886\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0725 - val_loss: 0.0886\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0719 - val_loss: 0.0885\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0713 - val_loss: 0.0885\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0707 - val_loss: 0.0884\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0702 - val_loss: 0.0884\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0697 - val_loss: 0.0883\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0692 - val_loss: 0.0883\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0688 - val_loss: 0.0882\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0683 - val_loss: 0.0882\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0679 - val_loss: 0.0881\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0676 - val_loss: 0.0881\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0672 - val_loss: 0.0880\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0668 - val_loss: 0.0880\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0665 - val_loss: 0.0879\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0662 - val_loss: 0.0879\n",
      "21/21 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(648, 67), test_y.shape=(648, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1199834821.8603294, my average MASE = 15362171394.25593\n",
      "Cluster 1, 1199834821.8603294\n",
      "Before prediction: train_X.shape=(1567, 10, 67), train_y.shape=(1567, 67), test_X.shape=(522, 10, 67), test_y.shape=(522, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.2378 - val_loss: 0.2753\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2280 - val_loss: 0.2658\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2204 - val_loss: 0.2582\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2142 - val_loss: 0.2520\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2091 - val_loss: 0.2467\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2046 - val_loss: 0.2420\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2006 - val_loss: 0.2378\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1971 - val_loss: 0.2340\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1938 - val_loss: 0.2304\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1908 - val_loss: 0.2272\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1880 - val_loss: 0.2241\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1853 - val_loss: 0.2213\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1829 - val_loss: 0.2187\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1806 - val_loss: 0.2163\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1784 - val_loss: 0.2141\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1765 - val_loss: 0.2121\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1747 - val_loss: 0.2103\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1730 - val_loss: 0.2086\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1715 - val_loss: 0.2071\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1702 - val_loss: 0.2056\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1689 - val_loss: 0.2042\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1677 - val_loss: 0.2029\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1666 - val_loss: 0.2016\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1655 - val_loss: 0.2005\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1645 - val_loss: 0.1993\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1636 - val_loss: 0.1983\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1626 - val_loss: 0.1973\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1618 - val_loss: 0.1962\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1610 - val_loss: 0.1953\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1602 - val_loss: 0.1944\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.1594 - val_loss: 0.1935\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.1587 - val_loss: 0.1927\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1580 - val_loss: 0.1919\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1573 - val_loss: 0.1911\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1567 - val_loss: 0.1904\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1561 - val_loss: 0.1897\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1555 - val_loss: 0.1890\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1550 - val_loss: 0.1883\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1544 - val_loss: 0.1877\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1539 - val_loss: 0.1871\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(522, 67), test_y.shape=(522, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 164.5516081334958, my average MASE = 793528060.8539205\n",
      "Cluster 2, 164.5516081334958\n",
      "Before prediction: train_X.shape=(88, 10, 67), train_y.shape=(88, 67), test_X.shape=(29, 10, 67), test_y.shape=(29, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3929 - val_loss: 0.4650\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3920 - val_loss: 0.4646\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3912 - val_loss: 0.4642\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3904 - val_loss: 0.4638\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3896 - val_loss: 0.4635\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3889 - val_loss: 0.4631\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3881 - val_loss: 0.4627\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3874 - val_loss: 0.4624\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3867 - val_loss: 0.4621\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3860 - val_loss: 0.4617\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3854 - val_loss: 0.4614\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3847 - val_loss: 0.4611\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3841 - val_loss: 0.4608\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3835 - val_loss: 0.4605\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3829 - val_loss: 0.4602\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3823 - val_loss: 0.4600\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3817 - val_loss: 0.4597\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3812 - val_loss: 0.4594\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3806 - val_loss: 0.4591\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3800 - val_loss: 0.4588\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3795 - val_loss: 0.4585\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3790 - val_loss: 0.4583\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3785 - val_loss: 0.4580\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3780 - val_loss: 0.4578\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3775 - val_loss: 0.4575\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3770 - val_loss: 0.4572\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3765 - val_loss: 0.4570\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3761 - val_loss: 0.4567\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3756 - val_loss: 0.4565\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3752 - val_loss: 0.4563\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3747 - val_loss: 0.4560\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3743 - val_loss: 0.4558\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3738 - val_loss: 0.4555\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3734 - val_loss: 0.4553\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3730 - val_loss: 0.4551\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3725 - val_loss: 0.4549\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3721 - val_loss: 0.4547\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3717 - val_loss: 0.4545\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3713 - val_loss: 0.4542\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3709 - val_loss: 0.4540\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(29, 67), test_y.shape=(29, 67)\n",
      "average MASE = 177.05816941536048, my average MASE = 104373067.45181417\n",
      "Cluster 3, 177.05816941536048\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4712 - val_loss: 0.3171\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4689 - val_loss: 0.3159\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4666 - val_loss: 0.3148\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4644 - val_loss: 0.3137\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4621 - val_loss: 0.3125\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4598 - val_loss: 0.3114\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4576 - val_loss: 0.3103\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4553 - val_loss: 0.3092\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4531 - val_loss: 0.3081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4509 - val_loss: 0.3071\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4487 - val_loss: 0.3062\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4465 - val_loss: 0.3053\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4444 - val_loss: 0.3044\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4422 - val_loss: 0.3036\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4401 - val_loss: 0.3028\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4379 - val_loss: 0.3020\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4358 - val_loss: 0.3012\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4337 - val_loss: 0.3004\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4317 - val_loss: 0.2996\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4297 - val_loss: 0.2988\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4278 - val_loss: 0.2979\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4259 - val_loss: 0.2970\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4240 - val_loss: 0.2961\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4222 - val_loss: 0.2952\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4204 - val_loss: 0.2943\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4186 - val_loss: 0.2934\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4168 - val_loss: 0.2924\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4151 - val_loss: 0.2915\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4134 - val_loss: 0.2906\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4118 - val_loss: 0.2898\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4102 - val_loss: 0.2890\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4086 - val_loss: 0.2883\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4070 - val_loss: 0.2876\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4054 - val_loss: 0.2869\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4039 - val_loss: 0.2863\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4024 - val_loss: 0.2856\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4009 - val_loss: 0.2850\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3995 - val_loss: 0.2845\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3980 - val_loss: 0.2840\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3966 - val_loss: 0.2835\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 254.89087963107983, my average MASE = 19795911.088192035\n",
      "Cluster 5, 254.89087963107983\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(176, 10, 67), train_y.shape=(176, 67), test_X.shape=(59, 10, 67), test_y.shape=(59, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.7010 - val_loss: 0.5674\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6997 - val_loss: 0.5667\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6984 - val_loss: 0.5660\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6972 - val_loss: 0.5654\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6960 - val_loss: 0.5647\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6948 - val_loss: 0.5641\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6937 - val_loss: 0.5635\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6925 - val_loss: 0.5629\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6914 - val_loss: 0.5623\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6903 - val_loss: 0.5617\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6893 - val_loss: 0.5611\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6882 - val_loss: 0.5605\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6871 - val_loss: 0.5599\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6861 - val_loss: 0.5594\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6851 - val_loss: 0.5588\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6841 - val_loss: 0.5583\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6831 - val_loss: 0.5577\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6821 - val_loss: 0.5572\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6811 - val_loss: 0.5567\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6802 - val_loss: 0.5562\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6792 - val_loss: 0.5556\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6783 - val_loss: 0.5551\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6774 - val_loss: 0.5546\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6765 - val_loss: 0.5541\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6756 - val_loss: 0.5536\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6747 - val_loss: 0.5531\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6738 - val_loss: 0.5526\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6729 - val_loss: 0.5521\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6720 - val_loss: 0.5516\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6712 - val_loss: 0.5512\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6703 - val_loss: 0.5507\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6695 - val_loss: 0.5502\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6686 - val_loss: 0.5497\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6678 - val_loss: 0.5493\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6670 - val_loss: 0.5488\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6661 - val_loss: 0.5483\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6653 - val_loss: 0.5479\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6645 - val_loss: 0.5474\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6637 - val_loss: 0.5469\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6629 - val_loss: 0.5464\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(59, 67), test_y.shape=(59, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 154.34881195949765, my average MASE = 318959730.30915844\n",
      "Cluster 7, 154.34881195949765\n",
      "Before prediction: train_X.shape=(150, 10, 67), train_y.shape=(150, 67), test_X.shape=(50, 10, 67), test_y.shape=(50, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3127 - val_loss: 0.2620\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3118 - val_loss: 0.2614\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3110 - val_loss: 0.2608\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3103 - val_loss: 0.2603\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3096 - val_loss: 0.2597\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3088 - val_loss: 0.2592\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3081 - val_loss: 0.2587\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3074 - val_loss: 0.2582\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3067 - val_loss: 0.2577\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3060 - val_loss: 0.2572\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3054 - val_loss: 0.2567\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3047 - val_loss: 0.2563\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3041 - val_loss: 0.2558\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3035 - val_loss: 0.2554\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3029 - val_loss: 0.2549\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3023 - val_loss: 0.2545\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3017 - val_loss: 0.2541\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3011 - val_loss: 0.2537\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3005 - val_loss: 0.2533\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3000 - val_loss: 0.2529\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.2994 - val_loss: 0.2525\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.2988 - val_loss: 0.2521\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.2983 - val_loss: 0.2517\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.2978 - val_loss: 0.2513\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2973 - val_loss: 0.2509\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.2967 - val_loss: 0.2505\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2962 - val_loss: 0.2502\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2957 - val_loss: 0.2498\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2952 - val_loss: 0.2494\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2948 - val_loss: 0.2491\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.2943 - val_loss: 0.2487\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.2938 - val_loss: 0.2484\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2933 - val_loss: 0.2480\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.2929 - val_loss: 0.2477\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.2924 - val_loss: 0.2473\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.2920 - val_loss: 0.2470\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.2915 - val_loss: 0.2467\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2911 - val_loss: 0.2463\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.2907 - val_loss: 0.2460\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.2902 - val_loss: 0.2457\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(50, 67), test_y.shape=(50, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 222.6692241509544, my average MASE = 77008768.33506098\n",
      "Cluster 8, 222.6692241509544\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=11, 11, 435, (11, 67)\n",
      "Before prediction: train_X.shape=(5, 10, 67), train_y.shape=(5, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4436 - val_loss: 0.3307\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4426 - val_loss: 0.3306\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4417 - val_loss: 0.3306\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4408 - val_loss: 0.3305\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4398 - val_loss: 0.3305\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4389 - val_loss: 0.3304\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4380 - val_loss: 0.3303\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4371 - val_loss: 0.3303\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4362 - val_loss: 0.3302\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4354 - val_loss: 0.3301\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4345 - val_loss: 0.3301\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4337 - val_loss: 0.3300\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4329 - val_loss: 0.3299\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4320 - val_loss: 0.3299\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4312 - val_loss: 0.3298\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4304 - val_loss: 0.3298\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4296 - val_loss: 0.3297\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4288 - val_loss: 0.3296\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4280 - val_loss: 0.3295\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4273 - val_loss: 0.3295\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4265 - val_loss: 0.3294\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4258 - val_loss: 0.3293\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4250 - val_loss: 0.3293\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4243 - val_loss: 0.3292\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4235 - val_loss: 0.3291\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4228 - val_loss: 0.3291\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4221 - val_loss: 0.3290\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4213 - val_loss: 0.3290\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4206 - val_loss: 0.3289\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4199 - val_loss: 0.3288\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4191 - val_loss: 0.3288\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4184 - val_loss: 0.3287\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4177 - val_loss: 0.3287\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4170 - val_loss: 0.3286\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4163 - val_loss: 0.3286\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4156 - val_loss: 0.3285\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4149 - val_loss: 0.3285\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4143 - val_loss: 0.3285\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4136 - val_loss: 0.3284\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4129 - val_loss: 0.3284\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 623.4242587364998, my average MASE = 13116998.58946489\n",
      "Cluster 0, 623.4242587364998\n",
      "Before prediction: train_X.shape=(18, 10, 67), train_y.shape=(18, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4512 - val_loss: 0.5917\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4491 - val_loss: 0.5900\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4470 - val_loss: 0.5884\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4449 - val_loss: 0.5867\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4429 - val_loss: 0.5851\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4408 - val_loss: 0.5835\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4388 - val_loss: 0.5818\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4369 - val_loss: 0.5802\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4349 - val_loss: 0.5786\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4330 - val_loss: 0.5770\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4311 - val_loss: 0.5755\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4293 - val_loss: 0.5739\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4275 - val_loss: 0.5724\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4257 - val_loss: 0.5709\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4239 - val_loss: 0.5694\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4222 - val_loss: 0.5680\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4205 - val_loss: 0.5666\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4188 - val_loss: 0.5652\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4171 - val_loss: 0.5638\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4155 - val_loss: 0.5625\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4139 - val_loss: 0.5611\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4123 - val_loss: 0.5598\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4107 - val_loss: 0.5584\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4092 - val_loss: 0.5571\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4077 - val_loss: 0.5558\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4062 - val_loss: 0.5545\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4047 - val_loss: 0.5532\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4032 - val_loss: 0.5519\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4017 - val_loss: 0.5506\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4003 - val_loss: 0.5494\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3988 - val_loss: 0.5482\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3974 - val_loss: 0.5470\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3960 - val_loss: 0.5458\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3946 - val_loss: 0.5446\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3933 - val_loss: 0.5435\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3920 - val_loss: 0.5424\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3906 - val_loss: 0.5414\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3893 - val_loss: 0.5403\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3881 - val_loss: 0.5393\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3868 - val_loss: 0.5382\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1669347283.8471758, my average MASE = 4344504132.76493\n",
      "Cluster 1, 1669347283.8471758\n",
      "Before prediction: train_X.shape=(5958, 10, 67), train_y.shape=(5958, 67), test_X.shape=(1986, 10, 67), test_y.shape=(1986, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0678 - val_loss: 0.0460\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0618 - val_loss: 0.0427\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0585 - val_loss: 0.0404\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0561 - val_loss: 0.0385\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0542 - val_loss: 0.0369\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0527 - val_loss: 0.0355\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0514 - val_loss: 0.0344\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0503 - val_loss: 0.0335\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0494 - val_loss: 0.0327\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0486 - val_loss: 0.0321\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0479 - val_loss: 0.0315\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0473 - val_loss: 0.0310\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0468 - val_loss: 0.0306\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0463 - val_loss: 0.0302\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0458 - val_loss: 0.0299\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0454 - val_loss: 0.0296\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0451 - val_loss: 0.0293\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0447 - val_loss: 0.0291\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0444 - val_loss: 0.0289\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0442 - val_loss: 0.0287\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0439 - val_loss: 0.0285\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0437 - val_loss: 0.0283\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0434 - val_loss: 0.0281\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0432 - val_loss: 0.0280\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0430 - val_loss: 0.0278\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0429 - val_loss: 0.0277\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0427 - val_loss: 0.0276\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0425 - val_loss: 0.0274\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0424 - val_loss: 0.0273\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0423 - val_loss: 0.0272\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0421 - val_loss: 0.0271\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0420 - val_loss: 0.0270\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0419 - val_loss: 0.0269\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0418 - val_loss: 0.0268\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0417 - val_loss: 0.0267\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0416 - val_loss: 0.0266\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0415 - val_loss: 0.0265\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0414 - val_loss: 0.0265\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0413 - val_loss: 0.0264\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0413 - val_loss: 0.0263\n",
      "63/63 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(1986, 67), test_y.shape=(1986, 67)\n",
      "average MASE = 811440996.3412759, my average MASE = 40052487777.48425\n",
      "Cluster 2, 811440996.3412759\n",
      "Before prediction: train_X.shape=(11, 10, 67), train_y.shape=(11, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.1986 - val_loss: 0.8095\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1967 - val_loss: 0.8095\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1948 - val_loss: 0.8095\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.1930 - val_loss: 0.8094\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1912 - val_loss: 0.8094\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1894 - val_loss: 0.8094\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1876 - val_loss: 0.8094\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1858 - val_loss: 0.8094\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.1840 - val_loss: 0.8094\n",
      "Epoch 9: early stopping\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 72.88641883174732, my average MASE = 50002377.286928184\n",
      "Cluster 3, 72.88641883174732\n",
      "Before prediction: train_X.shape=(87, 10, 67), train_y.shape=(87, 67), test_X.shape=(29, 10, 67), test_y.shape=(29, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5691 - val_loss: 0.5072\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5682 - val_loss: 0.5068\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5674 - val_loss: 0.5063\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5666 - val_loss: 0.5059\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5659 - val_loss: 0.5055\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5651 - val_loss: 0.5051\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5643 - val_loss: 0.5047\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5636 - val_loss: 0.5042\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5628 - val_loss: 0.5038\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5621 - val_loss: 0.5034\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5614 - val_loss: 0.5030\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5606 - val_loss: 0.5026\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5600 - val_loss: 0.5022\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5592 - val_loss: 0.5018\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5586 - val_loss: 0.5014\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5579 - val_loss: 0.5011\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5572 - val_loss: 0.5007\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5565 - val_loss: 0.5003\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5558 - val_loss: 0.4999\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5552 - val_loss: 0.4996\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5545 - val_loss: 0.4993\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5539 - val_loss: 0.4989\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5532 - val_loss: 0.4986\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5526 - val_loss: 0.4983\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5520 - val_loss: 0.4979\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5513 - val_loss: 0.4976\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5507 - val_loss: 0.4973\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5501 - val_loss: 0.4970\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5495 - val_loss: 0.4967\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5489 - val_loss: 0.4964\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5483 - val_loss: 0.4961\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5477 - val_loss: 0.4958\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5471 - val_loss: 0.4956\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5465 - val_loss: 0.4953\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5459 - val_loss: 0.4950\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5453 - val_loss: 0.4947\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5447 - val_loss: 0.4944\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5442 - val_loss: 0.4942\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5436 - val_loss: 0.4939\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5430 - val_loss: 0.4936\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(29, 67), test_y.shape=(29, 67)\n",
      "average MASE = 98.29689269118158, my average MASE = 78492579.83265808\n",
      "Cluster 4, 98.29689269118158\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(13, 10, 67), train_y.shape=(13, 67), test_X.shape=(4, 10, 67), test_y.shape=(4, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4003 - val_loss: 0.3997\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3997 - val_loss: 0.3991\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3991 - val_loss: 0.3986\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3985 - val_loss: 0.3981\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3979 - val_loss: 0.3976\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3973 - val_loss: 0.3971\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3968 - val_loss: 0.3967\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3962 - val_loss: 0.3962\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3957 - val_loss: 0.3958\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3951 - val_loss: 0.3953\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3946 - val_loss: 0.3949\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3940 - val_loss: 0.3945\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3935 - val_loss: 0.3941\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3930 - val_loss: 0.3937\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3924 - val_loss: 0.3932\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3919 - val_loss: 0.3928\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3914 - val_loss: 0.3925\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3909 - val_loss: 0.3921\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3904 - val_loss: 0.3917\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3899 - val_loss: 0.3913\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3894 - val_loss: 0.3909\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3889 - val_loss: 0.3906\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3884 - val_loss: 0.3902\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3879 - val_loss: 0.3898\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3874 - val_loss: 0.3894\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3870 - val_loss: 0.3891\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3865 - val_loss: 0.3887\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3860 - val_loss: 0.3883\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3856 - val_loss: 0.3880\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3851 - val_loss: 0.3876\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3847 - val_loss: 0.3872\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3842 - val_loss: 0.3869\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3838 - val_loss: 0.3865\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3833 - val_loss: 0.3861\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3829 - val_loss: 0.3857\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3824 - val_loss: 0.3853\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3820 - val_loss: 0.3850\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3815 - val_loss: 0.3846\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3811 - val_loss: 0.3842\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3807 - val_loss: 0.3838\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(4, 67), test_y.shape=(4, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 517837.8778283109, my average MASE = 16943290.739345513\n",
      "Cluster 6, 517837.8778283109\n",
      "Before prediction: train_X.shape=(17, 10, 67), train_y.shape=(17, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2151 - val_loss: 0.4042\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2146 - val_loss: 0.4041\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2142 - val_loss: 0.4041\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2137 - val_loss: 0.4040\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2133 - val_loss: 0.4040\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2128 - val_loss: 0.4040\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2124 - val_loss: 0.4039\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2119 - val_loss: 0.4039\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2115 - val_loss: 0.4038\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2111 - val_loss: 0.4038\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2107 - val_loss: 0.4038\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2103 - val_loss: 0.4038\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2099 - val_loss: 0.4037\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2096 - val_loss: 0.4037\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2092 - val_loss: 0.4037\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2088 - val_loss: 0.4036\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2084 - val_loss: 0.4036\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2081 - val_loss: 0.4036\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2077 - val_loss: 0.4035\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2074 - val_loss: 0.4035\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2070 - val_loss: 0.4035\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2067 - val_loss: 0.4035\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2063 - val_loss: 0.4035\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2060 - val_loss: 0.4034\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2057 - val_loss: 0.4034\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2054 - val_loss: 0.4034\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2050 - val_loss: 0.4034\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2047 - val_loss: 0.4033\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2044 - val_loss: 0.4033\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2041 - val_loss: 0.4033\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2038 - val_loss: 0.4032\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2035 - val_loss: 0.4032\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2032 - val_loss: 0.4032\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2029 - val_loss: 0.4031\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2026 - val_loss: 0.4031\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2023 - val_loss: 0.4031\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2020 - val_loss: 0.4030\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2017 - val_loss: 0.4030\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2014 - val_loss: 0.4029\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2011 - val_loss: 0.4029\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 181.01448499364625, my average MASE = 73613295.18701741\n",
      "Cluster 7, 181.01448499364625\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "FAIL - test_X.shape=(1, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(2, 10, 67)\n",
      "Before prediction: train_X.shape=(10, 10, 67), train_y.shape=(10, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2941 - val_loss: 0.3652\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2932 - val_loss: 0.3650\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2923 - val_loss: 0.3648\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2914 - val_loss: 0.3646\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2905 - val_loss: 0.3644\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2897 - val_loss: 0.3641\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2888 - val_loss: 0.3639\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2880 - val_loss: 0.3638\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2872 - val_loss: 0.3636\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2864 - val_loss: 0.3634\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2856 - val_loss: 0.3632\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2848 - val_loss: 0.3631\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2840 - val_loss: 0.3629\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2832 - val_loss: 0.3628\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2824 - val_loss: 0.3626\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2817 - val_loss: 0.3625\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2809 - val_loss: 0.3624\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2802 - val_loss: 0.3622\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2794 - val_loss: 0.3621\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2787 - val_loss: 0.3620\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2780 - val_loss: 0.3619\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2772 - val_loss: 0.3618\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2765 - val_loss: 0.3617\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2758 - val_loss: 0.3616\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2751 - val_loss: 0.3615\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2744 - val_loss: 0.3615\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2738 - val_loss: 0.3614\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2731 - val_loss: 0.3613\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2725 - val_loss: 0.3613\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2718 - val_loss: 0.3612\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2712 - val_loss: 0.3612\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2705 - val_loss: 0.3611\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2699 - val_loss: 0.3611\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2693 - val_loss: 0.3610\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2687 - val_loss: 0.3610\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2681 - val_loss: 0.3610\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2675 - val_loss: 0.3609\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2669 - val_loss: 0.3609\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2663 - val_loss: 0.3608\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2657 - val_loss: 0.3608\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 679.6913673037432, my average MASE = 68962615.13093176\n",
      "Cluster 9, 679.6913673037432\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "clusters_labels.shape=(408081,)\n",
      "N_clusters=2, 2, 11, (10056, 67)\n",
      "Before prediction: train_X.shape=(6027, 10, 67), train_y.shape=(6027, 67), test_X.shape=(2009, 10, 67), test_y.shape=(2009, 67)\n",
      "Epoch 1/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0641 - val_loss: 0.0494\n",
      "Epoch 2/40\n",
      "95/95 [==============================] - 3s 32ms/step - loss: 0.0590 - val_loss: 0.0459\n",
      "Epoch 3/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0558 - val_loss: 0.0432\n",
      "Epoch 4/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0535 - val_loss: 0.0410\n",
      "Epoch 5/40\n",
      "95/95 [==============================] - 3s 33ms/step - loss: 0.0516 - val_loss: 0.0393\n",
      "Epoch 6/40\n",
      "95/95 [==============================] - 3s 32ms/step - loss: 0.0501 - val_loss: 0.0378\n",
      "Epoch 7/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0489 - val_loss: 0.0366\n",
      "Epoch 8/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0478 - val_loss: 0.0357\n",
      "Epoch 9/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0470 - val_loss: 0.0348\n",
      "Epoch 10/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0462 - val_loss: 0.0341\n",
      "Epoch 11/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0455 - val_loss: 0.0335\n",
      "Epoch 12/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0449 - val_loss: 0.0330\n",
      "Epoch 13/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0443 - val_loss: 0.0325\n",
      "Epoch 14/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0439 - val_loss: 0.0321\n",
      "Epoch 15/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0434 - val_loss: 0.0317\n",
      "Epoch 16/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0430 - val_loss: 0.0314\n",
      "Epoch 17/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0426 - val_loss: 0.0311\n",
      "Epoch 18/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0423 - val_loss: 0.0308\n",
      "Epoch 19/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0419 - val_loss: 0.0305\n",
      "Epoch 20/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0416 - val_loss: 0.0303\n",
      "Epoch 21/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0414 - val_loss: 0.0300\n",
      "Epoch 22/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0411 - val_loss: 0.0298\n",
      "Epoch 23/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0409 - val_loss: 0.0296\n",
      "Epoch 24/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0406 - val_loss: 0.0294\n",
      "Epoch 25/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0404 - val_loss: 0.0293\n",
      "Epoch 26/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0403 - val_loss: 0.0291\n",
      "Epoch 27/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0401 - val_loss: 0.0290\n",
      "Epoch 28/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0399 - val_loss: 0.0289\n",
      "Epoch 29/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0397 - val_loss: 0.0287\n",
      "Epoch 30/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0396 - val_loss: 0.0286\n",
      "Epoch 31/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0394 - val_loss: 0.0286\n",
      "Epoch 32/40\n",
      "95/95 [==============================] - 3s 31ms/step - loss: 0.0393 - val_loss: 0.0285\n",
      "Epoch 33/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0392 - val_loss: 0.0284\n",
      "Epoch 34/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0391 - val_loss: 0.0283\n",
      "Epoch 35/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0389 - val_loss: 0.0282\n",
      "Epoch 36/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0388 - val_loss: 0.0282\n",
      "Epoch 37/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0387 - val_loss: 0.0281\n",
      "Epoch 38/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0386 - val_loss: 0.0281\n",
      "Epoch 39/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0385 - val_loss: 0.0280\n",
      "Epoch 40/40\n",
      "95/95 [==============================] - 3s 30ms/step - loss: 0.0384 - val_loss: 0.0280\n",
      "63/63 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(2009, 67), test_y.shape=(2009, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 137177179.8661396, my average MASE = 15662264218.215044\n",
      "Cluster 0, 137177179.8661396\n",
      "Before prediction: train_X.shape=(18366, 10, 67), train_y.shape=(18366, 67), test_X.shape=(6122, 10, 67), test_y.shape=(6122, 67)\n",
      "Epoch 1/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.3004 - val_loss: 0.3159\n",
      "Epoch 2/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2809 - val_loss: 0.3004\n",
      "Epoch 3/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2655 - val_loss: 0.2878\n",
      "Epoch 4/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2540 - val_loss: 0.2790\n",
      "Epoch 5/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2465 - val_loss: 0.2727\n",
      "Epoch 6/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2409 - val_loss: 0.2676\n",
      "Epoch 7/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2364 - val_loss: 0.2633\n",
      "Epoch 8/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2324 - val_loss: 0.2598\n",
      "Epoch 9/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2290 - val_loss: 0.2567\n",
      "Epoch 10/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2259 - val_loss: 0.2539\n",
      "Epoch 11/40\n",
      "287/287 [==============================] - 8s 30ms/step - loss: 0.2230 - val_loss: 0.2514\n",
      "Epoch 12/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2204 - val_loss: 0.2493\n",
      "Epoch 13/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2181 - val_loss: 0.2474\n",
      "Epoch 14/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.2161 - val_loss: 0.2457\n",
      "Epoch 15/40\n",
      "287/287 [==============================] - 8s 30ms/step - loss: 0.2144 - val_loss: 0.2442\n",
      "Epoch 16/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2129 - val_loss: 0.2429\n",
      "Epoch 17/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2115 - val_loss: 0.2418\n",
      "Epoch 18/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2103 - val_loss: 0.2406\n",
      "Epoch 19/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2092 - val_loss: 0.2396\n",
      "Epoch 20/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2083 - val_loss: 0.2386\n",
      "Epoch 21/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2073 - val_loss: 0.2378\n",
      "Epoch 22/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2065 - val_loss: 0.2370\n",
      "Epoch 23/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2057 - val_loss: 0.2363\n",
      "Epoch 24/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2050 - val_loss: 0.2356\n",
      "Epoch 25/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2043 - val_loss: 0.2350\n",
      "Epoch 26/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2037 - val_loss: 0.2344\n",
      "Epoch 27/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2031 - val_loss: 0.2338\n",
      "Epoch 28/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2025 - val_loss: 0.2333\n",
      "Epoch 29/40\n",
      "287/287 [==============================] - 9s 32ms/step - loss: 0.2020 - val_loss: 0.2328\n",
      "Epoch 30/40\n",
      "287/287 [==============================] - 8s 29ms/step - loss: 0.2015 - val_loss: 0.2324\n",
      "Epoch 31/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2010 - val_loss: 0.2319\n",
      "Epoch 32/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.2006 - val_loss: 0.2314\n",
      "Epoch 33/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.2002 - val_loss: 0.2311\n",
      "Epoch 34/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.1998 - val_loss: 0.2307\n",
      "Epoch 35/40\n",
      "287/287 [==============================] - 9s 33ms/step - loss: 0.1994 - val_loss: 0.2304\n",
      "Epoch 36/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.1991 - val_loss: 0.2301\n",
      "Epoch 37/40\n",
      "287/287 [==============================] - 8s 30ms/step - loss: 0.1987 - val_loss: 0.2298\n",
      "Epoch 38/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.1984 - val_loss: 0.2294\n",
      "Epoch 39/40\n",
      "287/287 [==============================] - 9s 31ms/step - loss: 0.1981 - val_loss: 0.2291\n",
      "Epoch 40/40\n",
      "287/287 [==============================] - 9s 30ms/step - loss: 0.1978 - val_loss: 0.2290\n",
      "192/192 [==============================] - 2s 10ms/step\n",
      "predicted_original.shape=(6122, 67), test_y.shape=(6122, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1800.7465590842255, my average MASE = 3633.615501921918\n",
      "Cluster 1, 1800.7465590842255\n",
      "clusters_labels.shape=(408081,)\n",
      "N_clusters=5, 5, 58, (3715, 67)\n",
      "Before prediction: train_X.shape=(2222, 10, 67), train_y.shape=(2222, 67), test_X.shape=(741, 10, 67), test_y.shape=(741, 67)\n",
      "Epoch 1/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.5001 - val_loss: 0.3691\n",
      "Epoch 2/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4918 - val_loss: 0.3651\n",
      "Epoch 3/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4851 - val_loss: 0.3618\n",
      "Epoch 4/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4795 - val_loss: 0.3589\n",
      "Epoch 5/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4744 - val_loss: 0.3563\n",
      "Epoch 6/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4699 - val_loss: 0.3539\n",
      "Epoch 7/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4657 - val_loss: 0.3517\n",
      "Epoch 8/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4618 - val_loss: 0.3497\n",
      "Epoch 9/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4582 - val_loss: 0.3478\n",
      "Epoch 10/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4548 - val_loss: 0.3460\n",
      "Epoch 11/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4516 - val_loss: 0.3443\n",
      "Epoch 12/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4485 - val_loss: 0.3427\n",
      "Epoch 13/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4455 - val_loss: 0.3412\n",
      "Epoch 14/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4426 - val_loss: 0.3398\n",
      "Epoch 15/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4398 - val_loss: 0.3384\n",
      "Epoch 16/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4370 - val_loss: 0.3370\n",
      "Epoch 17/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4342 - val_loss: 0.3357\n",
      "Epoch 18/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.4315 - val_loss: 0.3345\n",
      "Epoch 19/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4289 - val_loss: 0.3332\n",
      "Epoch 20/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4263 - val_loss: 0.3321\n",
      "Epoch 21/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4237 - val_loss: 0.3309\n",
      "Epoch 22/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4213 - val_loss: 0.3298\n",
      "Epoch 23/40\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4189 - val_loss: 0.3287\n",
      "Epoch 24/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4166 - val_loss: 0.3277\n",
      "Epoch 25/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4143 - val_loss: 0.3266\n",
      "Epoch 26/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4120 - val_loss: 0.3256\n",
      "Epoch 27/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4099 - val_loss: 0.3247\n",
      "Epoch 28/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4079 - val_loss: 0.3238\n",
      "Epoch 29/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4059 - val_loss: 0.3230\n",
      "Epoch 30/40\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.4040 - val_loss: 0.3222\n",
      "Epoch 31/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4022 - val_loss: 0.3214\n",
      "Epoch 32/40\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4005 - val_loss: 0.3206\n",
      "Epoch 33/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.3988 - val_loss: 0.3199\n",
      "Epoch 34/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.3972 - val_loss: 0.3193\n",
      "Epoch 35/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3956 - val_loss: 0.3186\n",
      "Epoch 36/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.3941 - val_loss: 0.3180\n",
      "Epoch 37/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3927 - val_loss: 0.3175\n",
      "Epoch 38/40\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3912 - val_loss: 0.3169\n",
      "Epoch 39/40\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.3899 - val_loss: 0.3163\n",
      "Epoch 40/40\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.3885 - val_loss: 0.3158\n",
      "24/24 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(741, 67), test_y.shape=(741, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 461.9721639366225, my average MASE = 1222.9207656733142\n",
      "Cluster 0, 461.9721639366225\n",
      "Before prediction: train_X.shape=(1948, 10, 67), train_y.shape=(1948, 67), test_X.shape=(649, 10, 67), test_y.shape=(649, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1169 - val_loss: 0.0965\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1118 - val_loss: 0.0947\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1077 - val_loss: 0.0935\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1043 - val_loss: 0.0926\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1013 - val_loss: 0.0919\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0987 - val_loss: 0.0913\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0964 - val_loss: 0.0909\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0942 - val_loss: 0.0906\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0923 - val_loss: 0.0904\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0906 - val_loss: 0.0901\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0890 - val_loss: 0.0899\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0875 - val_loss: 0.0897\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0861 - val_loss: 0.0895\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0849 - val_loss: 0.0894\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0837 - val_loss: 0.0892\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0826 - val_loss: 0.0891\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0816 - val_loss: 0.0889\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0807 - val_loss: 0.0888\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0798 - val_loss: 0.0886\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0790 - val_loss: 0.0885\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0782 - val_loss: 0.0884\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0775 - val_loss: 0.0883\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0768 - val_loss: 0.0881\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0761 - val_loss: 0.0880\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0755 - val_loss: 0.0879\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0749 - val_loss: 0.0878\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0743 - val_loss: 0.0877\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0738 - val_loss: 0.0876\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0732 - val_loss: 0.0875\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0727 - val_loss: 0.0875\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0722 - val_loss: 0.0874\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0718 - val_loss: 0.0874\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0713 - val_loss: 0.0874\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0709 - val_loss: 0.0873\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0705 - val_loss: 0.0873\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0702 - val_loss: 0.0873\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0698 - val_loss: 0.0872\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0695 - val_loss: 0.0872\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0692 - val_loss: 0.0872\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0689 - val_loss: 0.0871\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(649, 67), test_y.shape=(649, 67)\n",
      "average MASE = 979857680.8759673, my average MASE = 20246393999.460716\n",
      "Cluster 1, 979857680.8759673\n",
      "Before prediction: train_X.shape=(160, 10, 67), train_y.shape=(160, 67), test_X.shape=(53, 10, 67), test_y.shape=(53, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5150 - val_loss: 0.4228\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5137 - val_loss: 0.4220\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5125 - val_loss: 0.4212\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5113 - val_loss: 0.4205\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5101 - val_loss: 0.4197\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5090 - val_loss: 0.4189\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5079 - val_loss: 0.4182\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5068 - val_loss: 0.4175\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5058 - val_loss: 0.4168\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5047 - val_loss: 0.4161\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5037 - val_loss: 0.4154\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5026 - val_loss: 0.4148\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5016 - val_loss: 0.4141\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5006 - val_loss: 0.4135\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4997 - val_loss: 0.4128\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4987 - val_loss: 0.4122\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4977 - val_loss: 0.4116\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4968 - val_loss: 0.4110\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4958 - val_loss: 0.4104\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4949 - val_loss: 0.4098\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4940 - val_loss: 0.4092\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4931 - val_loss: 0.4086\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4923 - val_loss: 0.4081\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4914 - val_loss: 0.4075\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4905 - val_loss: 0.4069\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4897 - val_loss: 0.4064\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4888 - val_loss: 0.4059\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4880 - val_loss: 0.4053\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4871 - val_loss: 0.4048\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4863 - val_loss: 0.4043\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4855 - val_loss: 0.4038\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4847 - val_loss: 0.4033\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4840 - val_loss: 0.4028\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4832 - val_loss: 0.4023\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4824 - val_loss: 0.4018\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4817 - val_loss: 0.4013\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4809 - val_loss: 0.4008\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4802 - val_loss: 0.4003\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4795 - val_loss: 0.3998\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4787 - val_loss: 0.3994\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(53, 67), test_y.shape=(53, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 127.13783551729924, my average MASE = 51951538.633395046\n",
      "Cluster 2, 127.13783551729924\n",
      "Before prediction: train_X.shape=(7, 10, 67), train_y.shape=(7, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8383 - val_loss: 0.5938\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8360 - val_loss: 0.5928\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8338 - val_loss: 0.5919\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8316 - val_loss: 0.5910\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8294 - val_loss: 0.5901\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8272 - val_loss: 0.5891\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8251 - val_loss: 0.5882\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8229 - val_loss: 0.5873\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8208 - val_loss: 0.5864\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8187 - val_loss: 0.5855\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8167 - val_loss: 0.5847\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8146 - val_loss: 0.5838\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8126 - val_loss: 0.5830\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8107 - val_loss: 0.5821\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8087 - val_loss: 0.5812\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8068 - val_loss: 0.5803\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8048 - val_loss: 0.5794\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8029 - val_loss: 0.5786\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8009 - val_loss: 0.5777\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7989 - val_loss: 0.5768\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7970 - val_loss: 0.5760\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7951 - val_loss: 0.5751\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7932 - val_loss: 0.5743\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7913 - val_loss: 0.5735\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7894 - val_loss: 0.5727\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7875 - val_loss: 0.5720\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7857 - val_loss: 0.5712\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7839 - val_loss: 0.5704\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7820 - val_loss: 0.5697\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7802 - val_loss: 0.5690\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7784 - val_loss: 0.5682\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7767 - val_loss: 0.5675\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7749 - val_loss: 0.5668\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7732 - val_loss: 0.5661\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7715 - val_loss: 0.5655\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7698 - val_loss: 0.5648\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7681 - val_loss: 0.5641\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7664 - val_loss: 0.5635\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7647 - val_loss: 0.5629\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7630 - val_loss: 0.5623\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1378.9503839591846, my average MASE = 44152799.2879618\n",
      "Cluster 3, 1378.9503839591846\n",
      "Before prediction: train_X.shape=(44, 10, 67), train_y.shape=(44, 67), test_X.shape=(15, 10, 67), test_y.shape=(15, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5170 - val_loss: 0.7315\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5154 - val_loss: 0.7308\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5138 - val_loss: 0.7300\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5122 - val_loss: 0.7293\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5107 - val_loss: 0.7285\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5092 - val_loss: 0.7278\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5077 - val_loss: 0.7271\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5062 - val_loss: 0.7264\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5047 - val_loss: 0.7257\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5033 - val_loss: 0.7250\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5018 - val_loss: 0.7243\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5004 - val_loss: 0.7236\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4990 - val_loss: 0.7229\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4977 - val_loss: 0.7222\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4963 - val_loss: 0.7215\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4949 - val_loss: 0.7209\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4936 - val_loss: 0.7202\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4923 - val_loss: 0.7196\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4910 - val_loss: 0.7189\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4897 - val_loss: 0.7183\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4884 - val_loss: 0.7177\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4872 - val_loss: 0.7170\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4859 - val_loss: 0.7164\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4847 - val_loss: 0.7158\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4835 - val_loss: 0.7152\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4823 - val_loss: 0.7146\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4811 - val_loss: 0.7140\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4799 - val_loss: 0.7135\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4788 - val_loss: 0.7129\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4776 - val_loss: 0.7123\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4765 - val_loss: 0.7118\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4753 - val_loss: 0.7113\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4742 - val_loss: 0.7107\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4731 - val_loss: 0.7102\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4720 - val_loss: 0.7097\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4709 - val_loss: 0.7092\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4698 - val_loss: 0.7087\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4687 - val_loss: 0.7082\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4676 - val_loss: 0.7077\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4666 - val_loss: 0.7073\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(15, 67), test_y.shape=(15, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2846600473.0797715, my average MASE = 8256296780.299102\n",
      "Cluster 4, 2846600473.0797715\n",
      "clusters_labels.shape=(408081,)\n",
      "N_clusters=7, 7, 176, (267, 67)\n",
      "Before prediction: train_X.shape=(154, 10, 67), train_y.shape=(154, 67), test_X.shape=(51, 10, 67), test_y.shape=(51, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3225 - val_loss: 0.2694\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3217 - val_loss: 0.2689\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3209 - val_loss: 0.2684\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3201 - val_loss: 0.2679\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3194 - val_loss: 0.2674\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3187 - val_loss: 0.2669\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3179 - val_loss: 0.2664\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3172 - val_loss: 0.2659\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3165 - val_loss: 0.2655\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3159 - val_loss: 0.2650\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3152 - val_loss: 0.2646\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3145 - val_loss: 0.2641\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3139 - val_loss: 0.2637\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3133 - val_loss: 0.2633\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3127 - val_loss: 0.2629\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3121 - val_loss: 0.2624\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3115 - val_loss: 0.2620\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3109 - val_loss: 0.2616\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3103 - val_loss: 0.2612\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3098 - val_loss: 0.2609\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3092 - val_loss: 0.2605\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3087 - val_loss: 0.2601\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3081 - val_loss: 0.2598\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3076 - val_loss: 0.2594\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3071 - val_loss: 0.2590\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3066 - val_loss: 0.2587\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3061 - val_loss: 0.2584\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3056 - val_loss: 0.2580\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3051 - val_loss: 0.2576\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3046 - val_loss: 0.2573\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3041 - val_loss: 0.2569\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3037 - val_loss: 0.2566\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3032 - val_loss: 0.2563\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3027 - val_loss: 0.2559\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3023 - val_loss: 0.2556\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3018 - val_loss: 0.2553\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3013 - val_loss: 0.2550\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3009 - val_loss: 0.2546\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3005 - val_loss: 0.2543\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3000 - val_loss: 0.2540\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(51, 67), test_y.shape=(51, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 173.3029589389659, my average MASE = 89033224.06135572\n",
      "Cluster 0, 173.3029589389659\n",
      "Before prediction: train_X.shape=(5960, 10, 67), train_y.shape=(5960, 67), test_X.shape=(1987, 10, 67), test_y.shape=(1987, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0675 - val_loss: 0.0464\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0617 - val_loss: 0.0432\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0584 - val_loss: 0.0410\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0560 - val_loss: 0.0393\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0541 - val_loss: 0.0377\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0525 - val_loss: 0.0364\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0512 - val_loss: 0.0353\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0501 - val_loss: 0.0343\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0492 - val_loss: 0.0335\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0484 - val_loss: 0.0327\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0477 - val_loss: 0.0321\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0471 - val_loss: 0.0315\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0466 - val_loss: 0.0310\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0461 - val_loss: 0.0306\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0457 - val_loss: 0.0302\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 34ms/step - loss: 0.0453 - val_loss: 0.0299\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0449 - val_loss: 0.0296\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0445 - val_loss: 0.0293\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0442 - val_loss: 0.0291\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0439 - val_loss: 0.0288\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0437 - val_loss: 0.0286\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0434 - val_loss: 0.0284\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0432 - val_loss: 0.0282\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0430 - val_loss: 0.0280\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0428 - val_loss: 0.0278\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0426 - val_loss: 0.0277\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0424 - val_loss: 0.0275\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0423 - val_loss: 0.0274\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0421 - val_loss: 0.0272\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0420 - val_loss: 0.0271\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0419 - val_loss: 0.0270\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0417 - val_loss: 0.0269\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0416 - val_loss: 0.0268\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0415 - val_loss: 0.0266\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0414 - val_loss: 0.0265\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0413 - val_loss: 0.0265\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0412 - val_loss: 0.0264\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0411 - val_loss: 0.0263\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0410 - val_loss: 0.0262\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0409 - val_loss: 0.0261\n",
      "63/63 [==============================] - 1s 10ms/step\n",
      "predicted_original.shape=(1987, 67), test_y.shape=(1987, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 783030752.0753396, my average MASE = 36192993113.38622\n",
      "Cluster 1, 783030752.0753396\n",
      "Before prediction: train_X.shape=(6, 10, 67), train_y.shape=(6, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5458 - val_loss: 0.3458\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5436 - val_loss: 0.3449\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5413 - val_loss: 0.3440\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5391 - val_loss: 0.3431\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5369 - val_loss: 0.3422\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5348 - val_loss: 0.3413\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5326 - val_loss: 0.3405\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5304 - val_loss: 0.3396\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5283 - val_loss: 0.3387\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5261 - val_loss: 0.3379\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5240 - val_loss: 0.3371\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5219 - val_loss: 0.3362\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5198 - val_loss: 0.3354\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5177 - val_loss: 0.3346\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5157 - val_loss: 0.3338\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5137 - val_loss: 0.3330\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5117 - val_loss: 0.3322\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5097 - val_loss: 0.3314\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5078 - val_loss: 0.3308\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5059 - val_loss: 0.3301\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5041 - val_loss: 0.3295\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5022 - val_loss: 0.3290\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5004 - val_loss: 0.3284\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4986 - val_loss: 0.3279\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4968 - val_loss: 0.3274\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4951 - val_loss: 0.3269\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4933 - val_loss: 0.3264\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4917 - val_loss: 0.3260\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4900 - val_loss: 0.3257\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4884 - val_loss: 0.3254\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4868 - val_loss: 0.3251\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4852 - val_loss: 0.3249\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4836 - val_loss: 0.3247\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4821 - val_loss: 0.3245\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4805 - val_loss: 0.3244\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4790 - val_loss: 0.3242\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4775 - val_loss: 0.3241\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4760 - val_loss: 0.3240\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4745 - val_loss: 0.3239\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4730 - val_loss: 0.3238\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 300.2247881778862, my average MASE = 45822553.10580316\n",
      "Cluster 2, 300.2247881778862\n",
      "Before prediction: train_X.shape=(85, 10, 67), train_y.shape=(85, 67), test_X.shape=(28, 10, 67), test_y.shape=(28, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3796 - val_loss: 0.4594\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3789 - val_loss: 0.4589\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3782 - val_loss: 0.4585\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3776 - val_loss: 0.4581\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3770 - val_loss: 0.4577\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3764 - val_loss: 0.4573\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3759 - val_loss: 0.4569\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3753 - val_loss: 0.4565\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3748 - val_loss: 0.4561\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3743 - val_loss: 0.4558\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3737 - val_loss: 0.4554\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3732 - val_loss: 0.4550\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3727 - val_loss: 0.4547\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3722 - val_loss: 0.4543\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3717 - val_loss: 0.4540\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3713 - val_loss: 0.4537\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3708 - val_loss: 0.4533\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3703 - val_loss: 0.4530\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3699 - val_loss: 0.4527\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3694 - val_loss: 0.4524\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3690 - val_loss: 0.4521\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3685 - val_loss: 0.4517\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3681 - val_loss: 0.4514\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3677 - val_loss: 0.4512\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3673 - val_loss: 0.4509\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3669 - val_loss: 0.4506\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3664 - val_loss: 0.4503\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3661 - val_loss: 0.4501\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3657 - val_loss: 0.4498\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3653 - val_loss: 0.4495\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3649 - val_loss: 0.4493\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3645 - val_loss: 0.4490\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3641 - val_loss: 0.4487\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3638 - val_loss: 0.4485\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3634 - val_loss: 0.4482\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3630 - val_loss: 0.4480\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3627 - val_loss: 0.4478\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3623 - val_loss: 0.4475\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3619 - val_loss: 0.4473\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3616 - val_loss: 0.4470\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(28, 67), test_y.shape=(28, 67)\n",
      "average MASE = 329.8443195642471, my average MASE = 91063731.05477783\n",
      "Cluster 3, 329.8443195642471\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6262 - val_loss: 0.5214\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6239 - val_loss: 0.5202\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6216 - val_loss: 0.5190\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6193 - val_loss: 0.5179\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6170 - val_loss: 0.5167\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6148 - val_loss: 0.5156\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6125 - val_loss: 0.5145\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6102 - val_loss: 0.5134\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6080 - val_loss: 0.5123\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6058 - val_loss: 0.5112\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6036 - val_loss: 0.5101\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6015 - val_loss: 0.5090\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5994 - val_loss: 0.5080\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5973 - val_loss: 0.5070\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5953 - val_loss: 0.5060\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5932 - val_loss: 0.5051\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5912 - val_loss: 0.5041\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5892 - val_loss: 0.5032\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5872 - val_loss: 0.5023\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5853 - val_loss: 0.5015\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5834 - val_loss: 0.5006\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5816 - val_loss: 0.4999\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5799 - val_loss: 0.4991\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5782 - val_loss: 0.4984\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5765 - val_loss: 0.4977\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5749 - val_loss: 0.4970\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5732 - val_loss: 0.4963\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5716 - val_loss: 0.4957\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5699 - val_loss: 0.4951\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5683 - val_loss: 0.4946\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5667 - val_loss: 0.4941\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5651 - val_loss: 0.4936\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5635 - val_loss: 0.4931\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5619 - val_loss: 0.4927\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5604 - val_loss: 0.4922\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5588 - val_loss: 0.4917\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5573 - val_loss: 0.4913\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5557 - val_loss: 0.4909\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5542 - val_loss: 0.4906\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5526 - val_loss: 0.4902\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.32309710873507563, my average MASE = 0.5656513264705396\n",
      "Cluster 4, 0.32309710873507563\n",
      "Before prediction: train_X.shape=(43, 10, 67), train_y.shape=(43, 67), test_X.shape=(14, 10, 67), test_y.shape=(14, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5119 - val_loss: 0.6850\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5101 - val_loss: 0.6839\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5083 - val_loss: 0.6829\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5066 - val_loss: 0.6819\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5048 - val_loss: 0.6808\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5031 - val_loss: 0.6798\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5014 - val_loss: 0.6788\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4997 - val_loss: 0.6778\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4980 - val_loss: 0.6769\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4964 - val_loss: 0.6759\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4948 - val_loss: 0.6749\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4931 - val_loss: 0.6740\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4915 - val_loss: 0.6730\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4900 - val_loss: 0.6721\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4884 - val_loss: 0.6711\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4869 - val_loss: 0.6702\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4853 - val_loss: 0.6693\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4838 - val_loss: 0.6683\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4823 - val_loss: 0.6674\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4808 - val_loss: 0.6665\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4794 - val_loss: 0.6656\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4779 - val_loss: 0.6647\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4765 - val_loss: 0.6639\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4751 - val_loss: 0.6630\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4737 - val_loss: 0.6622\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4723 - val_loss: 0.6613\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4710 - val_loss: 0.6605\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4697 - val_loss: 0.6596\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4683 - val_loss: 0.6588\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4670 - val_loss: 0.6580\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4657 - val_loss: 0.6572\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4644 - val_loss: 0.6564\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4631 - val_loss: 0.6556\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4619 - val_loss: 0.6548\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4606 - val_loss: 0.6540\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4594 - val_loss: 0.6532\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4581 - val_loss: 0.6525\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4569 - val_loss: 0.6517\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4557 - val_loss: 0.6510\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4545 - val_loss: 0.6502\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(14, 67), test_y.shape=(14, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 2340337256.388025, my average MASE = 7806499481.251814\n",
      "Cluster 5, 2340337256.388025\n",
      "Before prediction: train_X.shape=(181, 10, 67), train_y.shape=(181, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6846 - val_loss: 0.5744\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6833 - val_loss: 0.5737\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6820 - val_loss: 0.5730\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6807 - val_loss: 0.5723\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6795 - val_loss: 0.5716\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6783 - val_loss: 0.5709\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6771 - val_loss: 0.5703\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6760 - val_loss: 0.5696\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6749 - val_loss: 0.5690\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6737 - val_loss: 0.5683\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6727 - val_loss: 0.5677\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6716 - val_loss: 0.5671\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6705 - val_loss: 0.5666\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6695 - val_loss: 0.5660\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6685 - val_loss: 0.5654\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6675 - val_loss: 0.5648\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6664 - val_loss: 0.5643\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6654 - val_loss: 0.5637\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6645 - val_loss: 0.5632\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6635 - val_loss: 0.5626\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6626 - val_loss: 0.5621\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.6616 - val_loss: 0.5615\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6607 - val_loss: 0.5610\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6597 - val_loss: 0.5605\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6588 - val_loss: 0.5600\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6579 - val_loss: 0.5594\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6570 - val_loss: 0.5589\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6561 - val_loss: 0.5584\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6553 - val_loss: 0.5579\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6544 - val_loss: 0.5574\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6536 - val_loss: 0.5569\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6527 - val_loss: 0.5563\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6519 - val_loss: 0.5558\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6510 - val_loss: 0.5553\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6502 - val_loss: 0.5548\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6493 - val_loss: 0.5543\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6485 - val_loss: 0.5538\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.6477 - val_loss: 0.5533\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6468 - val_loss: 0.5528\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.6460 - val_loss: 0.5523\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 153.88341287865387, my average MASE = 335532064.75560737\n",
      "Cluster 6, 153.88341287865387\n",
      "clusters_labels.shape=(408081,)\n",
      "N_clusters=9, 9, 3, (61, 67)\n",
      "Before prediction: train_X.shape=(30, 10, 67), train_y.shape=(30, 67), test_X.shape=(10, 10, 67), test_y.shape=(10, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5800 - val_loss: 0.4525\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5784 - val_loss: 0.4515\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5768 - val_loss: 0.4506\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5753 - val_loss: 0.4496\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5738 - val_loss: 0.4487\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5722 - val_loss: 0.4477\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5707 - val_loss: 0.4468\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5692 - val_loss: 0.4459\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5678 - val_loss: 0.4450\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5663 - val_loss: 0.4441\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5648 - val_loss: 0.4433\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5634 - val_loss: 0.4424\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5619 - val_loss: 0.4415\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5605 - val_loss: 0.4407\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5590 - val_loss: 0.4398\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5576 - val_loss: 0.4390\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5562 - val_loss: 0.4382\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5548 - val_loss: 0.4374\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5534 - val_loss: 0.4367\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5520 - val_loss: 0.4359\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5507 - val_loss: 0.4352\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5493 - val_loss: 0.4344\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5480 - val_loss: 0.4337\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5467 - val_loss: 0.4329\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5454 - val_loss: 0.4322\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5441 - val_loss: 0.4315\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5428 - val_loss: 0.4308\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5415 - val_loss: 0.4301\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5403 - val_loss: 0.4294\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5390 - val_loss: 0.4287\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5378 - val_loss: 0.4280\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5366 - val_loss: 0.4273\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5354 - val_loss: 0.4266\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5342 - val_loss: 0.4260\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5330 - val_loss: 0.4253\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5318 - val_loss: 0.4246\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5307 - val_loss: 0.4240\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5295 - val_loss: 0.4234\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5284 - val_loss: 0.4227\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5273 - val_loss: 0.4221\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_original.shape=(10, 67), test_y.shape=(10, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 35650627.66244785, my average MASE = 4899168813.139491\n",
      "Cluster 0, 35650627.66244785\n",
      "Before prediction: train_X.shape=(179, 10, 67), train_y.shape=(179, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.7142 - val_loss: 0.5624\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7129 - val_loss: 0.5618\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7116 - val_loss: 0.5612\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7104 - val_loss: 0.5605\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7092 - val_loss: 0.5599\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7080 - val_loss: 0.5593\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7069 - val_loss: 0.5588\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7057 - val_loss: 0.5582\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7046 - val_loss: 0.5576\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7035 - val_loss: 0.5571\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7024 - val_loss: 0.5565\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7013 - val_loss: 0.5560\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7002 - val_loss: 0.5555\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6993 - val_loss: 0.5550\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6982 - val_loss: 0.5545\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6972 - val_loss: 0.5540\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6962 - val_loss: 0.5535\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6952 - val_loss: 0.5530\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6942 - val_loss: 0.5525\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6932 - val_loss: 0.5520\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6922 - val_loss: 0.5516\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6913 - val_loss: 0.5511\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6903 - val_loss: 0.5506\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6894 - val_loss: 0.5502\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6885 - val_loss: 0.5498\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6875 - val_loss: 0.5493\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6866 - val_loss: 0.5489\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6857 - val_loss: 0.5485\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6848 - val_loss: 0.5481\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6839 - val_loss: 0.5477\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6830 - val_loss: 0.5472\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6821 - val_loss: 0.5468\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6812 - val_loss: 0.5464\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6803 - val_loss: 0.5460\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6794 - val_loss: 0.5456\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6786 - val_loss: 0.5452\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6777 - val_loss: 0.5449\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6768 - val_loss: 0.5445\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6759 - val_loss: 0.5441\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6751 - val_loss: 0.5437\n",
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n",
      "average MASE = 167.81473432422789, my average MASE = 220460085.3598021\n",
      "Cluster 1, 167.81473432422789\n",
      "Before prediction: train_X.shape=(15, 10, 67), train_y.shape=(15, 67), test_X.shape=(5, 10, 67), test_y.shape=(5, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4267 - val_loss: 0.6677\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4246 - val_loss: 0.6662\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4226 - val_loss: 0.6647\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4205 - val_loss: 0.6632\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4185 - val_loss: 0.6618\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4166 - val_loss: 0.6605\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4146 - val_loss: 0.6592\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4127 - val_loss: 0.6578\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4107 - val_loss: 0.6565\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4088 - val_loss: 0.6552\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4070 - val_loss: 0.6539\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4051 - val_loss: 0.6527\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4033 - val_loss: 0.6514\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4014 - val_loss: 0.6502\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3997 - val_loss: 0.6490\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3979 - val_loss: 0.6478\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3961 - val_loss: 0.6466\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3944 - val_loss: 0.6455\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3927 - val_loss: 0.6444\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3910 - val_loss: 0.6433\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3893 - val_loss: 0.6423\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3877 - val_loss: 0.6412\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3861 - val_loss: 0.6402\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3845 - val_loss: 0.6392\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3829 - val_loss: 0.6383\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3814 - val_loss: 0.6374\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3799 - val_loss: 0.6364\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3784 - val_loss: 0.6356\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3769 - val_loss: 0.6347\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3754 - val_loss: 0.6338\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3740 - val_loss: 0.6330\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3725 - val_loss: 0.6322\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3711 - val_loss: 0.6314\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3697 - val_loss: 0.6305\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3683 - val_loss: 0.6297\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3669 - val_loss: 0.6289\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3656 - val_loss: 0.6281\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3642 - val_loss: 0.6273\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3629 - val_loss: 0.6264\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3616 - val_loss: 0.6256\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(5, 67), test_y.shape=(5, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1967377505.4498963, my average MASE = 5262000054.755049\n",
      "Cluster 2, 1967377505.4498963\n",
      "Before prediction: train_X.shape=(3, 10, 67), train_y.shape=(3, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6328 - val_loss: 0.7373\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6300 - val_loss: 0.7359\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6272 - val_loss: 0.7344\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6245 - val_loss: 0.7330\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6218 - val_loss: 0.7316\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6191 - val_loss: 0.7301\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6164 - val_loss: 0.7287\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6137 - val_loss: 0.7273\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6111 - val_loss: 0.7259\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6085 - val_loss: 0.7245\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6058 - val_loss: 0.7230\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6033 - val_loss: 0.7216\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6009 - val_loss: 0.7202\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5985 - val_loss: 0.7188\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5961 - val_loss: 0.7174\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5938 - val_loss: 0.7159\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5915 - val_loss: 0.7146\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5892 - val_loss: 0.7133\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5869 - val_loss: 0.7119\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5847 - val_loss: 0.7106\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5825 - val_loss: 0.7092\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5803 - val_loss: 0.7078\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5781 - val_loss: 0.7064\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5759 - val_loss: 0.7050\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5738 - val_loss: 0.7036\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5716 - val_loss: 0.7022\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5695 - val_loss: 0.7009\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5674 - val_loss: 0.6996\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5653 - val_loss: 0.6983\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5632 - val_loss: 0.6971\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5611 - val_loss: 0.6959\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5590 - val_loss: 0.6946\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5569 - val_loss: 0.6934\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5549 - val_loss: 0.6922\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5528 - val_loss: 0.6910\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5509 - val_loss: 0.6898\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5490 - val_loss: 0.6886\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5472 - val_loss: 0.6874\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5454 - val_loss: 0.6862\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5436 - val_loss: 0.6850\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.27232041656941225, my average MASE = 0.501167640234202\n",
      "Cluster 3, 0.27232041656941225\n",
      "Before prediction: train_X.shape=(7, 10, 67), train_y.shape=(7, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.4602 - val_loss: 0.3734\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4580 - val_loss: 0.3723\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4558 - val_loss: 0.3714\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4536 - val_loss: 0.3704\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4515 - val_loss: 0.3694\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4494 - val_loss: 0.3685\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4474 - val_loss: 0.3675\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4454 - val_loss: 0.3666\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4435 - val_loss: 0.3657\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4416 - val_loss: 0.3648\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4397 - val_loss: 0.3640\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4379 - val_loss: 0.3631\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4361 - val_loss: 0.3623\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4344 - val_loss: 0.3616\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4328 - val_loss: 0.3608\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4312 - val_loss: 0.3601\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4296 - val_loss: 0.3594\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4279 - val_loss: 0.3587\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4263 - val_loss: 0.3581\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4247 - val_loss: 0.3575\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4231 - val_loss: 0.3569\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4216 - val_loss: 0.3564\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4200 - val_loss: 0.3558\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4184 - val_loss: 0.3553\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4169 - val_loss: 0.3547\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4154 - val_loss: 0.3542\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4138 - val_loss: 0.3537\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4123 - val_loss: 0.3533\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4109 - val_loss: 0.3528\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4094 - val_loss: 0.3524\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4080 - val_loss: 0.3520\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4066 - val_loss: 0.3516\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4053 - val_loss: 0.3512\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4039 - val_loss: 0.3508\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4026 - val_loss: 0.3505\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4013 - val_loss: 0.3502\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4000 - val_loss: 0.3498\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3987 - val_loss: 0.3495\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3974 - val_loss: 0.3492\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3961 - val_loss: 0.3490\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 363.8540679781247, my average MASE = 17706113.36217601\n",
      "Cluster 4, 363.8540679781247\n",
      "Before prediction: train_X.shape=(88, 10, 67), train_y.shape=(88, 67), test_X.shape=(29, 10, 67), test_y.shape=(29, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4172 - val_loss: 0.5128\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4165 - val_loss: 0.5124\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4158 - val_loss: 0.5121\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4151 - val_loss: 0.5117\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4145 - val_loss: 0.5113\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4139 - val_loss: 0.5110\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4132 - val_loss: 0.5107\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4126 - val_loss: 0.5103\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4120 - val_loss: 0.5100\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4114 - val_loss: 0.5096\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4108 - val_loss: 0.5093\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4102 - val_loss: 0.5090\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4097 - val_loss: 0.5086\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091 - val_loss: 0.5083\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4085 - val_loss: 0.5080\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4080 - val_loss: 0.5077\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4074 - val_loss: 0.5074\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4069 - val_loss: 0.5071\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4064 - val_loss: 0.5068\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4059 - val_loss: 0.5065\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4053 - val_loss: 0.5062\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4048 - val_loss: 0.5059\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4043 - val_loss: 0.5056\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4038 - val_loss: 0.5053\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4034 - val_loss: 0.5051\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4029 - val_loss: 0.5048\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4024 - val_loss: 0.5045\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4019 - val_loss: 0.5042\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4014 - val_loss: 0.5039\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4010 - val_loss: 0.5036\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4005 - val_loss: 0.5033\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4000 - val_loss: 0.5030\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3996 - val_loss: 0.5028\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3991 - val_loss: 0.5025\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3987 - val_loss: 0.5022\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3983 - val_loss: 0.5019\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3978 - val_loss: 0.5016\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3974 - val_loss: 0.5013\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3969 - val_loss: 0.5011\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3965 - val_loss: 0.5008\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(29, 67), test_y.shape=(29, 67)\n",
      "average MASE = 210.04101539413423, my average MASE = 60624707.44207928\n",
      "Cluster 5, 210.04101539413423\n",
      "Before prediction: train_X.shape=(104, 10, 67), train_y.shape=(104, 67), test_X.shape=(35, 10, 67), test_y.shape=(35, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3090 - val_loss: 0.2755\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3084 - val_loss: 0.2752\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3078 - val_loss: 0.2749\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3072 - val_loss: 0.2745\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3066 - val_loss: 0.2742\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3061 - val_loss: 0.2739\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3055 - val_loss: 0.2736\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3049 - val_loss: 0.2733\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3044 - val_loss: 0.2730\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3039 - val_loss: 0.2727\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3033 - val_loss: 0.2724\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3028 - val_loss: 0.2722\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3023 - val_loss: 0.2719\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3018 - val_loss: 0.2716\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3013 - val_loss: 0.2713\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3008 - val_loss: 0.2711\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3003 - val_loss: 0.2708\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2998 - val_loss: 0.2705\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2994 - val_loss: 0.2703\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.2989 - val_loss: 0.2700\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2984 - val_loss: 0.2698\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.2980 - val_loss: 0.2695\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2975 - val_loss: 0.2693\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2971 - val_loss: 0.2690\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2966 - val_loss: 0.2688\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2962 - val_loss: 0.2686\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2958 - val_loss: 0.2683\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2953 - val_loss: 0.2681\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.2949 - val_loss: 0.2678\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2945 - val_loss: 0.2676\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2941 - val_loss: 0.2674\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2937 - val_loss: 0.2671\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2933 - val_loss: 0.2669\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.2929 - val_loss: 0.2667\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2925 - val_loss: 0.2664\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2921 - val_loss: 0.2662\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2917 - val_loss: 0.2660\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2913 - val_loss: 0.2658\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2909 - val_loss: 0.2656\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.2906 - val_loss: 0.2654\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(35, 67), test_y.shape=(35, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 554.2347835476104, my average MASE = 43885073.200175166\n",
      "Cluster 6, 554.2347835476104\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6507 - val_loss: 0.5207\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6481 - val_loss: 0.5200\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6456 - val_loss: 0.5192\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6431 - val_loss: 0.5184\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6407 - val_loss: 0.5176\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6383 - val_loss: 0.5169\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6360 - val_loss: 0.5161\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6338 - val_loss: 0.5154\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6316 - val_loss: 0.5147\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6293 - val_loss: 0.5140\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6271 - val_loss: 0.5133\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6249 - val_loss: 0.5126\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6227 - val_loss: 0.5119\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6206 - val_loss: 0.5112\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6185 - val_loss: 0.5105\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6164 - val_loss: 0.5099\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6145 - val_loss: 0.5093\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6125 - val_loss: 0.5087\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6107 - val_loss: 0.5082\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6088 - val_loss: 0.5076\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6069 - val_loss: 0.5071\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6051 - val_loss: 0.5067\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6032 - val_loss: 0.5062\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6014 - val_loss: 0.5058\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5996 - val_loss: 0.5053\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5978 - val_loss: 0.5049\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5959 - val_loss: 0.5045\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5941 - val_loss: 0.5040\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5923 - val_loss: 0.5036\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5905 - val_loss: 0.5033\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5888 - val_loss: 0.5030\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5871 - val_loss: 0.5027\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5853 - val_loss: 0.5024\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5837 - val_loss: 0.5021\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5822 - val_loss: 0.5017\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5806 - val_loss: 0.5014\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5791 - val_loss: 0.5011\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5775 - val_loss: 0.5009\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5760 - val_loss: 0.5006\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5745 - val_loss: 0.5004\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.2390672605525707, my average MASE = 0.4689912396588464\n",
      "Cluster 7, 0.2390672605525707\n",
      "Before prediction: train_X.shape=(1948, 10, 67), train_y.shape=(1948, 67), test_X.shape=(649, 10, 67), test_y.shape=(649, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1191 - val_loss: 0.0992\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1138 - val_loss: 0.0972\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1096 - val_loss: 0.0959\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.1060 - val_loss: 0.0949\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1029 - val_loss: 0.0942\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1002 - val_loss: 0.0935\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0977 - val_loss: 0.0930\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0955 - val_loss: 0.0925\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0935 - val_loss: 0.0920\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0916 - val_loss: 0.0917\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0899 - val_loss: 0.0913\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0884 - val_loss: 0.0909\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0870 - val_loss: 0.0906\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0856 - val_loss: 0.0903\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0844 - val_loss: 0.0900\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0833 - val_loss: 0.0898\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0822 - val_loss: 0.0896\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0812 - val_loss: 0.0894\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0803 - val_loss: 0.0892\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0794 - val_loss: 0.0890\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0786 - val_loss: 0.0889\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0778 - val_loss: 0.0887\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0886\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0764 - val_loss: 0.0885\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0757 - val_loss: 0.0884\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0751 - val_loss: 0.0883\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0745 - val_loss: 0.0882\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0739 - val_loss: 0.0881\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0733 - val_loss: 0.0880\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0728 - val_loss: 0.0879\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0723 - val_loss: 0.0879\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0719 - val_loss: 0.0878\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0715 - val_loss: 0.0877\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0711 - val_loss: 0.0876\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0707 - val_loss: 0.0876\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0703 - val_loss: 0.0875\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0700 - val_loss: 0.0874\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0697 - val_loss: 0.0873\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0694 - val_loss: 0.0872\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0692 - val_loss: 0.0872\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(649, 67), test_y.shape=(649, 67)\n",
      "average MASE = 1385618831.00148, my average MASE = 22365614226.60064\n",
      "Cluster 8, 1385618831.00148\n",
      "clusters_labels.shape=(408081,)\n",
      "N_clusters=11, 11, 68, (49, 67)\n",
      "Before prediction: train_X.shape=(23, 10, 67), train_y.shape=(23, 67), test_X.shape=(8, 10, 67), test_y.shape=(8, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5560 - val_loss: 0.4967\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5553 - val_loss: 0.4965\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5546 - val_loss: 0.4963\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5538 - val_loss: 0.4961\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5531 - val_loss: 0.4960\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5524 - val_loss: 0.4958\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5517 - val_loss: 0.4956\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5510 - val_loss: 0.4955\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5503 - val_loss: 0.4953\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5496 - val_loss: 0.4952\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5489 - val_loss: 0.4950\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5482 - val_loss: 0.4949\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5476 - val_loss: 0.4947\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5469 - val_loss: 0.4946\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5463 - val_loss: 0.4945\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5456 - val_loss: 0.4943\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5450 - val_loss: 0.4942\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5443 - val_loss: 0.4940\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5437 - val_loss: 0.4939\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5431 - val_loss: 0.4938\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5425 - val_loss: 0.4936\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5419 - val_loss: 0.4935\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5413 - val_loss: 0.4934\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5407 - val_loss: 0.4933\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5401 - val_loss: 0.4932\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5395 - val_loss: 0.4930\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5389 - val_loss: 0.4929\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5384 - val_loss: 0.4928\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5378 - val_loss: 0.4927\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5372 - val_loss: 0.4926\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5367 - val_loss: 0.4925\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5361 - val_loss: 0.4923\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5356 - val_loss: 0.4922\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5351 - val_loss: 0.4921\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5345 - val_loss: 0.4920\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5340 - val_loss: 0.4919\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5335 - val_loss: 0.4918\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5329 - val_loss: 0.4917\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5324 - val_loss: 0.4915\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5319 - val_loss: 0.4914\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(8, 67), test_y.shape=(8, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 62.874751666083846, my average MASE = 63681940.89448814\n",
      "Cluster 0, 62.874751666083846\n",
      "Before prediction: train_X.shape=(10, 10, 67), train_y.shape=(10, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3794 - val_loss: 0.4534\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3770 - val_loss: 0.4520\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3747 - val_loss: 0.4506\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3724 - val_loss: 0.4494\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3701 - val_loss: 0.4481\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3678 - val_loss: 0.4468\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3656 - val_loss: 0.4455\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3634 - val_loss: 0.4443\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3612 - val_loss: 0.4430\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3591 - val_loss: 0.4418\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3571 - val_loss: 0.4406\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3551 - val_loss: 0.4394\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3531 - val_loss: 0.4382\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3511 - val_loss: 0.4371\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3492 - val_loss: 0.4359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3473 - val_loss: 0.4348\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3454 - val_loss: 0.4337\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3436 - val_loss: 0.4326\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3418 - val_loss: 0.4315\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3400 - val_loss: 0.4305\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3382 - val_loss: 0.4294\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3365 - val_loss: 0.4284\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3349 - val_loss: 0.4273\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3332 - val_loss: 0.4263\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3316 - val_loss: 0.4253\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3300 - val_loss: 0.4243\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3285 - val_loss: 0.4233\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3269 - val_loss: 0.4224\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3254 - val_loss: 0.4214\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3239 - val_loss: 0.4205\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3224 - val_loss: 0.4196\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3209 - val_loss: 0.4186\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3195 - val_loss: 0.4177\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3181 - val_loss: 0.4168\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3166 - val_loss: 0.4159\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3153 - val_loss: 0.4151\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3139 - val_loss: 0.4142\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3125 - val_loss: 0.4134\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3112 - val_loss: 0.4125\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3098 - val_loss: 0.4117\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1188796320.3185039, my average MASE = 2779981174.701244\n",
      "Cluster 1, 1188796320.3185039\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6283 - val_loss: 0.6105\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6261 - val_loss: 0.6091\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6239 - val_loss: 0.6076\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6218 - val_loss: 0.6062\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6196 - val_loss: 0.6047\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6175 - val_loss: 0.6033\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6153 - val_loss: 0.6018\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6132 - val_loss: 0.6004\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6110 - val_loss: 0.5990\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6089 - val_loss: 0.5976\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6067 - val_loss: 0.5961\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6046 - val_loss: 0.5947\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6026 - val_loss: 0.5933\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6005 - val_loss: 0.5919\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5985 - val_loss: 0.5905\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5964 - val_loss: 0.5891\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5944 - val_loss: 0.5877\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5923 - val_loss: 0.5866\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5903 - val_loss: 0.5854\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5882 - val_loss: 0.5842\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5861 - val_loss: 0.5831\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5841 - val_loss: 0.5819\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5820 - val_loss: 0.5808\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5799 - val_loss: 0.5796\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5778 - val_loss: 0.5784\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5758 - val_loss: 0.5773\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5737 - val_loss: 0.5762\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5717 - val_loss: 0.5751\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5697 - val_loss: 0.5740\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5676 - val_loss: 0.5731\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5656 - val_loss: 0.5721\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5636 - val_loss: 0.5712\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5616 - val_loss: 0.5703\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5597 - val_loss: 0.5694\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5577 - val_loss: 0.5686\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5558 - val_loss: 0.5678\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5538 - val_loss: 0.5670\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5519 - val_loss: 0.5663\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5499 - val_loss: 0.5655\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5480 - val_loss: 0.5647\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.32481366416229096, my average MASE = 0.6312112420260654\n",
      "Cluster 2, 0.32481366416229096\n",
      "Before prediction: train_X.shape=(6, 10, 67), train_y.shape=(6, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4564 - val_loss: 0.4796\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4555 - val_loss: 0.4793\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4547 - val_loss: 0.4791\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4538 - val_loss: 0.4788\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4529 - val_loss: 0.4786\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4521 - val_loss: 0.4783\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4512 - val_loss: 0.4781\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4504 - val_loss: 0.4778\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4496 - val_loss: 0.4776\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4487 - val_loss: 0.4773\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4479 - val_loss: 0.4771\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4471 - val_loss: 0.4768\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4463 - val_loss: 0.4766\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4455 - val_loss: 0.4763\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4447 - val_loss: 0.4760\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4439 - val_loss: 0.4758\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4431 - val_loss: 0.4755\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4423 - val_loss: 0.4753\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4415 - val_loss: 0.4750\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4407 - val_loss: 0.4747\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4400 - val_loss: 0.4745\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4392 - val_loss: 0.4742\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4385 - val_loss: 0.4739\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4377 - val_loss: 0.4736\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4370 - val_loss: 0.4733\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4362 - val_loss: 0.4730\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4355 - val_loss: 0.4727\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4348 - val_loss: 0.4724\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4341 - val_loss: 0.4721\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4334 - val_loss: 0.4717\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4327 - val_loss: 0.4714\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4320 - val_loss: 0.4711\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4313 - val_loss: 0.4708\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4307 - val_loss: 0.4705\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4300 - val_loss: 0.4701\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4293 - val_loss: 0.4698\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4286 - val_loss: 0.4694\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4280 - val_loss: 0.4691\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4273 - val_loss: 0.4688\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4266 - val_loss: 0.4684\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 1418783.6078857174, my average MASE = 35875462.94910899\n",
      "Cluster 3, 1418783.6078857174\n",
      "Before prediction: train_X.shape=(27, 10, 67), train_y.shape=(27, 67), test_X.shape=(9, 10, 67), test_y.shape=(9, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3810 - val_loss: 0.3926\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3805 - val_loss: 0.3924\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3799 - val_loss: 0.3923\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3794 - val_loss: 0.3921\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3789 - val_loss: 0.3920\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3784 - val_loss: 0.3918\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3779 - val_loss: 0.3917\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3774 - val_loss: 0.3916\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3769 - val_loss: 0.3914\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3764 - val_loss: 0.3913\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3759 - val_loss: 0.3911\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3754 - val_loss: 0.3910\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3749 - val_loss: 0.3909\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3744 - val_loss: 0.3907\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3739 - val_loss: 0.3906\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3734 - val_loss: 0.3905\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3729 - val_loss: 0.3903\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3725 - val_loss: 0.3902\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3720 - val_loss: 0.3901\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3715 - val_loss: 0.3900\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3711 - val_loss: 0.3899\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3706 - val_loss: 0.3898\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3701 - val_loss: 0.3896\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3697 - val_loss: 0.3895\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3692 - val_loss: 0.3894\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3688 - val_loss: 0.3893\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3683 - val_loss: 0.3892\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3679 - val_loss: 0.3891\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3674 - val_loss: 0.3890\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3670 - val_loss: 0.3889\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3666 - val_loss: 0.3888\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3661 - val_loss: 0.3887\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3657 - val_loss: 0.3886\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3653 - val_loss: 0.3885\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3648 - val_loss: 0.3884\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3644 - val_loss: 0.3883\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3640 - val_loss: 0.3882\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3636 - val_loss: 0.3881\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3632 - val_loss: 0.3880\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3627 - val_loss: 0.3879\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(9, 67), test_y.shape=(9, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 73.99923698083529, my average MASE = 28273926.299095076\n",
      "Cluster 4, 73.99923698083529\n",
      "Before prediction: train_X.shape=(1948, 10, 67), train_y.shape=(1948, 67), test_X.shape=(649, 10, 67), test_y.shape=(649, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1188 - val_loss: 0.0994\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1133 - val_loss: 0.0973\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1090 - val_loss: 0.0957\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1053 - val_loss: 0.0945\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1021 - val_loss: 0.0936\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0993 - val_loss: 0.0929\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0969 - val_loss: 0.0924\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0946 - val_loss: 0.0919\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0909 - val_loss: 0.0911\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0893 - val_loss: 0.0908\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0878 - val_loss: 0.0906\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0864 - val_loss: 0.0903\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0852 - val_loss: 0.0901\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0841 - val_loss: 0.0898\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0830 - val_loss: 0.0897\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0820 - val_loss: 0.0895\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0811 - val_loss: 0.0893\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0802 - val_loss: 0.0892\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0794 - val_loss: 0.0890\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0786 - val_loss: 0.0889\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0779 - val_loss: 0.0888\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0772 - val_loss: 0.0887\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0765 - val_loss: 0.0886\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0759 - val_loss: 0.0885\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0753 - val_loss: 0.0884\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0747 - val_loss: 0.0884\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0742 - val_loss: 0.0883\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0737 - val_loss: 0.0882\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0733 - val_loss: 0.0881\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0728 - val_loss: 0.0881\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0724 - val_loss: 0.0880\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0720 - val_loss: 0.0880\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0716 - val_loss: 0.0879\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0712 - val_loss: 0.0879\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0709 - val_loss: 0.0879\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0705 - val_loss: 0.0878\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0702 - val_loss: 0.0877\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0699 - val_loss: 0.0877\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0696 - val_loss: 0.0877\n",
      "21/21 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(649, 67), test_y.shape=(649, 67)\n",
      "average MASE = 772933595.2163092, my average MASE = 33112652144.926693\n",
      "Cluster 5, 772933595.2163092\n",
      "Before prediction: train_X.shape=(3, 10, 67), train_y.shape=(3, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6742 - val_loss: 0.5426\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6711 - val_loss: 0.5410\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6680 - val_loss: 0.5395\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6649 - val_loss: 0.5381\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6619 - val_loss: 0.5366\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6589 - val_loss: 0.5352\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6560 - val_loss: 0.5338\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6532 - val_loss: 0.5324\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6503 - val_loss: 0.5310\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6475 - val_loss: 0.5296\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6447 - val_loss: 0.5282\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6420 - val_loss: 0.5269\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6393 - val_loss: 0.5255\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6367 - val_loss: 0.5242\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6340 - val_loss: 0.5229\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6314 - val_loss: 0.5216\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6289 - val_loss: 0.5203\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6264 - val_loss: 0.5190\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6240 - val_loss: 0.5178\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6215 - val_loss: 0.5165\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6191 - val_loss: 0.5153\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6168 - val_loss: 0.5141\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6145 - val_loss: 0.5128\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6122 - val_loss: 0.5115\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6098 - val_loss: 0.5102\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6076 - val_loss: 0.5089\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6054 - val_loss: 0.5076\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6033 - val_loss: 0.5062\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6011 - val_loss: 0.5049\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5989 - val_loss: 0.5035\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5968 - val_loss: 0.5022\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5947 - val_loss: 0.5009\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5928 - val_loss: 0.4996\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5908 - val_loss: 0.4982\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5889 - val_loss: 0.4969\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5869 - val_loss: 0.4956\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5850 - val_loss: 0.4944\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5831 - val_loss: 0.4932\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5812 - val_loss: 0.4921\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5794 - val_loss: 0.4911\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.24234404135098125, my average MASE = 0.42071116848846357\n",
      "Cluster 6, 0.24234404135098125\n",
      "Before prediction: train_X.shape=(50, 10, 67), train_y.shape=(50, 67), test_X.shape=(17, 10, 67), test_y.shape=(17, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4441 - val_loss: 0.4379\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4435 - val_loss: 0.4378\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4430 - val_loss: 0.4376\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4425 - val_loss: 0.4374\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4419 - val_loss: 0.4372\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4414 - val_loss: 0.4371\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4409 - val_loss: 0.4369\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4404 - val_loss: 0.4367\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4398 - val_loss: 0.4366\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4393 - val_loss: 0.4364\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4388 - val_loss: 0.4362\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4383 - val_loss: 0.4361\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4378 - val_loss: 0.4359\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4373 - val_loss: 0.4357\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4368 - val_loss: 0.4356\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4363 - val_loss: 0.4354\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4358 - val_loss: 0.4352\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4353 - val_loss: 0.4351\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4348 - val_loss: 0.4349\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4343 - val_loss: 0.4348\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4339 - val_loss: 0.4346\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4334 - val_loss: 0.4344\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4329 - val_loss: 0.4343\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4324 - val_loss: 0.4341\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4320 - val_loss: 0.4340\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4315 - val_loss: 0.4338\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4310 - val_loss: 0.4337\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4306 - val_loss: 0.4335\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4301 - val_loss: 0.4334\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4296 - val_loss: 0.4332\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4292 - val_loss: 0.4331\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4287 - val_loss: 0.4329\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4283 - val_loss: 0.4328\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4278 - val_loss: 0.4326\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4274 - val_loss: 0.4325\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4269 - val_loss: 0.4323\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4265 - val_loss: 0.4322\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4261 - val_loss: 0.4320\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4256 - val_loss: 0.4319\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4252 - val_loss: 0.4317\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(17, 67), test_y.shape=(17, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 191.2474349797922, my average MASE = 74799483.45709221\n",
      "Cluster 7, 191.2474349797922\n",
      "Before prediction: train_X.shape=(2, 10, 67), train_y.shape=(2, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2212 - val_loss: 0.1239\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2202 - val_loss: 0.1236\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2192 - val_loss: 0.1233\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2183 - val_loss: 0.1230\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2174 - val_loss: 0.1228\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2165 - val_loss: 0.1225\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2156 - val_loss: 0.1223\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2147 - val_loss: 0.1221\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2138 - val_loss: 0.1219\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2129 - val_loss: 0.1217\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2120 - val_loss: 0.1215\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2111 - val_loss: 0.1213\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2102 - val_loss: 0.1212\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2093 - val_loss: 0.1211\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2085 - val_loss: 0.1210\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2076 - val_loss: 0.1209\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2068 - val_loss: 0.1208\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2059 - val_loss: 0.1207\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2051 - val_loss: 0.1207\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2042 - val_loss: 0.1206\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2034 - val_loss: 0.1206\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2026 - val_loss: 0.1205\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2018 - val_loss: 0.1204\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2010 - val_loss: 0.1204\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2002 - val_loss: 0.1203\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1995 - val_loss: 0.1203\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1987 - val_loss: 0.1203\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1980 - val_loss: 0.1203\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1973 - val_loss: 0.1203\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1965 - val_loss: 0.1203\n",
      "Epoch 30: early stopping\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.15391611936993946, my average MASE = 0.29566028544300293\n",
      "Cluster 8, 0.15391611936993946\n",
      "Before prediction: train_X.shape=(4, 10, 67), train_y.shape=(4, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5076 - val_loss: 0.4286\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5055 - val_loss: 0.4276\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5036 - val_loss: 0.4266\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5017 - val_loss: 0.4256\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4998 - val_loss: 0.4246\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4980 - val_loss: 0.4236\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4962 - val_loss: 0.4226\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4944 - val_loss: 0.4217\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4926 - val_loss: 0.4208\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4908 - val_loss: 0.4199\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4890 - val_loss: 0.4191\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4871 - val_loss: 0.4182\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4853 - val_loss: 0.4173\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4835 - val_loss: 0.4164\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4817 - val_loss: 0.4156\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4799 - val_loss: 0.4148\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4782 - val_loss: 0.4140\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4765 - val_loss: 0.4133\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4749 - val_loss: 0.4126\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4732 - val_loss: 0.4118\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4717 - val_loss: 0.4111\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4701 - val_loss: 0.4104\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4686 - val_loss: 0.4097\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4671 - val_loss: 0.4089\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4656 - val_loss: 0.4082\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4641 - val_loss: 0.4075\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4627 - val_loss: 0.4068\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4613 - val_loss: 0.4061\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4600 - val_loss: 0.4054\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4587 - val_loss: 0.4048\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4573 - val_loss: 0.4042\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4560 - val_loss: 0.4036\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4546 - val_loss: 0.4030\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4533 - val_loss: 0.4025\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4520 - val_loss: 0.4019\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4506 - val_loss: 0.4014\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4493 - val_loss: 0.4009\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4480 - val_loss: 0.4004\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4467 - val_loss: 0.3999\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4454 - val_loss: 0.3994\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.2425229695624874, my average MASE = 0.5232189989788755\n",
      "Cluster 9, 0.2425229695624874\n",
      "Before prediction: train_X.shape=(6, 10, 67), train_y.shape=(6, 67), test_X.shape=(2, 10, 67), test_y.shape=(2, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4116 - val_loss: 0.3522\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4095 - val_loss: 0.3515\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4075 - val_loss: 0.3508\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4055 - val_loss: 0.3501\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4035 - val_loss: 0.3494\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4016 - val_loss: 0.3488\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3996 - val_loss: 0.3481\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3977 - val_loss: 0.3475\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3958 - val_loss: 0.3468\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3939 - val_loss: 0.3462\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3920 - val_loss: 0.3456\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3902 - val_loss: 0.3450\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3884 - val_loss: 0.3445\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3865 - val_loss: 0.3439\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3847 - val_loss: 0.3434\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3829 - val_loss: 0.3430\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3811 - val_loss: 0.3425\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3793 - val_loss: 0.3420\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3776 - val_loss: 0.3416\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3759 - val_loss: 0.3411\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3742 - val_loss: 0.3407\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3725 - val_loss: 0.3402\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3708 - val_loss: 0.3398\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3692 - val_loss: 0.3394\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3675 - val_loss: 0.3390\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3659 - val_loss: 0.3386\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3644 - val_loss: 0.3382\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3628 - val_loss: 0.3379\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3612 - val_loss: 0.3376\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3597 - val_loss: 0.3373\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3583 - val_loss: 0.3370\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3568 - val_loss: 0.3367\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3554 - val_loss: 0.3364\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3540 - val_loss: 0.3361\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3526 - val_loss: 0.3358\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3512 - val_loss: 0.3355\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3498 - val_loss: 0.3352\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3484 - val_loss: 0.3350\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3471 - val_loss: 0.3347\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3457 - val_loss: 0.3344\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_original.shape=(2, 67), test_y.shape=(2, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 624.4867423423187, my average MASE = 18127046.32229549\n",
      "Cluster 10, 624.4867423423187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maes = defaultdict(lambda: [])\n",
    "mases = defaultdict(lambda: [])\n",
    "mapes = defaultdict(lambda: [])\n",
    "answers = {}\n",
    "bad_values = np.zeros(dataset.shape[1])\n",
    "\n",
    "dif=True\n",
    "\n",
    "for window_size in window_sizes_for_clustering:\n",
    "    for N_clusters in Ns_clusters:\n",
    "        dataset_windows, dataset_y = Forecasting.create_windows(dataset, window_size=window_size)\n",
    "        clusters_labels = Clustering.KMeans_for_windows(dataset_windows, W=window_size, N_clusters=N_clusters, max_iter=50)\n",
    "        print(f\"{clusters_labels.shape=}\")\n",
    "        datasets_clusters = Clustering.flatten_from_interceting_windows(dataset_windows, clusters_labels, W=window_size, \\\n",
    "                N_clusters=N_clusters)\n",
    "        # list of list of ndarrays [N_i, Q], dataset_clusters[cluster_num][i] - i-th part of dataset for cluster_num\n",
    "\n",
    "        print(f\"{N_clusters=}, {len(datasets_clusters)}, {len(datasets_clusters[0])}, {datasets_clusters[0][0].shape}\")\n",
    "        ###window_size for model\n",
    "        errors = [1] * N_clusters\n",
    "        for cluster_num in range(N_clusters):\n",
    "            sc = Forecasting.MyStandardScaler(dif=dif)\n",
    "            #datasets_clusters[cluster_num] - list of [N_i, Q] ndarrays\n",
    "            sc.fit(datasets_clusters[cluster_num])\n",
    "            prepared_data = sc.transform(datasets_clusters[cluster_num])\n",
    "            data_X, data_y = Forecasting.create_windows(prepared_data, window_size=10)\n",
    "            #data_X - list of [N_i-W, W, Q] ndarrays\n",
    "            train_X, train_y, valid_X, valid_y, test_X, test_y, ind = Forecasting.split_to_train_test(data_X, data_y, part_of_test=0.2, part_of_valid=0.2)\n",
    "            #ndarrays [N_i, W, Q] or [N_i, Q]\n",
    "            ind = np.array(ind) + window_size\n",
    "            print(f\"Before prediction: {train_X.shape=}, {train_y.shape=}, {test_X.shape=}, {test_y.shape=}\")\n",
    "            try:\n",
    "                assert(len(test_X.shape) == 3 and test_X.shape[0] > 0)\n",
    "                assert(len(valid_X.shape) == 3 and valid_X.shape[0] > 0)\n",
    "                assert(len(train_X.shape) == 3 and train_X.shape[0] > 0)\n",
    "            except AssertionError:\n",
    "                print(f\"FAIL - {test_X.shape=}, {valid_X.shape=}, {train_X.shape=}\")\n",
    "                errors[cluster_num] = np.Inf\n",
    "                continue\n",
    "            model, history = Forecasting.learn(train_X, train_y, valid_X=valid_X, valid_y=valid_y)\n",
    "            predicted = model.predict(test_X)\n",
    "            predicted_original = sc.inverse_transform(predicted)[0]\n",
    "            #inverse_trasform returns list of ndarrays \n",
    "            # if dif:\n",
    "                #константа при дифференцировании\n",
    "                # predicted_original = sc.add_first_element(predicted_original, ind)[0]\n",
    "            print(f\"{predicted_original.shape=}, {test_y.shape=}\")\n",
    "\n",
    "            #calc all metrics\n",
    "            cur_mae = mae(test_y, predicted_original, multioutput='raw_values')\n",
    "#             error_out = mase(test_y, predicted_original, y_train=test_y)\n",
    "#             error_in = mase(test_y, predicted_original, y_train=train_y)\n",
    "            # cur_mase = mase(test_y, predicted_original, y_train=test_y)\n",
    "            cur_mape = mape(test_y, predicted_original)\n",
    "            cur_mase = Forecasting.my_mase(test_y, predicted_original, multioutput='raw_values')\n",
    "            maes[(window_size, N_clusters)].append(cur_mae)\n",
    "#             mases[(window_size, N_clusters)].append((error_in, error_out))\n",
    "            mapes[(window_size, N_clusters)].append(cur_mape)\n",
    "#             errors[cluster_num] = mase_uni(test_y, predicted_original, y_train=test_y)\n",
    "            tmp_bad = cur_mase > np.percentile(cur_mase, 90)\n",
    "            bad_values += tmp_bad\n",
    "            cur_mase[tmp_bad] = -1\n",
    "#             errors[cluster_num] = Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')\n",
    "            errors[cluster_num] = np.mean(cur_mase[~tmp_bad])\n",
    "            \n",
    "            #show all metrics\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.suptitle(f\"K={N_clusters}, W={window_size}, C={cluster_num}\")\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(cur_mae, color=\"green\", label=\"library\")\n",
    "            plt.plot(Forecasting.my_mae(test_y, predicted_original, multioutput='raw_values'), color=\"red\", label=\"custom\")\n",
    "            plt.title(\"MAE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "#             plt.plot(error_in, label=\"library, in\")\n",
    "#             plt.plot(error_out, label=\"library, out\")\n",
    "            plt.plot(cur_mase, label=\"custom, out\")\n",
    "            plt.title(\"MASE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(cur_mape)\n",
    "            plt.title(\"MAPE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f\"plots/Dataset2/K={N_clusters}  W={window_size} C={cluster_num}.png\")\n",
    "#             plt.show()    \n",
    "            plt.clf()\n",
    "            # print(f\"{cur_mae=}, {cur_mase=}, {cur_mape=}\")\n",
    "            # my_mase = mase()\n",
    "            # print(f\"MASE in_sample = {error_in}, MASE out_sample = {error_out}\")\n",
    "            print(f\"average MASE = {errors[cluster_num]}, my average MASE = {Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')}\")\n",
    "            print(f\"Cluster {cluster_num}, {errors[cluster_num]}\")\n",
    "        answers[(window_size, N_clusters)] = errors\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.suptitle(f\"K={N_clusters}, W={window_size}\")\n",
    "        plt.subplot(2, 2, 1)\n",
    "\n",
    "        plt.bar(np.arange(N_clusters), [np.sum(clusters_labels == i) for i in range(N_clusters)], color='blue')\n",
    "        plt.title(\"Размеры кластеров\")\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(np.arange(N_clusters), [len(datasets_clusters[i]) for i in range(N_clusters)], color=\"green\")\n",
    "        plt.title(\"Количество непрерывных отрезков\")\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.bar(np.arange(N_clusters), errors, color=\"red\")\n",
    "        plt.title(\"MASE на тесте каждого из кластеров\")\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis('tight')\n",
    "        plt.axis('off')\n",
    "        plt.table(cellText= [[f\"{x:.2f}\"] for x in errors],\n",
    "                      rowLabels=list(range(N_clusters)),\n",
    "                      loc='center')\n",
    "#         plt.show()\n",
    "        plt.savefig(f\"plots/Dataset2/method1: {N_clusters=}  W={window_size}.png\")\n",
    "        #         plt.show()\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(np.arange(bad_values.shape[0]), bad_values)\n",
    "plt.title(\"Количество раз, когда переменная имела максимум MASE\")\n",
    "plt.savefig(f\"plots/Dataset2/bad_values.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([ 2,  4,  7,  8, 11, 14, 18, 21, 23, 25, 28, 30, 33, 37, 39, 43, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 5)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 24, 25, 26, 28, 30, 31, 33,\n",
      "       39, 48, 50, 53, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 28, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 33, 37, 39,\n",
      "       41, 43, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 9)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 12, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 23, 25, 26, 28, 30, 39, 41, 42, 48,\n",
      "       51, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(3, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 13, 14, 15, 18, 21, 23, 25, 26, 28, 30, 33,\n",
      "       37, 39, 41, 42, 44, 48, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 5)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 26, 28, 30, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 7)\n",
      "     (array([ 2, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 35, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 9)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 23, 25, 28, 30, 31, 33, 35, 39,\n",
      "       41, 43, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(3, 11)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 37, 39, 44, 48,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 2)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 12, 14, 18, 21, 23, 25, 30, 31, 33, 37, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 5)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 31, 39, 60, 61,\n",
      "       62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(5, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 16, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 23, 25, 28, 30, 33, 35, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "(5, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 14, 15, 16, 18, 19, 21, 23, 25, 27, 28, 30,\n",
      "       39, 42, 44, 47, 48, 52, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 15, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35,\n",
      "       37, 39, 60, 61, 62, 63, 65]),)\n",
      "(5, 11)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 28, 30, 31, 33, 39,\n",
      "       41, 42, 49, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(10, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 11, 13, 14, 18, 21, 23, 25, 28, 30, 31, 33,\n",
      "       35, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 5)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 22, 23, 25, 28, 30, 31, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 7)\n",
      "     (array([ 2,  4,  8,  9, 14, 16, 18, 19, 30, 58, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "(10, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 23, 24, 25, 26, 28, 30, 31,\n",
      "       33, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(10, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 12, 14, 18, 21, 22, 23, 25, 28, 30, 39, 56, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in maes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mases.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c[0] < 1)}, {np.where(val_c[1] < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([14, 18, 25, 30, 31, 33, 35, 37, 43]),)\n",
      "     (array([14, 30]),)\n",
      "(1, 5)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(1, 7)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 11, 14]),)\n",
      "     (array([14]),)\n",
      "(1, 9)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 4,  8, 14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14, 60, 61]),)\n",
      "(1, 11)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(3, 2)\n",
      "     (array([ 4, 14, 18, 30, 31, 33, 35, 37, 42, 62]),)\n",
      "     (array([14, 30]),)\n",
      "(3, 5)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(3, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61]),)\n",
      "(3, 9)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 28, 39, 62]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 19, 30]),)\n",
      "(3, 11)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 23, 28, 39, 62, 65]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "(5, 2)\n",
      "     (array([12, 14, 18, 30, 31, 33, 35, 37, 60, 62, 65]),)\n",
      "     (array([14, 30]),)\n",
      "(5, 5)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "(5, 7)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 9, 11, 14, 18, 19, 21, 23, 25, 30, 39, 42, 44, 47, 52, 60, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 11)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 21, 23, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "(10, 2)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "(10, 5)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 14]),)\n",
      "(10, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 11)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in mapes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anna",
   "language": "python",
   "name": "anna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
