{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import Clustering, Forecasting\n",
    "# importlib.reload(Clustering)\n",
    "# importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DataSet2.csv\", sep=\";\")#, parse_dates=['Timestamp']) #, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns\n",
    "#CV, DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafile = open(\"DataSet2_fix.json\")\n",
    "metafile = json.load(metafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_DV = [x[0] for x in metafile['ColumnKind'].items() if x[1] == \"CV\" or x[1] == \"DV\"]\n",
    "CV_DV = [x for x in list(columns) if x in CV_DV]\n",
    "data = data[CV_DV]\n",
    "CV_DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 30\n",
    "df = data.groupby(data.index // K).mean() #усреднение\n",
    "df_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError, MeanAbsolutePercentageError\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mase = MeanAbsoluteScaledError(multioutput='raw_values')\n",
    "mase_uni = MeanAbsoluteScaledError(multioutput='uniform_average')\n",
    "mape = MeanAbsolutePercentageError(multioutput='raw_values')\n",
    "# mae = MeanAbsoluteError()\n",
    "\n",
    "\n",
    "#if ‘raw_values’, returns a full set of errors in case of multioutput input. If ‘uniform_average’, errors of all outputs are averaged with uniform weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_np[:, :]\n",
    "# dataset = np.concatenate((dataset[:, :8], dataset[:, 9:]), axis=1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__Отбор признаков на минималках__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаление константных\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.001)\n",
    "dataset1 = selector.fit_transform(dataset) \n",
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(range(dataset1.shape[-1])) - set(selector.get_feature_names_out(input_features=np.arange(dataset.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(dataset[:, 14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(dataset[:, 8], color=\"green\")\n",
    "plt.plot(np.concatenate((dataset[:32714, 8], dataset[32717:, 8])))\n",
    "plt.title(\"consumption_09\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.unique(dataset[:, 8])\n",
    "# print(values)\n",
    "# print([len(np.argwhere(dataset[:, 8] == val)) for val in values])\n",
    "# print(f\"Изменение только в {np.argwhere(dataset[1:, 8] - dataset[:-1, 8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эти датчики пока выкинем из-за константности: ** \n",
    "\n",
    "8: consumption_09\n",
    "\n",
    "14 : consumption_07 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns[15], columns[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset[7])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import correlate\n",
    "# a = np.array([np.sin(x + 1) for x in range(10)])\n",
    "# b = np.array([np.sin(x + 2) for x in range(10)])\n",
    "# c = correlate(a, b, mode=\"full\") #mode='same'\n",
    "# plt.plot(a, label=\"a\")\n",
    "# plt.plot(b, label=\"b\")\n",
    "# plt.plot(c, label=\"correlation\")\n",
    "# plt.legend()\n",
    "# c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset1\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Прогнозирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset1[:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset[..., 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "window_sizes_for_clustering = 10\n",
    "# X, y = dataset[:-window_sizes_for_clustering, ...], dataset[window_sizes_for_clustering:, ...]\n",
    "# X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "n_split = round(0.2 * dataset.shape[0])\n",
    "dataset_train, dataset_test = dataset[:-n_split, ...], dataset[-n_split:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_train.shape, dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes_for_clustering = [1, 5, 10]#[1, 5, 10, 15] #[1, 3, 5, 10, 15]\n",
    "Ns_clusters = [2, 9, 11] #, 3 5, 7]#[2, 5, 7, 9, 11, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_algorithms = [\"MeanShift\", \"Kmeans\"] #[\"Agglomerative\"] #[\"MeanShift\"]\n",
    "clustering_algorithms = []\n",
    "clustering_algorithms += [Clustering.Kmeans_for_windows(W=W, N_clusters=N_cluster) for W in window_sizes_for_clustering for N_cluster in Ns_clusters]\n",
    "# clustering_algorithms += [Clustering.MeanShift_for_windows(W=W) for W in window_sizes_for_clustering]\n",
    "clustering_algorithms += [Clustering.AgglomerativeClustering_for_windows(W=W, N_clusters=N_cluster) for W in window_sizes_for_clustering for N_cluster in Ns_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithms[0].N_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ns_clusters = [2]\n",
    "# window_sizes_for_clustering = [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, M, Q = 100, 100, 2\n",
    "# dataset_train = np.column_stack([[np.sin(x / 40) for x in range(N)], [0.01 + np.sin(x / 20) for x in range(N)]])\n",
    "# # dataset_train = np.array([np.sin(x / 10000) for x in range(N)])[:, None]\n",
    "# # dataset_train = np.column_stack(([np.sqrt(x / 10) for x in range(N)], [x + ])\n",
    "# dataset_test = np.column_stack([[np.sin(x / 30) for x in range(M)], [0.01 + np.sin(x / 30)*1.01 for x in range(M)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# a = Clustering.MeanShift_for_windows(W=W).fit_predict(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters_labels, clusters_model \n",
    "# clustering_results = [Clustering.apply_clustering(dataset, cur_cluster_alg) for cur_cluster_alg in clustering_algorithms]\n",
    "# clusters_labels = [x[0] for x in clustering_results]\n",
    "# clusters_models = [x[1] for x in clustering_results]\n",
    "clustering_results = [cur_cluster_alg.fit_predict(dataset_train) for cur_cluster_alg in clustering_algorithms]\n",
    "clusters_labels = clustering_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clustering_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# models, results_training \n",
    "# forecasting_training_results = [Forecasting.apply_forecasting_training(dataset, cur_cluster_labels, W=10) for cur_cluster_labels in clusters_labels]\n",
    "# models = [x[0] for x in forecasting_training_results]\n",
    "# results_training = [x[1] for x in forecasting_training_results]\n",
    "# metrics = [x[2] for x in forecasting_training_results]\n",
    "#_simple_version\n",
    "forecasting_training_results = [Forecasting.apply_forecasting_training_simple_version(dataset_train, cur_cluster_labels, W=W) for cur_cluster_labels in clusters_labels]\n",
    "models = [x[0] for x in forecasting_training_results]\n",
    "results_training = [x[1] for x in forecasting_training_results]\n",
    "metrics_training = [x[2] for x in forecasting_training_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_training = results_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(models) == len(results_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = dataset.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_training[0][:100, 8], label=\"true\")\n",
    "plt.plot(results_training[0][:100, Q+8], label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Оригинальные значения\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# # full_results = np.concatenate((results_training, results_testing), axis=0)\n",
    "# # full_results = results_training\n",
    "# full_results = np.concatenate([results_training[:, :2*Q], results_training[:, 4*Q:]], axis=1)\n",
    "# with open(\"output_table_03-24.csv\", \"w\") as fout:\n",
    "#     writer = csv.writer(fout)\n",
    "#     writer.writerow([\"real \"+str(i) for i in range(Q)] + [\"predicted \" + str(i) for i in range(Q)] + [\"cluster_num\", \"mode\"])\n",
    "#     for i in range(full_results.shape[0]):\n",
    "#         writer.writerow(full_results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"weighted mae:\", [x[-1] for x in forecasting_training_results])\n",
    "print(np.mean([x['mae'] for x in metrics_training[2]], axis=1))\n",
    "print(np.mean([x['mase'] for x in metrics_training[2]], axis=1))\n",
    "# plt.plot([x['mae'] for x in metrics_training[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_sizes = [[np.sum(cur_clusters_labels == i) for i in range(len(np.unique(cur_clusters_labels)))] for cur_clusters_labels in clusters_labels]\n",
    "N_clusters = len(clusters_sizes)\n",
    "mae_on_max_cluster = np.array([metrics_training[i][np.argmax(clusters_sizes[i])]['mae'] for i in range(len(forecasting_training_results))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maes = np.array([[metrics_training[i][c]['mae'] for c in range(N_clusters)] for i in range(len(metrics_training))])\n",
    "# # weighted_mae = np.array([np.average(maes[i, :, :], weights=clusters_sizes[i], axis=0) for i in range(len(forecasting_training_results))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mae_on_max_cluster, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_testing = [Forecasting.predict_through_clusters(dataset_test, clustering_algorithms[i], models[i], W=W) for i in range(len(results_training))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(x) for x in clustering_algorithms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = [np.concatenate((\n",
    "                    results_training[i],\n",
    "                    np.concatenate((dataset_test[W:], results_testing[i]), axis=1)\n",
    "                    ),\n",
    "                    axis=0\n",
    "                ) for i in range(len(results_training))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp\n",
    "# full_results = results_testing\n",
    "# full_results = [np.concatenate((dataset_test[W:], results_testing[i]), axis=1) for i in range(len(results_training))]\n",
    "# full_results = results_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metrics = [Forecasting.calc_metrics_for_full_results(x) for x in full_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_metrics\n",
    "clusters_sizes = [np.array([np.sum(x[:, -2] == i) for i in range(len(np.unique(x[:, -2])))]) for x in full_results]\n",
    "N_clusters = [len(x) for x in clusters_sizes]\n",
    "print(N_clusters)\n",
    "# mae_on_max_cluster = np.array([global_metrics[i][np.argmax(clusters_sizes[i])]['mae'] for i in range(len(full_results))])\n",
    "# mae_mean = np.array([np.mean(global_metrics[i][np.argmax(clusters_sizes[i])]['mae']) for i in range(full_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = np.array([global_metrics[i]['mae'] for i in range(len(forecasting_training_results))])\n",
    "mae_on_max_cluster = np.array([\n",
    "        Forecasting.calc_metrics_for_full_results(\n",
    "            full_results[i][full_results[i][:, -2] == np.argmax(clusters_sizes[i])]\n",
    "        )['mae'] \n",
    "    for i in range(len(full_results))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(maes, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.around(np.mean(mae_on_max_cluster, axis=1).reshape(2, 3, 3),decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.mean(mae_on_max_cluster, axis=1), label=\"on max cluster\")\n",
    "plt.plot(np.mean(maes, axis=1), label=\"global mae\")\n",
    "plt.legend()\n",
    "plt.title(\"Average mae\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"Number of experiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = np.argmin(np.mean(mae_on_max_cluster, axis=1))\n",
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clusters_labels[ind1])\n",
    "plt.title(\"Размеры кластеров\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(full_results[0][full_results[0][:, -1] == 3, 5])\n",
    "plt.show()\n",
    "plt.hist(full_results[0][full_results[0][:, -1] == 0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(global_metrics[ind1]['mase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.mean(x['mase']) for x in global_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2 = np.argmin([np.mean(x['mase']) for x in global_metrics])\n",
    "ind2, np.mean(global_metrics[ind2]['mase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (np.abs(full_results[15][:, :Q] - full_results[15][:, Q:2*Q])).sum(axis=0) / full_results[15].shape[0]\n",
    "denom = (np.abs(full_results[15][1:, :Q] - full_results[15][:-1, :Q])).sum(axis=0) / (full_results[15].shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom.mean(), num.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(num / denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование (старое)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#номер сохраняемого эксперимента\n",
    "IND = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = np.array([1, 1, 1, 5, 5, 5, 10, 10, 10] + [1, 1, 1, 5, 5, 5, 10, 10, 10])\n",
    "len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = N_clusters\n",
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = [\"Kmeans\"] * 9 + [\"Agglomer\"] * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# tmp = np.concatenate((dataset_test[W:], results_testing[0]), axis=1)\n",
    "# full_results = np.zeros((results_training.shape[0] + results_testing.shape[0], results_training))\n",
    "# full_results = np.concatenate((results_training[0], tmp), axis=0)\n",
    "# full_results = results_training\n",
    "# full_results = np.concatenate([results_training[:, :2*Q], results_training[:, 4*Q:]], axis=1)\n",
    "for IND in range(0, 18):\n",
    "    with open(f\"./results/W20_group30/{IND:02}_{algs[IND]}_N{N_clusters[IND]}_w{ws[IND]}.csv\", \"w\") as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow([\"real \"+str(i) for i in range(Q)] + [\"predicted \" + str(i) for i in range(Q)] + [\"cluster_num\", \"mode\"])\n",
    "        for i in range(full_results[IND].shape[0]):\n",
    "            writer.writerow(full_results[IND][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_csv(\"./output_table_04-07_for60_2.csv\")\n",
    "new_data = pd.DataFrame(read, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mae_on_max_cluster, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.mean([x[-1] for x in forecasting_training_results], axis=1), label=\"weighted MAE\")\n",
    "# plt.plot(np.mean(mae_on_max_cluster, axis=1), label=\"MAE on max cluster\")\n",
    "# plt.legend()\n",
    "# plt.title(\"MAE on different cluster algorithms\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results1 = [Clustering.apply_clustering(dataset, cur_cluster_alg) for cur_cluster_alg in [Clustering.MeanShift_for_windows(W=W) for W in window_sizes_for_clustering]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_labels1 = [x[0] for x in clustering_results1]\n",
    "\n",
    "clusters_sizes1 = [[np.sum(cur_clusters_labels == i) for i in range(len(np.unique(cur_clusters_labels)))] for cur_clusters_labels in clusters_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clusters_labels1[-1])\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.mean(mae_on_max_cluster, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"dif\":True, \n",
    "              \"cluster_algs\":clustering_algorithms}\n",
    "# models, model_mae, results_training = Forecasting.try_parameters(parameters, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((5, 4))\n",
    "sc = Forecasting.MyStandardScaler()\n",
    "sc.fit(a)\n",
    "sc.transform(a)\n",
    "sc.mean, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IND = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for i, file_name in enumerate(list(os.listdir(\"scalers\"))):\n",
    "#     os.unlink(\"scalers/\"+file_name)\n",
    "# for i in range(len(models['scalers'])):\n",
    "#     with open(\"scalers/\"+str(i)+\".pkl\", \"wb\") as f:\n",
    "#         pickle.dump(models['scalers'][i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.unlink(\"clusters_model.pkl\")\n",
    "with open(\"clusters_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(models['clusters_model'], f)\n",
    "        pickle.dump(clustering_algorithms[IND], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "for i, file_name in enumerate(list(os.listdir(\"models\"))):\n",
    "    shutil.rmtree(\"models/\"+file_name)\n",
    "for i in range(len(models[IND])):\n",
    "    if not isinstance(models[IND][i], int):\n",
    "        models[IND][i].save(\"models/\"+str(i))\n",
    "print(len(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "N_clusters = len(os.listdir('models'))\n",
    "assert(len(os.listdir('scalers')) == N_clusters)\n",
    "models = {'models':[], 'clusters_model':[], 'scalers':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "for file_name in os.listdir('models'):\n",
    "    models['models'].append(keras.models.load_model('models/'+file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir('scalers'):\n",
    "    with open('scalers/'+file_name, 'rb') as f:\n",
    "        models['scalers'].append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clusters_model.pkl', 'rb') as f:\n",
    "    models['clusters_model'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_model = models[\"clusters_model\"]\n",
    "forecasting_models = models['models']\n",
    "scalers = models['scalers']\n",
    "assert(len(forecasting_models) == len(scalers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes_for_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_clusters = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_windows = sliding_window_view(dataset_train, (window_size_clustering, dataset_train.shape[-1])) #(N, 1, W, Q)\n",
    "# new_shape = dataset_windows.shape\n",
    "# dataset_windows = dataset_windows.reshape(new_shape[0], new_shape[2] * new_shape[3])\n",
    "# cluster_nums = clusters_model.labels_\n",
    "# print(f\"{dataset.shape=}, {dataset_windows.shape=}, {cluster_nums.shape=}, {dataset.shape[0] - dataset_windows.shape[0]}\")\n",
    "# cluster_nums = np.pad(cluster_nums, (dataset.shape[0] - dataset_windows.shape[0], 0), mode='constant', constant_values=(cluster_nums[0])) #-1\n",
    "# print(f\"After pad: {dataset.shape=}, {cluster_nums.shape=}\")\n",
    "# # if window_size_clustering > window_size_forecasting:\n",
    "# #     cluster_num = np.pad(cluster_nums, (0, ), mode='constant', constant_values=(cluster_nums[-1]))\n",
    "# y_pred = np.zeros((dataset.shape[0] - window_size_forecasting, dataset.shape[-1]))\n",
    "# #only if dif then +1\n",
    "# dataset_windows = sliding_window_view(dataset, (window_size_forecasting + 1, dataset.shape[-1]))\n",
    "# cluster_nums = cluster_nums[window_size_forecasting:] #+1\n",
    "    \n",
    "# full_results = np.zeros((cluster_nums.shape[0], 2 * dataset_train.shape[-1] + 2)) # real_Q, Q, cluster_num, mask\n",
    "\n",
    "# for cluster_num in range(N_clusters):\n",
    "#     mask = (cluster_nums == cluster_num)\n",
    "#     if np.sum(mask) == 0:\n",
    "#         continue\n",
    "\n",
    "#     cur_windows = dataset_windows[mask, 0, ...] #(M, Wf, Q)\n",
    "#     scalers[cluster_num].save_first_elements(cur_windows)\n",
    "#     cur_windows = np.array(scalers[cluster_num].transform(cur_windows))\n",
    "#     cur_pred = np.array(prediction_models[cluster_num](cur_windows)) #(M, Q)\n",
    "#     cur_pred = scalers[cluster_num].inverse_transform(cur_pred)\n",
    "#     cur_pred = scalers[cluster_num].add_first_elements(cur_pred)\n",
    "#     y_pred[mask] = cur_pred\n",
    "#     Q = dataset.shape[-1]\n",
    "#     full_results[mask, Q:2 * Q] = cur_pred\n",
    "#     full_results[mask, 2 * Q] = cluster_num\n",
    "#     full_results[mask, 2 * Q + 1] = 3 #global test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size_for_clustering = clusters_model.cluster_centers_.shape[-1] // dataset_test.shape[-1]\n",
    "window_size_for_clustering = window_sizes_for_clustering[0]\n",
    "y_pred, results_testing = Forecasting.predict_through_clusters(dataset_test, clusters_model, forecasting_models, scalers, window_size_clustering=window_size_for_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#лучшие параметры\n",
    "window_size_for_clustering, len(forecasting_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, dataset.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{y_pred.shape=}, {dataset_test.shape=}\")\n",
    "y_true = dataset_test[-y_pred.shape[0]:]\n",
    "print(y_true.shape, results_testing[:, :dataset_train.shape[-1]].shape)\n",
    "results_testing[:, :dataset_train.shape[-1]] = y_true\n",
    "cur_mase = Forecasting.my_mase(y_true, y_pred, multioutput='raw_values')\n",
    "cur_mae = Forecasting.my_mae(y_true, y_pred, multioutput=\"raw_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = dataset_train.shape[-1]\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# full_results = np.concatenate((results_training, results_testing), axis=0)\n",
    "full_results = results_training\n",
    "with open(\"output_table_03-23.csv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow([\"real \"+str(i) for i in range(Q)] + [\"predicted \" + str(i) for i in range(Q)] + [\"cluster_num\", \"mode\"])\n",
    "    for i in range(full_results.shape[0]):\n",
    "        writer.writerow(full_results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_mase, cur_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cur_mase <= np.percentile(cur_mase, 60)\n",
    "plt.plot(np.arange(cur_mase.shape[0])[mask], cur_mase[mask])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"MASE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cur_mae <= np.percentile(cur_mae, 60)\n",
    "# plt.plot(np.arange(cur_mae.shape[0])[mask], cur_mae[mask])\n",
    "plt.plot(cur_mae[mask])\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"best MAE\")\n",
    "# plt.show()\n",
    "plt.savefig(\"best MAE.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cur_mase[cur_mase <= np.percentile(cur_mase, 100)])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"50% best MASE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mape = mape(y_true, y_pred)\n",
    "plt.plot(cur_mape[cur_mape < np.percentile(cur_mape, 50)])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.title(\"50% best MAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мелкое тестирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Q = 1000, 10\n",
    "# test_array = (np.random.random((N * Q)) + np.sin(np.arange(N * Q) / 100)).reshape(N, Q)\n",
    "test_array = (np.sin(np.arange(N*Q)/100) * 5).reshape(N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 5\n",
    "sc = Forecasting.MyStandardScaler()\n",
    "clusters_X, clusters_labels = Clustering.split_to_clusters(test_array, np.zeros(test_array.shape[0]), W=W + 2)\n",
    "test_array_windows = clusters_X[0]\n",
    "X_true, y_true = Forecasting.split_X_y(test_array_windows)\n",
    "prepared_array = sc.fit_transform(test_array_windows)\n",
    "prepared_X, prepared_y = Forecasting.split_X_y(prepared_array)\n",
    "y_changed = sc.inverse_transform(prepared_y)\n",
    "# array_changed = sc.inverse_transform(prepared_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[:100, 0], label=\"true\")\n",
    "plt.plot(prepared_y[:100, 0], label=\"transformed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(y_true - y_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.zeros(N)\n",
    "# test_labels[test_array[:, 0] <= np.percentile(test_array, 50)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, test_results, _ = Forecasting.apply_forecasting_training_simple_version(test_array, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = test_array.shape[-1]\n",
    "plt.plot(test_results[:40, 0], label=\"true\")\n",
    "plt.plot(test_results[:40, Q], label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"output_table_03-22.csv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow([\"real \"+str(i) for i in range(Q)] + [\"predicted \" + str(i) for i in range(Q)] + [\"cluster_num\", \"mode\"])\n",
    "    for i in range(test_results.shape[0]):\n",
    "        writer.writerow(test_results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
