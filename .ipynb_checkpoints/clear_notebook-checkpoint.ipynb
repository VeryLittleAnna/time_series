{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 12:23:35.054652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-30 12:23:35.054709: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/anna/Desktop/MSU/научка/git/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DataSet2.csv\", sep=\";\")#, parse_dates=['Timestamp']) #, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "df = data.groupby(data.index // K).mean() #усреднение\n",
    "df_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError, MeanAbsolutePercentageError\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mase = MeanAbsoluteScaledError(multioutput='raw_values')\n",
    "mase_uni = MeanAbsoluteScaledError(multioutput='uniform_average')\n",
    "mape = MeanAbsolutePercentageError(multioutput='raw_values')\n",
    "# mae = MeanAbsoluteError()\n",
    "\n",
    "\n",
    "#if ‘raw_values’, returns a full set of errors in case of multioutput input. If ‘uniform_average’, errors of all outputs are averaged with uniform weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes_for_clustering = [1, 3, 5, 10]\n",
    "Ns_clusters = [2, 5, 7, 9, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns_clusters = [7]\n",
    "window_sizes_for_clustering = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_np[:100000, :]\n",
    "# dataset = np.concatenate((dataset[:, :8], dataset[:, 9:]), axis=1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters_labels.shape=(99997,)\n",
      "N_clusters=7, 7, 1, (2617, 67)\n",
      "Before prediction: train_X.shape=(1564, 10, 67), train_y.shape=(1564, 67), test_X.shape=(521, 10, 67), test_y.shape=(521, 67)\n",
      "Epoch 1/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2407 - val_loss: 0.2787\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2297 - val_loss: 0.2680\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.2210 - val_loss: 0.2595\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.2142 - val_loss: 0.2525\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2086 - val_loss: 0.2467\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2039 - val_loss: 0.2418\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1998 - val_loss: 0.2375\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1962 - val_loss: 0.2336\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1930 - val_loss: 0.2302\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1900 - val_loss: 0.2271\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1873 - val_loss: 0.2242\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1848 - val_loss: 0.2217\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1825 - val_loss: 0.2193\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1804 - val_loss: 0.2171\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1785 - val_loss: 0.2151\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1767 - val_loss: 0.2133\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1751 - val_loss: 0.2116\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1736 - val_loss: 0.2100\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1721 - val_loss: 0.2085\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1708 - val_loss: 0.2070\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1696 - val_loss: 0.2057\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1684 - val_loss: 0.2044\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1673 - val_loss: 0.2031\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1663 - val_loss: 0.2019\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1653 - val_loss: 0.2008\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1644 - val_loss: 0.1997\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.1635 - val_loss: 0.1987\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1626 - val_loss: 0.1978\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1618 - val_loss: 0.1968\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1610 - val_loss: 0.1959\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1603 - val_loss: 0.1950\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1595 - val_loss: 0.1942\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1588 - val_loss: 0.1933\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1582 - val_loss: 0.1926\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1575 - val_loss: 0.1919\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1569 - val_loss: 0.1911\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1563 - val_loss: 0.1904\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1558 - val_loss: 0.1898\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1552 - val_loss: 0.1891\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.1547 - val_loss: 0.1885\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(521, 67), test_y.shape=(521, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 2416311492.249654\n",
      "Cluster 0, -1.0\n",
      "Before prediction: train_X.shape=(8, 10, 67), train_y.shape=(8, 67), test_X.shape=(3, 10, 67), test_y.shape=(3, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4245 - val_loss: 0.4124\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4238 - val_loss: 0.4123\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4231 - val_loss: 0.4121\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4224 - val_loss: 0.4119\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4217 - val_loss: 0.4118\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4210 - val_loss: 0.4116\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4203 - val_loss: 0.4115\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4196 - val_loss: 0.4113\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4189 - val_loss: 0.4112\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4182 - val_loss: 0.4110\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4175 - val_loss: 0.4108\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4168 - val_loss: 0.4107\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4162 - val_loss: 0.4105\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4155 - val_loss: 0.4103\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4148 - val_loss: 0.4102\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4142 - val_loss: 0.4100\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4136 - val_loss: 0.4099\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4130 - val_loss: 0.4097\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4124 - val_loss: 0.4096\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4118 - val_loss: 0.4094\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4113 - val_loss: 0.4093\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4107 - val_loss: 0.4091\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4101 - val_loss: 0.4090\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4095 - val_loss: 0.4088\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4089 - val_loss: 0.4087\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4084 - val_loss: 0.4085\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4078 - val_loss: 0.4084\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4073 - val_loss: 0.4082\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4067 - val_loss: 0.4081\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4062 - val_loss: 0.4080\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4057 - val_loss: 0.4078\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4051 - val_loss: 0.4077\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4046 - val_loss: 0.4075\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4040 - val_loss: 0.4074\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4035 - val_loss: 0.4073\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4030 - val_loss: 0.4071\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4025 - val_loss: 0.4070\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4019 - val_loss: 0.4068\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4014 - val_loss: 0.4067\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4009 - val_loss: 0.4065\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_original.shape=(3, 67), test_y.shape=(3, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 21172834.68282392\n",
      "Cluster 1, -1.0\n",
      "Before prediction: train_X.shape=(191, 10, 67), train_y.shape=(191, 67), test_X.shape=(64, 10, 67), test_y.shape=(64, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.7193 - val_loss: 0.8799\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7179 - val_loss: 0.8786\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7166 - val_loss: 0.8774\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7153 - val_loss: 0.8762\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7139 - val_loss: 0.8750\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7127 - val_loss: 0.8738\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7115 - val_loss: 0.8727\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7102 - val_loss: 0.8716\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7090 - val_loss: 0.8704\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7078 - val_loss: 0.8693\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.7066 - val_loss: 0.8682\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7054 - val_loss: 0.8672\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.7043 - val_loss: 0.8661\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7032 - val_loss: 0.8651\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.7021 - val_loss: 0.8641\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7010 - val_loss: 0.8631\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6999 - val_loss: 0.8621\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6988 - val_loss: 0.8611\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6978 - val_loss: 0.8601\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6967 - val_loss: 0.8592\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6957 - val_loss: 0.8582\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6946 - val_loss: 0.8573\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6937 - val_loss: 0.8564\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6927 - val_loss: 0.8555\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6917 - val_loss: 0.8546\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6907 - val_loss: 0.8537\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6898 - val_loss: 0.8528\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6888 - val_loss: 0.8520\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6879 - val_loss: 0.8511\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6869 - val_loss: 0.8503\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6860 - val_loss: 0.8494\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6851 - val_loss: 0.8486\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6842 - val_loss: 0.8478\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6833 - val_loss: 0.8470\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6824 - val_loss: 0.8462\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6815 - val_loss: 0.8454\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6807 - val_loss: 0.8446\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6798 - val_loss: 0.8438\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6790 - val_loss: 0.8431\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6782 - val_loss: 0.8423\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(64, 67), test_y.shape=(64, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 125500805.44851069\n",
      "Cluster 2, -1.0\n",
      "Before prediction: train_X.shape=(19, 10, 67), train_y.shape=(19, 67), test_X.shape=(6, 10, 67), test_y.shape=(6, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4932 - val_loss: 0.2957\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4915 - val_loss: 0.2948\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4898 - val_loss: 0.2938\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4881 - val_loss: 0.2929\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4864 - val_loss: 0.2919\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4847 - val_loss: 0.2910\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4830 - val_loss: 0.2901\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4814 - val_loss: 0.2893\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4798 - val_loss: 0.2884\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4781 - val_loss: 0.2876\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4765 - val_loss: 0.2869\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4750 - val_loss: 0.2861\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4734 - val_loss: 0.2854\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4719 - val_loss: 0.2847\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4703 - val_loss: 0.2840\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4688 - val_loss: 0.2834\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4673 - val_loss: 0.2827\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4659 - val_loss: 0.2821\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4644 - val_loss: 0.2814\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4630 - val_loss: 0.2808\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4615 - val_loss: 0.2803\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4601 - val_loss: 0.2797\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4587 - val_loss: 0.2792\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4573 - val_loss: 0.2787\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4559 - val_loss: 0.2782\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4546 - val_loss: 0.2777\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4533 - val_loss: 0.2772\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4519 - val_loss: 0.2767\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4506 - val_loss: 0.2762\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4493 - val_loss: 0.2758\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4480 - val_loss: 0.2753\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4468 - val_loss: 0.2748\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4455 - val_loss: 0.2744\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4443 - val_loss: 0.2740\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4430 - val_loss: 0.2736\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4418 - val_loss: 0.2732\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4406 - val_loss: 0.2728\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4395 - val_loss: 0.2724\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4383 - val_loss: 0.2720\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4372 - val_loss: 0.2716\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(6, 67), test_y.shape=(6, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 823845837.6841807\n",
      "Cluster 3, -1.0\n",
      "Before prediction: train_X.shape=(38, 10, 67), train_y.shape=(38, 67), test_X.shape=(13, 10, 67), test_y.shape=(13, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4459 - val_loss: 0.5291\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4443 - val_loss: 0.5284\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4427 - val_loss: 0.5276\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4411 - val_loss: 0.5268\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4395 - val_loss: 0.5260\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4380 - val_loss: 0.5253\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4365 - val_loss: 0.5245\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4350 - val_loss: 0.5238\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4335 - val_loss: 0.5230\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4320 - val_loss: 0.5223\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4305 - val_loss: 0.5215\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4291 - val_loss: 0.5208\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4277 - val_loss: 0.5201\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4263 - val_loss: 0.5194\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4249 - val_loss: 0.5187\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4236 - val_loss: 0.5181\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4222 - val_loss: 0.5174\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4209 - val_loss: 0.5168\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4196 - val_loss: 0.5161\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4183 - val_loss: 0.5155\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4171 - val_loss: 0.5149\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4158 - val_loss: 0.5143\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4146 - val_loss: 0.5137\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4134 - val_loss: 0.5132\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4122 - val_loss: 0.5126\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4110 - val_loss: 0.5121\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4098 - val_loss: 0.5116\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4086 - val_loss: 0.5111\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4075 - val_loss: 0.5107\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4064 - val_loss: 0.5102\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4053 - val_loss: 0.5098\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4042 - val_loss: 0.5093\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4031 - val_loss: 0.5089\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4021 - val_loss: 0.5084\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4010 - val_loss: 0.5080\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4000 - val_loss: 0.5076\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3990 - val_loss: 0.5072\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3979 - val_loss: 0.5067\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3969 - val_loss: 0.5063\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3959 - val_loss: 0.5059\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(13, 67), test_y.shape=(13, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 13860284980.878109\n",
      "Cluster 4, -1.0\n",
      "Before prediction: train_X.shape=(157, 10, 67), train_y.shape=(157, 67), test_X.shape=(52, 10, 67), test_y.shape=(52, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5878 - val_loss: 0.5018\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5864 - val_loss: 0.5008\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5852 - val_loss: 0.4999\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5840 - val_loss: 0.4989\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5828 - val_loss: 0.4980\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5817 - val_loss: 0.4971\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5805 - val_loss: 0.4962\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5794 - val_loss: 0.4954\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5783 - val_loss: 0.4945\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.5772 - val_loss: 0.4937\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5762 - val_loss: 0.4928\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5751 - val_loss: 0.4920\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5740 - val_loss: 0.4912\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5730 - val_loss: 0.4905\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5720 - val_loss: 0.4897\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5710 - val_loss: 0.4890\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5700 - val_loss: 0.4882\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5691 - val_loss: 0.4875\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5681 - val_loss: 0.4868\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5672 - val_loss: 0.4861\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5663 - val_loss: 0.4854\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5654 - val_loss: 0.4847\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5645 - val_loss: 0.4840\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5636 - val_loss: 0.4834\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5627 - val_loss: 0.4827\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.5618 - val_loss: 0.4820\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5610 - val_loss: 0.4814\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5601 - val_loss: 0.4807\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5593 - val_loss: 0.4801\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5584 - val_loss: 0.4794\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5576 - val_loss: 0.4788\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5568 - val_loss: 0.4782\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5560 - val_loss: 0.4776\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5552 - val_loss: 0.4769\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.5544 - val_loss: 0.4763\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5536 - val_loss: 0.4757\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5528 - val_loss: 0.4751\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5520 - val_loss: 0.4746\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.5513 - val_loss: 0.4740\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5505 - val_loss: 0.4734\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "predicted_original.shape=(52, 67), test_y.shape=(52, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 81005703.87776664\n",
      "Cluster 5, -1.0\n",
      "Before prediction: train_X.shape=(1939, 10, 67), train_y.shape=(1939, 67), test_X.shape=(646, 10, 67), test_y.shape=(646, 67)\n",
      "Epoch 1/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1142 - val_loss: 0.0945\n",
      "Epoch 2/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1087 - val_loss: 0.0926\n",
      "Epoch 3/40\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1043 - val_loss: 0.0912\n",
      "Epoch 4/40\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1005 - val_loss: 0.0900\n",
      "Epoch 5/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0971 - val_loss: 0.0890\n",
      "Epoch 6/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0942 - val_loss: 0.0883\n",
      "Epoch 7/40\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0916 - val_loss: 0.0877\n",
      "Epoch 8/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0893 - val_loss: 0.0872\n",
      "Epoch 9/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0872 - val_loss: 0.0868\n",
      "Epoch 10/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0853 - val_loss: 0.0864\n",
      "Epoch 11/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0836 - val_loss: 0.0861\n",
      "Epoch 12/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0820 - val_loss: 0.0857\n",
      "Epoch 13/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0806 - val_loss: 0.0854\n",
      "Epoch 14/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0793 - val_loss: 0.0851\n",
      "Epoch 15/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0781 - val_loss: 0.0849\n",
      "Epoch 16/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0770 - val_loss: 0.0846\n",
      "Epoch 17/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0760 - val_loss: 0.0844\n",
      "Epoch 18/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0751 - val_loss: 0.0841\n",
      "Epoch 19/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0742 - val_loss: 0.0840\n",
      "Epoch 20/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0734 - val_loss: 0.0837\n",
      "Epoch 21/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0727 - val_loss: 0.0836\n",
      "Epoch 22/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0720 - val_loss: 0.0834\n",
      "Epoch 23/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0713 - val_loss: 0.0833\n",
      "Epoch 24/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0707 - val_loss: 0.0831\n",
      "Epoch 25/40\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0701 - val_loss: 0.0830\n",
      "Epoch 26/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0696 - val_loss: 0.0829\n",
      "Epoch 27/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0691 - val_loss: 0.0828\n",
      "Epoch 28/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0686 - val_loss: 0.0827\n",
      "Epoch 29/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0681 - val_loss: 0.0826\n",
      "Epoch 30/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0677 - val_loss: 0.0825\n",
      "Epoch 31/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0672 - val_loss: 0.0825\n",
      "Epoch 32/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0668 - val_loss: 0.0824\n",
      "Epoch 33/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0664 - val_loss: 0.0824\n",
      "Epoch 34/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0660 - val_loss: 0.0824\n",
      "Epoch 35/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0656 - val_loss: 0.0823\n",
      "Epoch 36/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0652 - val_loss: 0.0823\n",
      "Epoch 37/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0649 - val_loss: 0.0823\n",
      "Epoch 38/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0645 - val_loss: 0.0823\n",
      "Epoch 39/40\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0642 - val_loss: 0.0822\n",
      "Epoch 40/40\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0639 - val_loss: 0.0822\n",
      "21/21 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(646, 67), test_y.shape=(646, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = -1.0, my average MASE = 24121383654.82687\n",
      "Cluster 6, -1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maes = defaultdict(lambda: [])\n",
    "mases = defaultdict(lambda: [])\n",
    "mapes = defaultdict(lambda: [])\n",
    "answers = {}\n",
    "bad_values = np.zeros(dataset.shape[1])\n",
    "\n",
    "dif=True\n",
    "\n",
    "for window_size in window_sizes_for_clustering:\n",
    "    for N_clusters in Ns_clusters:\n",
    "        dataset_windows, dataset_y = Forecasting.create_windows(dataset, window_size=window_size)\n",
    "        clusters_labels = Clustering.KMeans_for_windows(dataset_windows, W=window_size, N_clusters=N_clusters, max_iter=50)\n",
    "        print(f\"{clusters_labels.shape=}\")\n",
    "        datasets_clusters = Clustering.flatten_from_interceting_windows(dataset_windows, clusters_labels, W=window_size, \\\n",
    "                N_clusters=N_clusters)\n",
    "        # list of list of ndarrays [N_i, Q], dataset_clusters[cluster_num][i] - i-th part of dataset for cluster_num\n",
    "\n",
    "        print(f\"{N_clusters=}, {len(datasets_clusters)}, {len(datasets_clusters[0])}, {datasets_clusters[0][0].shape}\")\n",
    "        ###window_size for model\n",
    "        errors = [1] * N_clusters\n",
    "        for cluster_num in range(N_clusters):\n",
    "            sc = Forecasting.MyStandardScaler(dif=dif)\n",
    "            #datasets_clusters[cluster_num] - list of [N_i, Q] ndarrays\n",
    "            sc.fit(datasets_clusters[cluster_num])\n",
    "            prepared_data = sc.transform(datasets_clusters[cluster_num])\n",
    "            data_X, data_y = Forecasting.create_windows(prepared_data, window_size=10)\n",
    "            #data_X - list of [N_i-W, W, Q] ndarrays\n",
    "            train_X, train_y, valid_X, valid_y, test_X, test_y, ind = Forecasting.split_to_train_test(data_X, data_y, part_of_test=0.2, part_of_valid=0.2)\n",
    "            #ndarrays [N_i, W, Q] or [N_i, Q]\n",
    "            ind = np.array(ind) + window_size\n",
    "            print(f\"Before prediction: {train_X.shape=}, {train_y.shape=}, {test_X.shape=}, {test_y.shape=}\")\n",
    "            try:\n",
    "                assert(len(test_X.shape) == 3 and test_X.shape[0] > 0)\n",
    "                assert(len(valid_X.shape) == 3 and valid_X.shape[0] > 0)\n",
    "                assert(len(train_X.shape) == 3 and train_X.shape[0] > 0)\n",
    "            except AssertionError:\n",
    "                print(f\"FAIL - {test_X.shape=}, {valid_X.shape=}, {train_X.shape=}\")\n",
    "                errors[cluster_num] = np.Inf\n",
    "                continue\n",
    "            model, history = Forecasting.learn(train_X, train_y, valid_X=valid_X, valid_y=valid_y)\n",
    "            predicted = model.predict(test_X)\n",
    "            predicted_original = sc.inverse_transform(predicted)[0]\n",
    "            #inverse_trasform returns list of ndarrays \n",
    "            # if dif:\n",
    "                #константа при дифференцировании\n",
    "                # predicted_original = sc.add_first_element(predicted_original, ind)[0]\n",
    "            print(f\"{predicted_original.shape=}, {test_y.shape=}\")\n",
    "\n",
    "            #calc all metrics\n",
    "            cur_mae = mae(test_y, predicted_original, multioutput='raw_values')\n",
    "#             error_out = mase(test_y, predicted_original, y_train=test_y)\n",
    "#             error_in = mase(test_y, predicted_original, y_train=train_y)\n",
    "            # cur_mase = mase(test_y, predicted_original, y_train=test_y)\n",
    "            cur_mape = mape(test_y, predicted_original)\n",
    "            cur_mase = Forecasting.my_mase(test_y, predicted_original, multioutput='raw_values')\n",
    "            maes[(window_size, N_clusters)].append(cur_mae)\n",
    "#             mases[(window_size, N_clusters)].append((error_in, error_out))\n",
    "            mapes[(window_size, N_clusters)].append(cur_mape)\n",
    "#             errors[cluster_num] = mase_uni(test_y, predicted_original, y_train=test_y)\n",
    "            tmp_bad = cur_mase > np.percentile(cur_mase, 90)\n",
    "            bad_values += tmp_bad\n",
    "            cur_mase[tmp_bad] = -1\n",
    "#             errors[cluster_num] = Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')\n",
    "            errors[cluster_num] = np.mean(cur_mase[~tmp_bad])\n",
    "            \n",
    "            #show all metrics\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.suptitle(f\"K={N_clusters}, W={window_size}, C={cluster_num}\")\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(cur_mae, color=\"green\", label=\"library\")\n",
    "#             plt.plot(Forecasting.my_mae(test_y, predicted_original, multioutput='raw_values'), color=\"red\", label=\"custom\")\n",
    "            plt.title(\"MAE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "#             plt.plot(error_in, label=\"library, in\")\n",
    "#             plt.plot(error_out, label=\"library, out\")\n",
    "            plt.plot(cur_mase, label=\"custom, out\")\n",
    "            plt.title(\"MASE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(cur_mape)\n",
    "            plt.title(\"MAPE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f\"plots/Dataset2/K={N_clusters}  W={window_size} C={cluster_num}.png\")\n",
    "#             plt.show()\n",
    "            plt.clf()\n",
    "            # print(f\"{cur_mae=}, {cur_mase=}, {cur_mape=}\")\n",
    "            # my_mase = mase()\n",
    "            # print(f\"MASE in_sample = {error_in}, MASE out_sample = {error_out}\")\n",
    "            print(f\"average MASE = {errors[cluster_num]}, my average MASE = {Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')}\")\n",
    "            print(f\"Cluster {cluster_num}, {errors[cluster_num]}\")\n",
    "        answers[(window_size, N_clusters)] = errors\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.suptitle(f\"K={N_clusters}, W={window_size}\")\n",
    "        plt.subplot(2, 2, 1)\n",
    "\n",
    "        plt.bar(np.arange(N_clusters), [np.sum(clusters_labels == i) for i in range(N_clusters)], color='blue')\n",
    "        plt.title(\"Размеры кластеров\")\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(np.arange(N_clusters), [len(datasets_clusters[i]) for i in range(N_clusters)], color=\"green\")\n",
    "        plt.title(\"Количество непрерывных отрезков\")\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.bar(np.arange(N_clusters), errors, color=\"red\")\n",
    "        plt.title(\"MASE на тесте каждого из кластеров\")\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis('tight')\n",
    "        plt.axis('off')\n",
    "        plt.table(cellText= [[f\"{x:.2f}\"] for x in errors],\n",
    "                      rowLabels=list(range(N_clusters)),\n",
    "                      loc='center')\n",
    "#         plt.show()\n",
    "        plt.savefig(f\"plots/Dataset2/method1: {N_clusters=}  W={window_size}.png\")\n",
    "        #         plt.show()\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(np.arange(bad_values.shape[0]), bad_values)\n",
    "plt.title(\"Количество раз, когда переменная имела максимум MASE\")\n",
    "plt.savefig(f\"plots/Dataset2/bad_values.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([ 2,  4,  7,  8, 11, 14, 18, 21, 23, 25, 28, 30, 33, 37, 39, 43, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 5)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 24, 25, 26, 28, 30, 31, 33,\n",
      "       39, 48, 50, 53, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 28, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 33, 37, 39,\n",
      "       41, 43, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 9)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 12, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 23, 25, 26, 28, 30, 39, 41, 42, 48,\n",
      "       51, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(3, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 13, 14, 15, 18, 21, 23, 25, 26, 28, 30, 33,\n",
      "       37, 39, 41, 42, 44, 48, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 5)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 26, 28, 30, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 7)\n",
      "     (array([ 2, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 35, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 9)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 23, 25, 28, 30, 31, 33, 35, 39,\n",
      "       41, 43, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(3, 11)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 37, 39, 44, 48,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 2)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 12, 14, 18, 21, 23, 25, 30, 31, 33, 37, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 5)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 31, 39, 60, 61,\n",
      "       62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(5, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 16, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 23, 25, 28, 30, 33, 35, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "(5, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 14, 15, 16, 18, 19, 21, 23, 25, 27, 28, 30,\n",
      "       39, 42, 44, 47, 48, 52, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 15, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35,\n",
      "       37, 39, 60, 61, 62, 63, 65]),)\n",
      "(5, 11)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 28, 30, 31, 33, 39,\n",
      "       41, 42, 49, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(10, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 11, 13, 14, 18, 21, 23, 25, 28, 30, 31, 33,\n",
      "       35, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 5)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 22, 23, 25, 28, 30, 31, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 7)\n",
      "     (array([ 2,  4,  8,  9, 14, 16, 18, 19, 30, 58, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "(10, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 23, 24, 25, 26, 28, 30, 31,\n",
      "       33, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(10, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 12, 14, 18, 21, 22, 23, 25, 28, 30, 39, 56, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in maes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mases.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c[0] < 1)}, {np.where(val_c[1] < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([14, 18, 25, 30, 31, 33, 35, 37, 43]),)\n",
      "     (array([14, 30]),)\n",
      "(1, 5)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(1, 7)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 11, 14]),)\n",
      "     (array([14]),)\n",
      "(1, 9)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 4,  8, 14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14, 60, 61]),)\n",
      "(1, 11)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(3, 2)\n",
      "     (array([ 4, 14, 18, 30, 31, 33, 35, 37, 42, 62]),)\n",
      "     (array([14, 30]),)\n",
      "(3, 5)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(3, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61]),)\n",
      "(3, 9)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 28, 39, 62]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 19, 30]),)\n",
      "(3, 11)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 23, 28, 39, 62, 65]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "(5, 2)\n",
      "     (array([12, 14, 18, 30, 31, 33, 35, 37, 60, 62, 65]),)\n",
      "     (array([14, 30]),)\n",
      "(5, 5)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "(5, 7)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 9, 11, 14, 18, 19, 21, 23, 25, 30, 39, 42, 44, 47, 52, 60, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 11)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 21, 23, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "(10, 2)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "(10, 5)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 14]),)\n",
      "(10, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 11)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in mapes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anna",
   "language": "python",
   "name": "anna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
