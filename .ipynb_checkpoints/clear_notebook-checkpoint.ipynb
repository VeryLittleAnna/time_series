{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 12:23:35.054652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-30 12:23:35.054709: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'Forecasting' from '/home/anna/Desktop/MSU/научка/git/time_series/Forecasting.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Clustering, Forecasting\n",
    "importlib.reload(Clustering)\n",
    "importlib.reload(Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DataSet2.csv\", sep=\";\")#, parse_dates=['Timestamp']) #, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "df = data.groupby(data.index // K).mean() #усреднение\n",
    "df_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError, MeanAbsolutePercentageError\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mase = MeanAbsoluteScaledError(multioutput='raw_values')\n",
    "mase_uni = MeanAbsoluteScaledError(multioutput='uniform_average')\n",
    "mape = MeanAbsolutePercentageError(multioutput='raw_values')\n",
    "# mae = MeanAbsoluteError()\n",
    "\n",
    "\n",
    "#if ‘raw_values’, returns a full set of errors in case of multioutput input. If ‘uniform_average’, errors of all outputs are averaged with uniform weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes_for_clustering = [1, 3, 5, 10]\n",
    "Ns_clusters = [2, 5, 7, 9, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns_clusters = [7]\n",
    "window_sizes_for_clustering = [3, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408096, 67)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df_np[:, :]\n",
    "# dataset = np.concatenate((dataset[:, :8], dataset[:, 9:]), axis=1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters_labels.shape=(408093,)\n",
      "N_clusters=7, 7, 200, (141, 67)\n",
      "Before prediction: train_X.shape=(78, 10, 67), train_y.shape=(78, 67), test_X.shape=(26, 10, 67), test_y.shape=(26, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4048 - val_loss: 0.4836\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4040 - val_loss: 0.4830\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4033 - val_loss: 0.4825\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4025 - val_loss: 0.4820\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4018 - val_loss: 0.4815\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4011 - val_loss: 0.4810\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4004 - val_loss: 0.4805\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3998 - val_loss: 0.4800\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3991 - val_loss: 0.4795\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3985 - val_loss: 0.4791\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3978 - val_loss: 0.4787\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3972 - val_loss: 0.4782\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3966 - val_loss: 0.4778\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3961 - val_loss: 0.4774\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3955 - val_loss: 0.4769\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3949 - val_loss: 0.4765\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3944 - val_loss: 0.4761\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3938 - val_loss: 0.4757\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3933 - val_loss: 0.4754\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3928 - val_loss: 0.4750\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3923 - val_loss: 0.4746\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3917 - val_loss: 0.4742\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3912 - val_loss: 0.4738\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3907 - val_loss: 0.4735\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3902 - val_loss: 0.4731\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3898 - val_loss: 0.4727\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3893 - val_loss: 0.4724\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3888 - val_loss: 0.4720\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3883 - val_loss: 0.4717\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3879 - val_loss: 0.4713\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3874 - val_loss: 0.4710\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3870 - val_loss: 0.4706\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3866 - val_loss: 0.4703\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3861 - val_loss: 0.4700\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3857 - val_loss: 0.4696\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3853 - val_loss: 0.4693\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3848 - val_loss: 0.4690\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3844 - val_loss: 0.4687\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3840 - val_loss: 0.4684\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3836 - val_loss: 0.4681\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(26, 67), test_y.shape=(26, 67)\n",
      "average MASE = 200.24191807816652, my average MASE = 115004995.86397019\n",
      "Cluster 0, 200.24191807816652\n",
      "Before prediction: train_X.shape=(96, 10, 67), train_y.shape=(96, 67), test_X.shape=(32, 10, 67), test_y.shape=(32, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3700 - val_loss: 0.3405\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3693 - val_loss: 0.3402\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3686 - val_loss: 0.3398\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3679 - val_loss: 0.3394\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3673 - val_loss: 0.3391\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3667 - val_loss: 0.3388\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3660 - val_loss: 0.3384\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3654 - val_loss: 0.3381\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3648 - val_loss: 0.3378\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3642 - val_loss: 0.3374\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3636 - val_loss: 0.3371\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3631 - val_loss: 0.3368\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3625 - val_loss: 0.3365\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3620 - val_loss: 0.3362\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3614 - val_loss: 0.3359\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3609 - val_loss: 0.3356\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3603 - val_loss: 0.3353\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3598 - val_loss: 0.3350\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3593 - val_loss: 0.3348\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3588 - val_loss: 0.3345\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3583 - val_loss: 0.3342\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3578 - val_loss: 0.3339\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3573 - val_loss: 0.3337\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3568 - val_loss: 0.3334\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3563 - val_loss: 0.3331\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3558 - val_loss: 0.3329\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3554 - val_loss: 0.3326\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3549 - val_loss: 0.3324\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3544 - val_loss: 0.3321\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3540 - val_loss: 0.3319\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3535 - val_loss: 0.3317\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3531 - val_loss: 0.3314\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3526 - val_loss: 0.3312\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3522 - val_loss: 0.3309\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3517 - val_loss: 0.3307\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3513 - val_loss: 0.3305\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3509 - val_loss: 0.3302\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3505 - val_loss: 0.3300\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3500 - val_loss: 0.3298\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3496 - val_loss: 0.3296\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(32, 67), test_y.shape=(32, 67)\n",
      "average MASE = 487.79282677939574, my average MASE = 64654885.315701246\n",
      "Cluster 1, 487.79282677939574\n",
      "Before prediction: train_X.shape=(133, 10, 67), train_y.shape=(133, 67), test_X.shape=(44, 10, 67), test_y.shape=(44, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4229 - val_loss: 0.6605\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4222 - val_loss: 0.6600\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4217 - val_loss: 0.6595\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4211 - val_loss: 0.6590\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4206 - val_loss: 0.6585\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4201 - val_loss: 0.6581\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4197 - val_loss: 0.6576\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4192 - val_loss: 0.6572\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4188 - val_loss: 0.6568\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4183 - val_loss: 0.6563\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4179 - val_loss: 0.6559\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4174 - val_loss: 0.6555\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4170 - val_loss: 0.6551\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4166 - val_loss: 0.6548\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4162 - val_loss: 0.6544\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4158 - val_loss: 0.6540\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4154 - val_loss: 0.6536\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4150 - val_loss: 0.6533\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4146 - val_loss: 0.6529\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4142 - val_loss: 0.6526\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4138 - val_loss: 0.6522\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4134 - val_loss: 0.6519\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4131 - val_loss: 0.6515\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4127 - val_loss: 0.6512\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4123 - val_loss: 0.6508\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4119 - val_loss: 0.6505\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4116 - val_loss: 0.6501\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4112 - val_loss: 0.6498\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4109 - val_loss: 0.6494\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4105 - val_loss: 0.6491\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4102 - val_loss: 0.6487\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4098 - val_loss: 0.6484\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4095 - val_loss: 0.6481\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4092 - val_loss: 0.6478\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4088 - val_loss: 0.6475\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4085 - val_loss: 0.6471\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4082 - val_loss: 0.6468\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4078 - val_loss: 0.6465\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4075 - val_loss: 0.6461\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4072 - val_loss: 0.6458\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "predicted_original.shape=(44, 67), test_y.shape=(44, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 135.48804839760106, my average MASE = 169500157.01922315\n",
      "Cluster 2, 135.48804839760106\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(0, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(179, 10, 67), train_y.shape=(179, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.7124 - val_loss: 0.5670\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.7109 - val_loss: 0.5663\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7095 - val_loss: 0.5656\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.7081 - val_loss: 0.5649\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.7067 - val_loss: 0.5642\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.7054 - val_loss: 0.5636\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7040 - val_loss: 0.5629\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7027 - val_loss: 0.5623\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7015 - val_loss: 0.5616\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7002 - val_loss: 0.5610\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6990 - val_loss: 0.5604\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6978 - val_loss: 0.5598\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6967 - val_loss: 0.5592\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6955 - val_loss: 0.5586\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6944 - val_loss: 0.5581\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6932 - val_loss: 0.5575\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6922 - val_loss: 0.5570\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6910 - val_loss: 0.5564\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6900 - val_loss: 0.5559\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6890 - val_loss: 0.5554\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6879 - val_loss: 0.5549\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6869 - val_loss: 0.5544\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6859 - val_loss: 0.5539\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6849 - val_loss: 0.5534\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6838 - val_loss: 0.5530\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6829 - val_loss: 0.5525\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6819 - val_loss: 0.5521\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6809 - val_loss: 0.5516\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6800 - val_loss: 0.5511\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6790 - val_loss: 0.5507\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6781 - val_loss: 0.5502\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6772 - val_loss: 0.5498\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6763 - val_loss: 0.5494\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6754 - val_loss: 0.5489\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6745 - val_loss: 0.5485\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6736 - val_loss: 0.5481\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6728 - val_loss: 0.5476\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6719 - val_loss: 0.5472\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6710 - val_loss: 0.5468\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.6702 - val_loss: 0.5464\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 152.2675980549456, my average MASE = 139382297.75580722\n",
      "Cluster 4, 152.2675980549456\n",
      "Before prediction: train_X.shape=(34, 10, 67), train_y.shape=(34, 67), test_X.shape=(11, 10, 67), test_y.shape=(11, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5094 - val_loss: 0.4190\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5075 - val_loss: 0.4178\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5057 - val_loss: 0.4166\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5038 - val_loss: 0.4154\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5020 - val_loss: 0.4143\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5002 - val_loss: 0.4131\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4985 - val_loss: 0.4120\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4967 - val_loss: 0.4108\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4950 - val_loss: 0.4097\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4933 - val_loss: 0.4086\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4916 - val_loss: 0.4075\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4899 - val_loss: 0.4064\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4883 - val_loss: 0.4054\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4867 - val_loss: 0.4043\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4851 - val_loss: 0.4033\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4835 - val_loss: 0.4022\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4820 - val_loss: 0.4013\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4804 - val_loss: 0.4003\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4789 - val_loss: 0.3993\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4774 - val_loss: 0.3983\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4759 - val_loss: 0.3974\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4744 - val_loss: 0.3965\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4730 - val_loss: 0.3956\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4715 - val_loss: 0.3947\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4701 - val_loss: 0.3938\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4687 - val_loss: 0.3929\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4673 - val_loss: 0.3920\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4659 - val_loss: 0.3912\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4646 - val_loss: 0.3903\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4632 - val_loss: 0.3895\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4619 - val_loss: 0.3887\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4606 - val_loss: 0.3879\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4593 - val_loss: 0.3871\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4580 - val_loss: 0.3864\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4567 - val_loss: 0.3856\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4555 - val_loss: 0.3849\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4542 - val_loss: 0.3841\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4530 - val_loss: 0.3834\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4518 - val_loss: 0.3827\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4505 - val_loss: 0.3820\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "predicted_original.shape=(11, 67), test_y.shape=(11, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 3207106257.6277976, my average MASE = 7550197135.568965\n",
      "Cluster 5, 3207106257.6277976\n",
      "Before prediction: train_X.shape=(5954, 10, 67), train_y.shape=(5954, 67), test_X.shape=(1985, 10, 67), test_y.shape=(1985, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0657 - val_loss: 0.0462\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0601 - val_loss: 0.0430\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0569 - val_loss: 0.0407\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0545 - val_loss: 0.0387\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0526 - val_loss: 0.0371\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0511 - val_loss: 0.0358\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0499 - val_loss: 0.0348\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0488 - val_loss: 0.0339\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0480 - val_loss: 0.0331\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 28ms/step - loss: 0.0472 - val_loss: 0.0324\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0466 - val_loss: 0.0318\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0460 - val_loss: 0.0313\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0454 - val_loss: 0.0309\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0449 - val_loss: 0.0305\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0445 - val_loss: 0.0302\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0441 - val_loss: 0.0298\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0437 - val_loss: 0.0295\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0434 - val_loss: 0.0293\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0430 - val_loss: 0.0291\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0427 - val_loss: 0.0288\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0425 - val_loss: 0.0286\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0422 - val_loss: 0.0285\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0420 - val_loss: 0.0283\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0418 - val_loss: 0.0281\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0416 - val_loss: 0.0280\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0414 - val_loss: 0.0278\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0412 - val_loss: 0.0277\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0410 - val_loss: 0.0276\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0409 - val_loss: 0.0275\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0408 - val_loss: 0.0274\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0406 - val_loss: 0.0273\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0405 - val_loss: 0.0272\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0404 - val_loss: 0.0271\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0403 - val_loss: 0.0270\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0402 - val_loss: 0.0269\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0401 - val_loss: 0.0268\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0400 - val_loss: 0.0268\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0399 - val_loss: 0.0267\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0398 - val_loss: 0.0267\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0397 - val_loss: 0.0266\n",
      "63/63 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(1985, 67), test_y.shape=(1985, 67)\n",
      "average MASE = 611501077.0781492, my average MASE = 15009065950.824871\n",
      "Cluster 6, 611501077.0781492\n",
      "clusters_labels.shape=(408086,)\n",
      "N_clusters=7, 7, 31, (76, 67)\n",
      "Before prediction: train_X.shape=(39, 10, 67), train_y.shape=(39, 67), test_X.shape=(13, 10, 67), test_y.shape=(13, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4669 - val_loss: 0.4342\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4652 - val_loss: 0.4330\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4635 - val_loss: 0.4318\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4618 - val_loss: 0.4306\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4601 - val_loss: 0.4294\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4585 - val_loss: 0.4282\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4569 - val_loss: 0.4271\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4554 - val_loss: 0.4260\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4538 - val_loss: 0.4249\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4523 - val_loss: 0.4238\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4508 - val_loss: 0.4227\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4493 - val_loss: 0.4217\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4479 - val_loss: 0.4207\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4464 - val_loss: 0.4196\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4450 - val_loss: 0.4186\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4436 - val_loss: 0.4176\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4422 - val_loss: 0.4167\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4408 - val_loss: 0.4157\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4394 - val_loss: 0.4148\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4381 - val_loss: 0.4138\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4368 - val_loss: 0.4129\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4354 - val_loss: 0.4120\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4341 - val_loss: 0.4111\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4328 - val_loss: 0.4103\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4315 - val_loss: 0.4094\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4303 - val_loss: 0.4086\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4290 - val_loss: 0.4078\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4278 - val_loss: 0.4069\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4266 - val_loss: 0.4061\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4254 - val_loss: 0.4054\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4242 - val_loss: 0.4046\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4230 - val_loss: 0.4039\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4218 - val_loss: 0.4031\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4207 - val_loss: 0.4024\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4196 - val_loss: 0.4017\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4185 - val_loss: 0.4010\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4174 - val_loss: 0.4003\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4163 - val_loss: 0.3996\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4152 - val_loss: 0.3989\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4141 - val_loss: 0.3982\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "predicted_original.shape=(13, 67), test_y.shape=(13, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 4567449100.022725, my average MASE = 9366794241.280212\n",
      "Cluster 0, 4567449100.022725\n",
      "Before prediction: train_X.shape=(82, 10, 67), train_y.shape=(82, 67), test_X.shape=(27, 10, 67), test_y.shape=(27, 67)\n",
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3898 - val_loss: 0.4776\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3889 - val_loss: 0.4770\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3882 - val_loss: 0.4765\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3874 - val_loss: 0.4760\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3867 - val_loss: 0.4755\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3860 - val_loss: 0.4750\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3853 - val_loss: 0.4745\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3846 - val_loss: 0.4741\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3839 - val_loss: 0.4736\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3833 - val_loss: 0.4732\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3826 - val_loss: 0.4727\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3820 - val_loss: 0.4723\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3814 - val_loss: 0.4719\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3808 - val_loss: 0.4715\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3802 - val_loss: 0.4711\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3796 - val_loss: 0.4707\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3791 - val_loss: 0.4704\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3785 - val_loss: 0.4700\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3780 - val_loss: 0.4697\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3775 - val_loss: 0.4694\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3770 - val_loss: 0.4691\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3765 - val_loss: 0.4687\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3760 - val_loss: 0.4684\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3755 - val_loss: 0.4681\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3750 - val_loss: 0.4678\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3746 - val_loss: 0.4676\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3741 - val_loss: 0.4673\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3737 - val_loss: 0.4670\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3732 - val_loss: 0.4667\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3728 - val_loss: 0.4665\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3724 - val_loss: 0.4662\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3719 - val_loss: 0.4660\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3715 - val_loss: 0.4657\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3711 - val_loss: 0.4654\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3707 - val_loss: 0.4652\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3703 - val_loss: 0.4649\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3699 - val_loss: 0.4647\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3695 - val_loss: 0.4645\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3691 - val_loss: 0.4642\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3687 - val_loss: 0.4640\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "predicted_original.shape=(27, 67), test_y.shape=(27, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 160.0762013007482, my average MASE = 258926191.58356202\n",
      "Cluster 1, 160.0762013007482\n",
      "Before prediction: train_X.shape=(180, 10, 67), train_y.shape=(180, 67), test_X.shape=(60, 10, 67), test_y.shape=(60, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.6862 - val_loss: 0.5648\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6848 - val_loss: 0.5639\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6835 - val_loss: 0.5630\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6822 - val_loss: 0.5622\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6810 - val_loss: 0.5614\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6798 - val_loss: 0.5606\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6786 - val_loss: 0.5598\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6775 - val_loss: 0.5590\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6763 - val_loss: 0.5583\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6752 - val_loss: 0.5576\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6741 - val_loss: 0.5569\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6730 - val_loss: 0.5562\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6720 - val_loss: 0.5555\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6710 - val_loss: 0.5549\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6699 - val_loss: 0.5542\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6689 - val_loss: 0.5536\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6679 - val_loss: 0.5529\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6669 - val_loss: 0.5523\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6660 - val_loss: 0.5517\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6650 - val_loss: 0.5511\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6641 - val_loss: 0.5505\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6631 - val_loss: 0.5499\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6622 - val_loss: 0.5494\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6613 - val_loss: 0.5488\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6603 - val_loss: 0.5482\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6594 - val_loss: 0.5477\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6585 - val_loss: 0.5471\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6577 - val_loss: 0.5466\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6568 - val_loss: 0.5461\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6559 - val_loss: 0.5456\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.6551 - val_loss: 0.5451\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.6543 - val_loss: 0.5446\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6534 - val_loss: 0.5441\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6526 - val_loss: 0.5436\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6518 - val_loss: 0.5431\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6510 - val_loss: 0.5426\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6502 - val_loss: 0.5422\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6494 - val_loss: 0.5417\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6486 - val_loss: 0.5412\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6478 - val_loss: 0.5408\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "predicted_original.shape=(60, 67), test_y.shape=(60, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 155.71054664955898, my average MASE = 230428993.22967753\n",
      "Cluster 2, 155.71054664955898\n",
      "Before prediction: train_X.shape=(5958, 10, 67), train_y.shape=(5958, 67), test_X.shape=(1986, 10, 67), test_y.shape=(1986, 67)\n",
      "Epoch 1/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0667 - val_loss: 0.0445\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0610 - val_loss: 0.0417\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0580 - val_loss: 0.0399\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0558 - val_loss: 0.0384\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0541 - val_loss: 0.0371\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0527 - val_loss: 0.0360\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0515 - val_loss: 0.0350\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0505 - val_loss: 0.0341\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0496 - val_loss: 0.0334\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0489 - val_loss: 0.0327\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0482 - val_loss: 0.0321\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0477 - val_loss: 0.0316\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0472 - val_loss: 0.0311\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0467 - val_loss: 0.0308\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0463 - val_loss: 0.0304\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0459 - val_loss: 0.0301\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0455 - val_loss: 0.0298\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0452 - val_loss: 0.0295\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0449 - val_loss: 0.0293\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0446 - val_loss: 0.0291\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0444 - val_loss: 0.0289\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0441 - val_loss: 0.0287\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0438 - val_loss: 0.0285\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0436 - val_loss: 0.0284\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0434 - val_loss: 0.0282\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0432 - val_loss: 0.0281\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 0.0430 - val_loss: 0.0279\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 0.0428 - val_loss: 0.0278\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0426 - val_loss: 0.0276\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0425 - val_loss: 0.0275\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0423 - val_loss: 0.0274\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0422 - val_loss: 0.0273\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0420 - val_loss: 0.0272\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0419 - val_loss: 0.0271\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.0418 - val_loss: 0.0270\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0417 - val_loss: 0.0269\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 0.0416 - val_loss: 0.0268\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0415 - val_loss: 0.0268\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0414 - val_loss: 0.0267\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 0.0413 - val_loss: 0.0266\n",
      "63/63 [==============================] - 1s 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_original.shape=(1986, 67), test_y.shape=(1986, 67)\n",
      "average MASE = 906749133.8828449, my average MASE = 30689573062.1186\n",
      "Cluster 3, 906749133.8828449\n",
      "Before prediction: train_X.shape=(152, 10, 67), train_y.shape=(152, 67), test_X.shape=(51, 10, 67), test_y.shape=(51, 67)\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.3385 - val_loss: 0.2818\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3375 - val_loss: 0.2811\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3365 - val_loss: 0.2804\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3357 - val_loss: 0.2798\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3348 - val_loss: 0.2792\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3339 - val_loss: 0.2785\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3331 - val_loss: 0.2779\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3322 - val_loss: 0.2774\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3314 - val_loss: 0.2768\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3306 - val_loss: 0.2762\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3298 - val_loss: 0.2757\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3290 - val_loss: 0.2751\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3283 - val_loss: 0.2746\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3275 - val_loss: 0.2741\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3268 - val_loss: 0.2736\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3261 - val_loss: 0.2731\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3254 - val_loss: 0.2726\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3247 - val_loss: 0.2721\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3240 - val_loss: 0.2716\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3233 - val_loss: 0.2712\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3227 - val_loss: 0.2707\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3220 - val_loss: 0.2703\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3214 - val_loss: 0.2698\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3207 - val_loss: 0.2694\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3201 - val_loss: 0.2690\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3195 - val_loss: 0.2685\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3189 - val_loss: 0.2681\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3183 - val_loss: 0.2677\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.3177 - val_loss: 0.2673\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3171 - val_loss: 0.2669\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3165 - val_loss: 0.2665\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.3160 - val_loss: 0.2661\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.3154 - val_loss: 0.2657\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3149 - val_loss: 0.2653\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3143 - val_loss: 0.2649\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3138 - val_loss: 0.2645\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3133 - val_loss: 0.2641\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.3128 - val_loss: 0.2637\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3122 - val_loss: 0.2634\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3117 - val_loss: 0.2630\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "predicted_original.shape=(51, 67), test_y.shape=(51, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 187.77667932725828, my average MASE = 74366520.82473153\n",
      "Cluster 4, 187.77667932725828\n",
      "Before prediction: train_X.shape=(1, 10, 67), train_y.shape=(1, 67), test_X.shape=(0, 10, 67), test_y.shape=(0, 67)\n",
      "FAIL - test_X.shape=(0, 10, 67), valid_X.shape=(1, 10, 67), train_X.shape=(1, 10, 67)\n",
      "Before prediction: train_X.shape=(3, 10, 67), train_y.shape=(3, 67), test_X.shape=(1, 10, 67), test_y.shape=(1, 67)\n",
      "Epoch 1/40\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6025 - val_loss: 0.4427\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5996 - val_loss: 0.4405\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5967 - val_loss: 0.4383\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5939 - val_loss: 0.4362\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5911 - val_loss: 0.4342\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5883 - val_loss: 0.4321\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5857 - val_loss: 0.4301\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5831 - val_loss: 0.4282\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5806 - val_loss: 0.4263\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5781 - val_loss: 0.4245\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5756 - val_loss: 0.4229\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5733 - val_loss: 0.4213\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5709 - val_loss: 0.4198\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5685 - val_loss: 0.4182\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5662 - val_loss: 0.4167\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5638 - val_loss: 0.4151\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5614 - val_loss: 0.4136\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5590 - val_loss: 0.4121\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5567 - val_loss: 0.4106\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5544 - val_loss: 0.4091\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5523 - val_loss: 0.4076\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5501 - val_loss: 0.4063\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5479 - val_loss: 0.4052\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5458 - val_loss: 0.4041\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5437 - val_loss: 0.4032\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5417 - val_loss: 0.4023\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5398 - val_loss: 0.4013\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5380 - val_loss: 0.4003\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5361 - val_loss: 0.3993\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5343 - val_loss: 0.3983\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5325 - val_loss: 0.3972\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5307 - val_loss: 0.3962\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5289 - val_loss: 0.3951\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5271 - val_loss: 0.3939\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5253 - val_loss: 0.3928\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5236 - val_loss: 0.3917\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5218 - val_loss: 0.3905\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5201 - val_loss: 0.3894\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5184 - val_loss: 0.3883\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5167 - val_loss: 0.3872\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "predicted_original.shape=(1, 67), test_y.shape=(1, 67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average MASE = 0.20676093836285814, my average MASE = 0.32252548620908084\n",
      "Cluster 6, 0.20676093836285814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maes = defaultdict(lambda: [])\n",
    "mases = defaultdict(lambda: [])\n",
    "mapes = defaultdict(lambda: [])\n",
    "answers = {}\n",
    "bad_values = np.zeros(dataset.shape[1])\n",
    "\n",
    "dif=True\n",
    "\n",
    "for window_size in window_sizes_for_clustering:\n",
    "    for N_clusters in Ns_clusters:\n",
    "        dataset_windows, dataset_y = Forecasting.create_windows(dataset, window_size=window_size)\n",
    "        clusters_labels = Clustering.KMeans_for_windows(dataset_windows, W=window_size, N_clusters=N_clusters, max_iter=50)\n",
    "        print(f\"{clusters_labels.shape=}\")\n",
    "        datasets_clusters = Clustering.flatten_from_interceting_windows(dataset_windows, clusters_labels, W=window_size, \\\n",
    "                N_clusters=N_clusters)\n",
    "        # list of list of ndarrays [N_i, Q], dataset_clusters[cluster_num][i] - i-th part of dataset for cluster_num\n",
    "\n",
    "        print(f\"{N_clusters=}, {len(datasets_clusters)}, {len(datasets_clusters[0])}, {datasets_clusters[0][0].shape}\")\n",
    "        ###window_size for model\n",
    "        errors = [1] * N_clusters\n",
    "        for cluster_num in range(N_clusters):\n",
    "            sc = Forecasting.MyStandardScaler(dif=dif)\n",
    "            #datasets_clusters[cluster_num] - list of [N_i, Q] ndarrays\n",
    "            sc.fit(datasets_clusters[cluster_num])\n",
    "            prepared_data = sc.transform(datasets_clusters[cluster_num])\n",
    "            data_X, data_y = Forecasting.create_windows(prepared_data, window_size=10)\n",
    "            #data_X - list of [N_i-W, W, Q] ndarrays\n",
    "            train_X, train_y, valid_X, valid_y, test_X, test_y, ind = Forecasting.split_to_train_test(data_X, data_y, part_of_test=0.2, part_of_valid=0.2)\n",
    "            #ndarrays [N_i, W, Q] or [N_i, Q]\n",
    "            ind = np.array(ind) + window_size\n",
    "            print(f\"Before prediction: {train_X.shape=}, {train_y.shape=}, {test_X.shape=}, {test_y.shape=}\")\n",
    "            try:\n",
    "                assert(len(test_X.shape) == 3 and test_X.shape[0] > 0)\n",
    "                assert(len(valid_X.shape) == 3 and valid_X.shape[0] > 0)\n",
    "                assert(len(train_X.shape) == 3 and train_X.shape[0] > 0)\n",
    "            except AssertionError:\n",
    "                print(f\"FAIL - {test_X.shape=}, {valid_X.shape=}, {train_X.shape=}\")\n",
    "                errors[cluster_num] = np.Inf\n",
    "                continue\n",
    "            model, history = Forecasting.learn(train_X, train_y, valid_X=valid_X, valid_y=valid_y)\n",
    "            predicted = model.predict(test_X)\n",
    "            predicted_original = sc.inverse_transform(predicted)[0]\n",
    "            #inverse_trasform returns list of ndarrays \n",
    "            # if dif:\n",
    "                #константа при дифференцировании\n",
    "                # predicted_original = sc.add_first_element(predicted_original, ind)[0]\n",
    "            print(f\"{predicted_original.shape=}, {test_y.shape=}\")\n",
    "\n",
    "            #calc all metrics\n",
    "            cur_mae = mae(test_y, predicted_original, multioutput='raw_values')\n",
    "#             error_out = mase(test_y, predicted_original, y_train=test_y)\n",
    "#             error_in = mase(test_y, predicted_original, y_train=train_y)\n",
    "            # cur_mase = mase(test_y, predicted_original, y_train=test_y)\n",
    "            cur_mape = mape(test_y, predicted_original)\n",
    "            cur_mase = Forecasting.my_mase(test_y, predicted_original, multioutput='raw_values')\n",
    "            maes[(window_size, N_clusters)].append(cur_mae)\n",
    "#             mases[(window_size, N_clusters)].append((error_in, error_out))\n",
    "            mapes[(window_size, N_clusters)].append(cur_mape)\n",
    "#             errors[cluster_num] = mase_uni(test_y, predicted_original, y_train=test_y)\n",
    "            tmp_bad = cur_mase > np.percentile(cur_mase, 90)\n",
    "            bad_values += tmp_bad\n",
    "            cur_mase[tmp_bad] = -1\n",
    "#             errors[cluster_num] = Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')\n",
    "            errors[cluster_num] = np.mean(cur_mase[~tmp_bad])\n",
    "            \n",
    "            #show all metrics\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.suptitle(f\"K={N_clusters}, W={window_size}, C={cluster_num}\")\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(cur_mae, color=\"green\", label=\"library\")\n",
    "            plt.plot(Forecasting.my_mae(test_y, predicted_original, multioutput='raw_values'), color=\"red\", label=\"custom\")\n",
    "            plt.title(\"MAE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "#             plt.plot(error_in, label=\"library, in\")\n",
    "#             plt.plot(error_out, label=\"library, out\")\n",
    "            plt.plot(cur_mase, label=\"custom, out\")\n",
    "            plt.title(\"MASE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(cur_mape)\n",
    "            plt.title(\"MAPE\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f\"plots/Dataset2/K={N_clusters}  W={window_size} C={cluster_num}.png\")\n",
    "#             plt.show()\n",
    "            plt.clf()\n",
    "            # print(f\"{cur_mae=}, {cur_mase=}, {cur_mape=}\")\n",
    "            # my_mase = mase()\n",
    "            # print(f\"MASE in_sample = {error_in}, MASE out_sample = {error_out}\")\n",
    "            print(f\"average MASE = {errors[cluster_num]}, my average MASE = {Forecasting.my_mase(test_y, predicted_original, multioutput='uniform_average')}\")\n",
    "            print(f\"Cluster {cluster_num}, {errors[cluster_num]}\")\n",
    "        answers[(window_size, N_clusters)] = errors\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.suptitle(f\"K={N_clusters}, W={window_size}\")\n",
    "        plt.subplot(2, 2, 1)\n",
    "\n",
    "        plt.bar(np.arange(N_clusters), [np.sum(clusters_labels == i) for i in range(N_clusters)], color='blue')\n",
    "        plt.title(\"Размеры кластеров\")\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(np.arange(N_clusters), [len(datasets_clusters[i]) for i in range(N_clusters)], color=\"green\")\n",
    "        plt.title(\"Количество непрерывных отрезков\")\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.bar(np.arange(N_clusters), errors, color=\"red\")\n",
    "        plt.title(\"MASE на тесте каждого из кластеров\")\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis('tight')\n",
    "        plt.axis('off')\n",
    "        plt.table(cellText= [[f\"{x:.2f}\"] for x in errors],\n",
    "                      rowLabels=list(range(N_clusters)),\n",
    "                      loc='center')\n",
    "#         plt.show()\n",
    "        plt.savefig(f\"plots/Dataset2/method1: {N_clusters=}  W={window_size}.png\")\n",
    "        #         plt.show()\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(np.arange(bad_values.shape[0]), bad_values)\n",
    "plt.title(\"Количество раз, когда переменная имела максимум MASE\")\n",
    "plt.savefig(f\"plots/Dataset2/bad_values.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([ 2,  4,  7,  8, 11, 14, 18, 21, 23, 25, 28, 30, 33, 37, 39, 43, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 5)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 24, 25, 26, 28, 30, 31, 33,\n",
      "       39, 48, 50, 53, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 28, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 33, 37, 39,\n",
      "       41, 43, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 9)\n",
      "     (array([ 2,  4,  9, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 30, 31, 33, 35, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(1, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 12, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 11, 14, 18, 20, 21, 23, 25, 26, 28, 30, 39, 41, 42, 48,\n",
      "       51, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(3, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 13, 14, 15, 18, 21, 23, 25, 26, 28, 30, 33,\n",
      "       37, 39, 41, 42, 44, 48, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 5)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 26, 28, 30, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 7)\n",
      "     (array([ 2, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 22, 23, 25, 26, 28, 30, 35, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(3, 9)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 20, 21, 23, 25, 28, 30, 31, 33, 35, 39,\n",
      "       41, 43, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(3, 11)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 37, 39, 44, 48,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 12, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 2)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 12, 14, 18, 21, 23, 25, 30, 31, 33, 37, 39,\n",
      "       60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(5, 5)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 23, 25, 26, 28, 30, 31, 39, 60, 61,\n",
      "       62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(5, 7)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 16, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 23, 25, 28, 30, 33, 35, 39, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "(5, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 10, 11, 14, 15, 16, 18, 19, 21, 23, 25, 27, 28, 30,\n",
      "       39, 42, 44, 47, 48, 52, 58, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 15, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35,\n",
      "       37, 39, 60, 61, 62, 63, 65]),)\n",
      "(5, 11)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 39, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 21, 22, 23, 25, 28, 30, 31, 33, 39,\n",
      "       41, 42, 49, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62]),)\n",
      "(10, 2)\n",
      "     (array([ 2,  4,  7,  8,  9, 10, 11, 13, 14, 18, 21, 23, 25, 28, 30, 31, 33,\n",
      "       35, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 5)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 22, 23, 25, 28, 30, 31, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "(10, 7)\n",
      "     (array([ 2,  4,  8,  9, 14, 16, 18, 19, 30, 58, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 14, 18, 21, 22, 23, 25, 26, 28, 30, 33, 35, 37,\n",
      "       39, 60, 61, 62, 63, 65]),)\n",
      "(10, 9)\n",
      "     (array([ 2,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 13, 14, 18, 20, 21, 23, 24, 25, 26, 28, 30, 31,\n",
      "       33, 37, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "(10, 11)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  8,  9, 11, 12, 14, 18, 21, 22, 23, 25, 28, 30, 39, 56, 60,\n",
      "       61, 62, 63, 65]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62]),)\n",
      "     (array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 29, 30, 39, 60, 61, 62, 63, 65]),)\n",
      "     (array([ 2,  4, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8,  9, 14, 18, 19, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n",
      "     (array([ 2,  4,  8, 14, 18, 30, 60, 61, 62, 63]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in maes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in mases.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c[0] < 1)}, {np.where(val_c[1] < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "     (array([14, 18, 25, 30, 31, 33, 35, 37, 43]),)\n",
      "     (array([14, 30]),)\n",
      "(1, 5)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(1, 7)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 11, 14]),)\n",
      "     (array([14]),)\n",
      "(1, 9)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 4,  8, 14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14, 60, 61]),)\n",
      "(1, 11)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([ 8, 14, 65]),)\n",
      "     (array([14]),)\n",
      "(3, 2)\n",
      "     (array([ 4, 14, 18, 30, 31, 33, 35, 37, 42, 62]),)\n",
      "     (array([14, 30]),)\n",
      "(3, 5)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(3, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61]),)\n",
      "(3, 9)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 28, 39, 62]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 19, 30]),)\n",
      "(3, 11)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 23, 28, 39, 62, 65]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "(5, 2)\n",
      "     (array([12, 14, 18, 30, 31, 33, 35, 37, 60, 62, 65]),)\n",
      "     (array([14, 30]),)\n",
      "(5, 5)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "(5, 7)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([ 9, 11, 14, 18, 19, 21, 23, 25, 30, 39, 42, 44, 47, 52, 60, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "(5, 11)\n",
      "     (array([14, 18, 30, 60]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([ 8, 14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 21, 23, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14, 18, 30, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "(10, 2)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "(10, 5)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 7)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 61, 62]),)\n",
      "     (array([ 8, 14]),)\n",
      "(10, 9)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14, 60, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([ 8, 14]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n",
      "(10, 11)\n",
      "     (array([14, 61]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 60]),)\n",
      "     (array([14]),)\n",
      "     (array([ 4,  5,  6,  7, 10, 11, 14, 25, 28, 39, 62, 65]),)\n",
      "     (array([14]),)\n",
      "     (array([14, 18, 30]),)\n",
      "     (array([14]),)\n",
      "     (array([14]),)\n"
     ]
    }
   ],
   "source": [
    "for key, val in mapes.items():\n",
    "    print(key)\n",
    "    for val_c in val:\n",
    "        print(f\"     {np.where(val_c < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anna",
   "language": "python",
   "name": "anna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
